{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A guide for updating item schemas in Amazon Personalize\n",
    "\n",
    "The purpose of this notebook is to walk through the process for *updating* your *item schema* when using Amazon Personalize in the context of *User Segmentation* use cases.\n",
    "\n",
    "This notebook is inspired by the official AWS [Retail Demo Store Workshop](https://github.com/aws-samples/retail-demo-store). The main difference between this notebook and the Retail Demo Store workshop, is that the latter mainly focuses on real-time recommendations, whereas this notebook demonstrates how to implement item schema updates for batch segmentation jobs, specifially for solutions that use the item-attribute-affinity recipe. Though the teachings in this notebook apply to the other recipe types as well.\n",
    "\n",
    "\n",
    "## Notebook overview\n",
    "\n",
    "### Core content:\n",
    "\n",
    "Before we can demonstrate schema changes, we will need to do some set-up. This set up portion is documented in the first 5 chapters of this notebook.\n",
    "\n",
    "After we perform the set up, we will then walk through the process required to change an item schema. \n",
    "This is what the remaining chapters go over.\n",
    "\n",
    "Table of Contents:\n",
    "\n",
    "------------Set up------------\n",
    "- **Chapter 1**: Set Up Amazon S3 Bucket, IAM Policies, and IAM Roles (_5 minutes_)\n",
    "- **Chapter 2**: Fetch and Inspect the Datasets (_5 minutes_)\n",
    "- **Chapter 3**: Create Schemas and Import Datasets in Amazon Personalize (_15 minutes_)\n",
    "- **Chapter 4**: Create an e-commerce custom solution in Amazon Personalize (_60 minutes_)\n",
    "- **Chapter 5**: Run a Batch Segmentation Job on your Solution Version (_20 minutes_)\n",
    "\n",
    "------------Update Schema------------\n",
    "- **Chapter 6.0**: Overview of the Steps involved for Changing Item Schemas (_60 minutes_)\n",
    "- **Chapter 6 Step 1**: Update the schema (via the CreateSchema API)\n",
    "- **Chapter 6 Step 2**: Update the dataset that corresponds to your updated schema (via the UpdateDataset API)\n",
    "- **Chapter 6 Step 3**: Import your new data/columns (via the CreateDatasetImportJob API)\n",
    "- **Chapter 6 Step 4**: Create new a Solution (via the CreateSolution API) using the updated schema\n",
    "- **Chapter 6 Step 5**: Train new a solution version (via the CreateSolutionVersion API)\n",
    "\n",
    "------------Analysis------------\n",
    "- **Chapter 7.0**: Submit the Same Batch Segmentation Job to our New Solution Version (_20 minutes_)\n",
    "- **Chapter 7 Step 1**: Analysis: Inspect the outputs of our job & compare it to the output from the first solution version from chapter 5\n",
    "- **Chapter 7 Step 2**: Analysis: Compare the metrics of the two solution versions\n",
    "\n",
    "------------Clean up------------\n",
    "- **Chapter 8**: Clean up (_15 minutes_)\n",
    "\n",
    "\n",
    "#### Relevant Information:\n",
    "\n",
    "- This notebook was developed and tested in the us-east-1 Region.\n",
    "\n",
    "- To ensure a reliable run, please don't *concurrently* run multiple copies of this notebook on the same Sagemaker Notebook Instance. If you want to concurrently run multiple copies of this notebook, run each notebook in its own environment/instance. \n",
    "\n",
    "- After you finish running this notebook, please run the code cells in the `Clean up` chapter of this notebook (final chapter). This will prevent incurring additional costs.\n",
    "\n",
    "- The purpose of this notebook is to demonstrate a high-level implementation of an end-to-end Personalize Workflow for schema updates. As such, the code within this notebook has not been tested for a production environment and for the sake of brevity, not all security best practices may have been implemented. For additional information to secure your Personalize-dependent workloads, refer to the [Security in Amazon Personalize](https://docs.aws.amazon.com/personalize/latest/dg/security.html) section of the Amazon Personalize documentation.\n",
    "\n",
    "- The notebook will be using the python programming language and the AWS SDK for python (referred to as boto3). Even if you are not fluent in python, the code cells should be reasonably intuitive. In practice, you can use any programming language supported by the AWS SDK to complete the same steps from this notebook in your application environment. Visit the [official boto3 documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) for more information about the AWS SDK for Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Amazon Personalize\n",
    "\n",
    "[Amazon Personalize](https://aws.amazon.com/personalize/) makes it easy for customers to develop applications with a wide array of personalization use cases, including real time product recommendations and customized direct marketing. Amazon Personalize brings the same machine learning technology used by Amazon.com to everyone for use in their applications – with no machine learning experience required. Amazon Personalize customers pay for what they use, with no minimum fees or upfront commitment. You can start using Amazon Personalize with a simple three step process, which only takes a few clicks in the AWS console, or a set of simple API calls. First, point Amazon Personalize to user data, catalog data, and activity stream of views, clicks, purchases, etc. in Amazon S3 or upload using a simple API call. Second, with a single click in the console or an API call, train a custom private recommendation model for your data. Third, retrieve personalized recommendations for any user by creating a recommender, campaign, or batch job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Set Up Amazon S3 Bucket, IAM Policies, and IAM Roles\n",
    "\n",
    "In this Chapter, we are going to focus setting up our Amazon S3 bucket, and initializing the proper IAM Policies & Roles required to run this workflow.\n",
    "\n",
    "This chapter will take about 5 minutes.\n",
    "\n",
    "### Update dependencies\n",
    "\n",
    "To get started, we need to perform a bit of setup. First, we need to ensure that a current version of botocore is locally installed. The botocore library is used by boto3, the AWS SDK library for python. We need a current version to be able to access some of the newer Amazon Personalize features.\n",
    "\n",
    "The following cell will update pip and install the latest botocore library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (23.3.1)\n",
      "Collecting botocore\n",
      "  Using cached botocore-1.31.72-py3-none-any.whl.metadata (6.1 kB)\n",
      "Using cached botocore-1.31.72-py3-none-any.whl (11.3 MB)\n",
      "Installing collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.31.72\n",
      "    Uninstalling botocore-1.31.72:\n",
      "      Successfully uninstalled botocore-1.31.72\n",
      "Successfully installed botocore-1.31.72\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade --no-deps --force-reinstall botocore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "\n",
    "Next we need to import some dependencies/libraries needed to complete this part of the notebook.\n",
    "These are not all of the dependencies we'll be using throughout this notebook. But we will import the rest of them as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import csv  \n",
    "import os\n",
    "\n",
    "from io import StringIO\n",
    "import uuid\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import numpy\n",
    "import botocore\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create clients\n",
    "\n",
    "Next we need to create the AWS service clients needed for this demonstration.\n",
    "\n",
    "- **personalize**: this client is used to create resources in Amazon Personalize\n",
    "- **s3**: this client is used to access S3 commands and resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup clients\n",
    "personalize = boto3.client('personalize')\n",
    "s3 = boto3.Session().client('s3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up our Amazon S3 bucket\n",
    "\n",
    "For simplicity, we will use this bucket to store our input data, output data, helper scripts, and other files. \n",
    "Though, in a production environment, you may want to store these assets seperately/in seperate buckets.\n",
    "\n",
    "To ensure a consistent naming convention throughout this notebook, we generate a random number for the 'token' variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of your token is:\"85652\".\n",
      "Bucket: /personalize-dataset-schema-update-example-85652\n"
     ]
    }
   ],
   "source": [
    "# Use an epoch timestamp w/ precision to the nearest millisecond to present a pseduo-randomly generated value for token. \n",
    "# Alternatively, enter your own *lowercase alphanumeric* string of 5 characters here. The 'token` is used for naming aws resources. \n",
    "token = str(round(time.time()*1000))[-5:]\n",
    "print(f'The value of your token is:\"{token}\".')\n",
    "\n",
    "# Bucket name *must* contain the substring 'Personalize' or 'personalize'. \n",
    "#  This is to ensure compliance with the execution role of this Sagemaker Notebook instance.\n",
    "bucket_name = 'personalize-dataset-schema-update-example-' + token\n",
    "\n",
    "# If creating bucket outside of us-east-1, specify the region code within CreateBucketConfiguration.LocationConstraint attribute.\n",
    "# Reference: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/create_bucket.html\n",
    "bucket = s3.create_bucket(\n",
    "    Bucket=bucket_name\n",
    ")\n",
    "\n",
    "# The Bucket Policy we need to attach to the bucket in order to allow Amazon Personalize to access it.\n",
    "bucket_policy = {\n",
    "    'Version': '2012-10-17',\n",
    "    'Id': 'PersonalizeS3BucketAccessPolicy',\n",
    "    'Statement': [\n",
    "        {\n",
    "            'Sid': 'PersonalizeS3BucketAccessPolicy',\n",
    "            'Effect': 'Allow',\n",
    "            'Principal': {\n",
    "                'Service': 'personalize.amazonaws.com'\n",
    "            },\n",
    "            'Action': [\n",
    "                's3:GetObject',\n",
    "                's3:ListBucket',\n",
    "                's3:PutObject'\n",
    "            ],\n",
    "            'Resource': [\n",
    "                f'arn:aws:s3:::{bucket_name}',\n",
    "                f'arn:aws:s3:::{bucket_name}/*'\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the policy to a JSON string and attach it to the bucket\n",
    "bucket_policy = json.dumps(bucket_policy)\n",
    "s3.put_bucket_policy(Bucket=bucket_name, Policy=bucket_policy)\n",
    "\n",
    "\n",
    "# prints out the bucket\n",
    "print('Bucket: {}'.format(bucket['Location']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Amazon IAM Permissions for the Personalize Service\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to a bucket policy that allows Amazon Personalize access, we also need to explicitly grant the Amazon Personalize service those permissions within an IAM Role. This will enable the Personalize service to fetch and write data to Amazon S3. We use a custom-made customer-managed IAM policy to ensure we are abiding by the [Principle of Least Privilege best security practice](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating IAM Role...\n",
      "Created IAM Role. IAM Role ARN: arn:aws:iam::402114309305:role/PersonalizeRole-85652\n",
      "Creating IAM Policy...\n",
      "Created IAM Policy. Policy ARN: arn:aws:iam::402114309305:policy/PersonalizePolicy-85652\n",
      "Attached policy to Role\n"
     ]
    }
   ],
   "source": [
    "# Set up IAM for Personalize\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "# role_name must begin with the substring 'PersonalizeRole' to ensure compliance with the Execution Role of this Sagemaker Notebook instance.\n",
    "role_name = 'PersonalizeRole-'+token\n",
    "\n",
    "print(\"Creating IAM Role...\")\n",
    "role_arn = iam.create_role(\n",
    "    RoleName=role_name,\n",
    "    # Allow Amazon Personalize to assume this role\n",
    "    AssumeRolePolicyDocument=json.dumps({\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}))\n",
    "role_arn = role_arn['Role']['Arn']\n",
    "\n",
    "print(\"Created IAM Role. IAM Role ARN: \" + role_arn)\n",
    "\n",
    "\n",
    "# Create the IAM policy for Personalize\n",
    "personalize_policy_doc = {\n",
    "    'Version': '2012-10-17',\n",
    "    'Id': 'PersonalizeS3BucketAccessPolicy-'+token,\n",
    "    'Statement': [\n",
    "        {\n",
    "            'Sid': 'PersonalizeS3BucketAccessPolicy',\n",
    "            'Action': [\n",
    "                's3:GetObject',\n",
    "                's3:ListBucket',\n",
    "                's3:PutObject'\n",
    "            ],\n",
    "            'Resource': [\n",
    "                f'arn:aws:s3:::{bucket_name}',\n",
    "                f'arn:aws:s3:::{bucket_name}/*'\n",
    "            ],\n",
    "            'Effect': 'Allow'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "personalize_policy_doc = json.dumps(personalize_policy_doc)\n",
    "\n",
    "# role_name must begin with the substring 'PersonalizePolicy' to ensure compliance with the Execution Role of this Sagemaker Notebook instance.\n",
    "iam_policy_name = 'PersonalizePolicy-'+token\n",
    "\n",
    "print(\"Creating IAM Policy...\")\n",
    "policy_response = iam.create_policy(\n",
    "    PolicyName=iam_policy_name,\n",
    "    PolicyDocument=personalize_policy_doc,\n",
    "    Description='Policy to allow Personalize access to our S3 bucket'\n",
    ")\n",
    "\n",
    "# get arn of the policy\n",
    "policy_arn = policy_response['Policy']['Arn']\n",
    "policy_version = policy_response['Policy']['DefaultVersionId']\n",
    "print(\"Created IAM Policy. Policy ARN: \" + policy_arn)\n",
    "\n",
    "# Attach the policy to the role\n",
    "iam.attach_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyArn=policy_arn\n",
    ")\n",
    "print(\"Attached policy to Role\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Fetch and Inspect the Datasets\n",
    "\n",
    "Amazon Personalize provides predefined recipes, based on common use cases, for training models. A recipe is a machine learning algorithm that you use with settings, or hyperparameters, and the data you provide to train an Amazon Personalize model. The data you provide to train a model are organized into separate datasets by the type of data being provided. A collection of datasets are organized into a dataset group. The three dataset types supported by Personalize are items, users, and interactions. Depending on the recipe type you choose, a different combination of dataset types are required. For all recipe types, an interactions dataset is required. Interactions represent how users interact with items. For example, viewing a product, watching a video, listening to a recording, or reading an article. In this notebook, we will be using the item-attribute-affinity recipe, a recipe that supports all three dataset types.\n",
    "\n",
    "In this chapter, you will:\n",
    "\n",
    "    - copy public datasets to your private S3 bucket,\n",
    "    - Load the datasets into this notebook environment,\n",
    "    - Inspect the datasets so you have an understanding of the data\n",
    "    \n",
    "This chapter will take about 5 minutes.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some context on 'Items' datasets\n",
    "\n",
    "When training models in Amazon Personalize, we can provide structured and unstructured metadata about our items. This data helps improve the relevance of recommendations and is particularly useful when recommending new/cold items added to your catalog. \n",
    "\n",
    "Optional reading: For this notebook we will be creating 'custom solutions' for our use cases. Additionally, Personalize also has retail domain recommenders. This construct, which was released at re:Invent 2021 is used for real-time inferences. You can read more about them in the [Personalize blog](https://aws.amazon.com/blogs/machine-learning/amazon-personalize-announces-recommenders-optimized-for-retail-and-media-entertainment/).\n",
    "\n",
    "The retail domain recommenders stipulate some [reserved fields/columns](https://docs.aws.amazon.com/personalize/latest/dg/ECOMMERCE-datasets-and-schemas.html) that we must conform to. For example, some columns that Personalize supports for an `Items` dataset include `ITEM_ID`, `PRICE`, `CATEGORY_L1`, `CATEGORY_L2`, `PRODUCT_DESCRIPTION`, and `GENDER`. Personalize will automatically apply a natural language processing (NLP) machine learning model to the product description column to extract features from the text. The product's unique identifier is required. For items, at least one metadata column (such as price or level-1 category) is also required. In the first part of this notebook, we will create a model that only uses the `ITEM_ID`, `PRICE`, `CATEGORY_L1` columns. In the second part of the notebook, we will update the schema to include the `CATEGORY_L2`, `PRODUCT_DESCRIPTION`, and `GENDER` columns as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to CSV and upload to S3 bucket\n",
    "\n",
    "For this notebook, we will be using publicly available datasets. These datasets are part of the [Retail Demo Store](https://github.com/aws-samples/retail-demo-store) project and are provided as a public download. \n",
    "\n",
    "The following cell will copy the csv datasets from the download URL to the local volume and then upload to our private s3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing copying users.csv to personalize-dataset-schema-update-example-85652\n",
      "Finishing copying items.csv to personalize-dataset-schema-update-example-85652\n",
      "Finishing copying interactions.csv to personalize-dataset-schema-update-example-85652\n"
     ]
    }
   ],
   "source": [
    "users_filename, items_filename, interactions_filename = \"users.csv\", \"items.csv\", \"interactions.csv\"\n",
    "\n",
    "# copy the datasets from the public s3 bucket to our private s3 bucket\n",
    "for file in [users_filename, items_filename, interactions_filename]:\n",
    "    !wget https://code.retaildemostore.retail.aws.dev/csvs/{file}\n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(file).upload_file(file)\n",
    "    print(f'Finishing copying {file} to {bucket_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will download our datasets from our private s3 bucket into this notebook environment, and load them into a [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n",
    "\n",
    "Finally, we will display the first few rows of each dataset just so we have a sense of its dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Users:-----------\n",
      "   USER_ID  AGE GENDER\n",
      "0        1   31      M\n",
      "1        2   58      F\n",
      "2        3   43      M\n",
      "3        4   38      M\n",
      "4        5   24      M\n",
      "\n",
      "Original Items:-----------\n",
      "                                ITEM_ID   PRICE  CATEGORY_L1 CATEGORY_L2  \\\n",
      "0  6579c22f-be2b-444c-a52b-0116dd82df6c   90.99  accessories    backpack   \n",
      "1  2e852905-c6f4-47db-802c-654013571922  123.99  accessories    backpack   \n",
      "2  4ec7ff5c-f70f-4984-b6c4-c7ef37cc0c09   87.99  accessories    backpack   \n",
      "3  7977f680-2cf7-457d-8f4d-afa0aa168cb9  125.99  accessories    backpack   \n",
      "4  b5649d7c-4651-458d-a07f-912f253784ce  141.99  accessories    backpack   \n",
      "\n",
      "                                 PRODUCT_DESCRIPTION GENDER PROMOTED  \n",
      "0           This tan backpack is nifty for traveling      F        N  \n",
      "1                       Pale pink backpack for women      F        N  \n",
      "2  This gainsboro backpack for women is first-rat...      F        N  \n",
      "3  This gray backpack for women is first-rate for...      F        N  \n",
      "4                     Peru-orange backpack for women      F        N  \n",
      "\n",
      "Interactions:-----------\n",
      "                                ITEM_ID USER_ID EVENT_TYPE   TIMESTAMP  \\\n",
      "0  b93b7b15-9bb3-407c-b80b-517e7c45e090    3156       View  1690552936   \n",
      "1  b93b7b15-9bb3-407c-b80b-517e7c45e090    3156       View  1690552941   \n",
      "2  3946f4c8-1b5b-4161-b794-70b33affb671    2122       View  1690552959   \n",
      "3  3946f4c8-1b5b-4161-b794-70b33affb671    2122       View  1690552969   \n",
      "4  e9daa7cd-8230-4544-9f07-86fa84d7c3c1    2485       View  1690552979   \n",
      "\n",
      "  DISCOUNT  \n",
      "0       No  \n",
      "1       No  \n",
      "2       No  \n",
      "3       No  \n",
      "4       No  \n"
     ]
    }
   ],
   "source": [
    "# Users Dataset\n",
    "get_users_csv_response = s3.get_object(Bucket=bucket_name, Key=users_filename)\n",
    "users_csv_content = get_users_csv_response['Body'].read().decode('utf-8')\n",
    "\n",
    "# Create a pandas DataFrame from the CSV content\n",
    "users_df = pd.read_csv(StringIO(users_csv_content))\n",
    "print('\\nUsers:-----------')\n",
    "print(users_df.head())  # Inspect the first few rows of the DataFrame\n",
    "\n",
    "\n",
    "# Items Dataset\n",
    "get_items_csv_response = s3.get_object(Bucket=bucket_name, Key=items_filename)\n",
    "items_csv_content = get_items_csv_response['Body'].read().decode('utf-8')\n",
    "\n",
    "# Create a pandas DataFrame from the CSV content\n",
    "items_basic_df = pd.read_csv(StringIO(items_csv_content))\n",
    "print('\\nOriginal Items:-----------')\n",
    "print(items_basic_df.head())  # Inspect the first few rows of the DataFrame\n",
    "\n",
    "\n",
    "# Interactions Dataset\n",
    "get_interactions_csv_response = s3.get_object(Bucket=bucket_name, Key=interactions_filename)\n",
    "interactions_csv_content = get_interactions_csv_response['Body'].read().decode('utf-8')\n",
    "\n",
    "# Create a pandas DataFrame from the CSV content\n",
    "interactions_df = pd.read_csv(StringIO(interactions_csv_content))\n",
    "\n",
    "interactions_df['USER_ID'] = interactions_df.USER_ID.astype(str)\n",
    "interactions_df['TIMESTAMP'] = interactions_df.TIMESTAMP.astype(int)\n",
    "print('\\nInteractions:-----------')\n",
    "print(interactions_df.head())  # Inspect the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional reading: Inspection of our user & interactions input data\n",
    "\n",
    "Similar to the items dataset, we have provided metadata on our users when training models in Personalize. For this demonstration, we have included each user's age and gender. For more information about requirements for the users dataset, refer to the [aws documentation](https://docs.aws.amazon.com/personalize/latest/dg/ECOMMERCE-users-dataset.html).\n",
    "\n",
    "\n",
    "Additionally, take a look at the first few lines of the interactions file. Note: \n",
    "\n",
    "- An EVENT_TYPE column which can be used to train different Personalize campaigns & custom solutions, and can also be used to filter on recommendations. To simulate a real-world site, most of the EVENT_TYPE events are views, whereas a much smaller proportion is add to cart, checkout, and purchase events.\n",
    "- The custom DISCOUNT column which is a contextual metadata field, that a Personalize user personalization solution can take into account to predict on the best next product based the user's propensity to interact with discount products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim down the Items dataset\n",
    "The publicly-available `items dataset` is the dataset that we will be using *after* we update the schema. For the *orginial* schema, we want to have fewer columns (This means that when we use the out-of-the-box dataset, we'll essentially be simulating adding additional columns).\n",
    "\n",
    "Thus, we will trim down some non-required columns from the items dataset and create our first version on this modifed dataset.\n",
    "\n",
    "Specifically, we will:\n",
    "- remove the columns for product description, gender, category_l2, and promoted\n",
    "- upload this `basic` items dataset to s3. We will use this csv in the `first` solution we create.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'items_basic.csv' has been uploaded to S3 bucket 'personalize-dataset-schema-update-example-85652' with key 'items_basic.csv'.\n",
      "\n",
      " Basic Items dataset preview:-----------\n",
      "                                ITEM_ID   PRICE  CATEGORY_L1\n",
      "0  6579c22f-be2b-444c-a52b-0116dd82df6c   90.99  accessories\n",
      "1  2e852905-c6f4-47db-802c-654013571922  123.99  accessories\n",
      "2  4ec7ff5c-f70f-4984-b6c4-c7ef37cc0c09   87.99  accessories\n",
      "3  7977f680-2cf7-457d-8f4d-afa0aa168cb9  125.99  accessories\n",
      "4  b5649d7c-4651-458d-a07f-912f253784ce  141.99  accessories\n"
     ]
    }
   ],
   "source": [
    "\n",
    "items_basic_filename = 'items_basic.csv'\n",
    "\n",
    "# Remove the specified columns\n",
    "columns_to_remove = [\"PRODUCT_DESCRIPTION\", \"GENDER\", \"CATEGORY_L2\", \"PROMOTED\"]\n",
    "items_basic_df = items_basic_df.drop(columns=columns_to_remove)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "items_basic_df.to_csv(items_basic_filename, index=False)\n",
    "\n",
    "# Upload the modified CSV file to an S3 bucket\n",
    "s3.upload_file(items_basic_filename, bucket_name, items_basic_filename)\n",
    "\n",
    "print(f\"File '{items_basic_filename}' has been uploaded to S3 bucket '{bucket_name}' with key '{items_basic_filename}'.\")\n",
    "\n",
    "print('\\n Basic Items dataset preview:-----------')\n",
    "print(items_basic_df.head())  # Inspect the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the trimmed down items dataset only has the following columns: ITEM_ID, PRICE, CATEGORY_L1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 Summary - What have we accomplished?\n",
    "\n",
    "In this chapter, we fetched pre-prepared sample datasets for each dataset type (items, users, and interactions) and uploaded them to the Amazon S3 bucket for later use.\n",
    "\n",
    "We also inspected the three dataset types that will be used to train models and create custom solutions in Amazon Personalize.\n",
    "\n",
    "In the next chapter we will start creating resources in Amazon Personalize to receive our dataset files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Create Schemas and Import Datasets into Amazon Personalize\n",
    "\n",
    "\n",
    "In this Chapter we are going to create an Amazon Personalize dataset group and import our three datasets into Amazon Personalize.\n",
    "\n",
    "## Chapter 3 Objectives\n",
    "\n",
    "In this chapter we will accomplish the following steps. This chapter should take about 15 minutes to complete.\n",
    "\n",
    "- Create schema resources in Amazon Personalize that define the layout of our three dataset files (CSVs) created in the prior chapter\n",
    "- Create a dataset group in Amazon Personalize that will be used to receive our datasets\n",
    "- Create a dataset in the Personalize dataset group for the three dataset types and schemas\n",
    "    - Items: information about the products in the Retail Demo Store\n",
    "    - Users: information about the users in the Retail Deme Store\n",
    "    - Interactions: user-item interactions representing typical storefront behavior such as viewing products, adding products to a shopping cart, purchasing products, and so on\n",
    "- Create dataset import jobs to import each of the three datasets into Personalize\n",
    "\n",
    "Note: We will be using the trimmed version of the items dataset here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Amazon Personalize\n",
    "\n",
    "Now that we've prepared our three datasets and uploaded them to S3 we'll need to configure the Amazon Personalize service to understand our data so that it can be used to train models for generating recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Schemas for Datasets\n",
    "\n",
    "Amazon Personalize requires a schema for each dataset so it can map the columns in our CSVs to fields for model training. Each schema is declared in JSON using the [Apache Avro](https://avro.apache.org/) format.\n",
    "\n",
    "Let's define and create schemas in Personalize for our datasets.\n",
    "\n",
    "Note that categorical fields include an additional attribute of `\"categorical\": true` and the textual field has an additional attribute of `\"textual\": true`. Categorical fields are those where one or more values can be specified for the field value (i.e. enumerated values). For example, one or more category names/codes for the `CATEGORY_L1` field. A textual field indicates that Personalize should apply a natural language processing (NLP) model to the field's value to extract model features from unstructured text. In this case, we're using the product description as the textual field. You can only have one textual field in the items dataset. Finally, you will notice that the `PROMOTED` field does _not_ have `categorical` or `textual` specified. In this case, the `PROMOTED` column will not be included as a feature in the model but can be used for filtering (out of scope of this notebook).\n",
    "\n",
    "Another detail to note is that when we call the [CreateSchema](https://docs.aws.amazon.com/personalize/latest/dg/API_CreateSchema.html) API, we pass an optional `domain` parameter with a value of `ECOMMERCE`. This tells Personalize that we are creating a schema for Retail/E-commerce domain. We will do this for all three schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Users Dataset Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:us-east-1:402114309305:schema/users-schema-85652\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"e0de7810-0017-4604-9c89-fd9844b63eea\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 18:36:32 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"84\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"e0de7810-0017-4604-9c89-fd9844b63eea\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "users_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Users\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"AGE\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENDER\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True,\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    users_schema_name = 'users-schema-'+token\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = users_schema_name,\n",
    "        domain = \"ECOMMERCE\",\n",
    "        schema = json.dumps(users_schema)\n",
    "    )\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "    users_schema_arn = create_schema_response['schemaArn']\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this schema, seemingly')\n",
    "    paginator = personalize.get_paginator('list_schemas')\n",
    "    for paginate_result in paginator.paginate():\n",
    "        for schema in paginate_result['schemas']:\n",
    "            if schema['name'] == users_schema_name:\n",
    "                users_schema_arn = schema['schemaArn']\n",
    "                print(f\"Using existing schema: {users_schema_arn}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Items Datsaset Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:us-east-1:402114309305:schema/items-schema-85652\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"999ab420-c646-4ad8-a6d0-f030c65e09be\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 18:36:32 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"84\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"999ab420-c646-4ad8-a6d0-f030c65e09be\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# In this chapter, we are going to train our solution on only the three folloing columns: ITEM_ID, PRICE, CATEGORY_L1.\n",
    "# A later portion of this notebook will go over how to update the schema to include additional columns \n",
    "# such as `CATEGORY_L2`, `PRODUCT_DESCRIPTION`, and `GENDER`. \n",
    "\n",
    "items_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Items\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PRICE\",\n",
    "            \"type\": \"float\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CATEGORY_L1\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True,\n",
    "        },\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    items_schema_name = 'items-schema-'+token\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = items_schema_name,\n",
    "        domain = 'ECOMMERCE',\n",
    "        schema = json.dumps(items_schema)\n",
    "    )\n",
    "    items_schema_arn = create_schema_response['schemaArn']\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this schema, seemingly')\n",
    "    paginator = personalize.get_paginator('list_schemas')\n",
    "    for paginate_result in paginator.paginate():\n",
    "        for schema in paginate_result['schemas']:\n",
    "            if schema['name'] == items_schema_name:\n",
    "                items_schema_arn = schema['schemaArn']\n",
    "                print(f\"Using existing schema: {items_schema_arn}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactions Dataset Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:us-east-1:402114309305:schema/interactions-schema-85652\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"8882cc35-1e1f-4874-971d-f7bacbfdf1f7\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 18:36:32 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"91\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"8882cc35-1e1f-4874-971d-f7bacbfdf1f7\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "interactions_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"EVENT_TYPE\",  # \"View\", \"Purchase\", etc.\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DISCOUNT\",  # This is the contextual metadata - \"Yes\" or \"No\".\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    interactions_schema_name = 'interactions-schema-'+token\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = interactions_schema_name,\n",
    "        domain = \"ECOMMERCE\",\n",
    "        schema = json.dumps(interactions_schema)\n",
    "    )\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "    interactions_schema_arn = create_schema_response['schemaArn']\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this schema, seemingly')\n",
    "    paginator = personalize.get_paginator('list_schemas')\n",
    "    for paginate_result in paginator.paginate():\n",
    "        for schema in paginate_result['schemas']:\n",
    "            if schema['name'] == interactions_schema_name:\n",
    "                interactions_schema_arn = schema['schemaArn']\n",
    "                print(f\"Using existing schema: {interactions_schema_arn}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Dataset Group\n",
    "\n",
    "Next we need to create the dataset group that will contain our three datasets. This is one of many Personalize operations that are asynchronous. That is, we call an API to create a resource and have to wait for it to become active."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset Group\n",
    "\n",
    "Note that we are also passing `ECOMMERCE` for the `domain` parameter here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetGroupArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset-group/retaildemostore-products-DSG-85652\",\n",
      "  \"domain\": \"ECOMMERCE\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"6fc951ef-453d-4195-b2e0-515de29956f6\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 18:36:32 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"134\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"6fc951ef-453d-4195-b2e0-515de29956f6\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "DatasetGroupArn = arn:aws:personalize:us-east-1:402114309305:dataset-group/retaildemostore-products-DSG-85652\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset_group_name = 'retaildemostore-products-DSG-'+token\n",
    "    create_dataset_group_response = personalize.create_dataset_group(\n",
    "        name = dataset_group_name,\n",
    "        domain = 'ECOMMERCE'\n",
    "    )\n",
    "    dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "    print(json.dumps(create_dataset_group_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this dataset group, seemingly')\n",
    "    paginator = personalize.get_paginator('list_dataset_groups')\n",
    "    for paginate_result in paginator.paginate():\n",
    "        for dataset_group in paginate_result['datasetGroups']:\n",
    "            if dataset_group['name'] == dataset_group_name:\n",
    "                dataset_group_arn = dataset_group['datasetGroupArn']\n",
    "                break\n",
    "                \n",
    "print(f'DatasetGroupArn = {dataset_group_arn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Group to Have ACTIVE Status\n",
    "This should take about a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetGroup: CREATE PENDING\n",
      "DatasetGroup: CREATE PENDING\n",
      "DatasetGroup: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the three Datasets in Personalize\n",
    "Next we will create the datasets in Personalize for our three dataset types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Users Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85652/USERS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"fddd0bc1-265a-41a7-ad52-0bb1e4e3b402\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 18:37:02 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"108\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"fddd0bc1-265a-41a7-ad52-0bb1e4e3b402\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "Users dataset ARN = arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85652/USERS\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset_type = \"USERS\"\n",
    "    users_dataset_name = \"retaildemostore-products-users-ds-\"+token\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = users_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        schemaArn = users_schema_arn\n",
    "    )\n",
    "\n",
    "    users_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this dataset, seemingly')\n",
    "    paginator = personalize.get_paginator('list_datasets')\n",
    "    for paginate_result in paginator.paginate(datasetGroupArn = dataset_group_arn):\n",
    "        for dataset in paginate_result['datasets']:\n",
    "            if dataset['name'] == users_dataset_name:\n",
    "                users_dataset_arn = dataset['datasetArn']\n",
    "                break\n",
    "                \n",
    "print(f'Users dataset ARN = {users_dataset_arn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Items Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85652/ITEMS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"730a43fb-0ed2-4b6b-9de0-8dda72637055\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 18:37:02 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"108\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"730a43fb-0ed2-4b6b-9de0-8dda72637055\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "Items dataset ARN = arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85652/ITEMS\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset_type = \"ITEMS\"\n",
    "    items_dataset_name = \"retaildemostore-products-items-ds-\"+token\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = items_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        schemaArn = items_schema_arn\n",
    "    )\n",
    "\n",
    "    items_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this dataset, seemingly')\n",
    "    paginator = personalize.get_paginator('list_datasets')\n",
    "    for paginate_result in paginator.paginate(datasetGroupArn = dataset_group_arn):\n",
    "        for dataset in paginate_result['datasets']:\n",
    "            if dataset['name'] == items_dataset_name:\n",
    "                items_dataset_arn = dataset['datasetArn']\n",
    "                break\n",
    "                \n",
    "print(f'Items dataset ARN = {items_dataset_arn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Interactions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85652/INTERACTIONS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"88365159-5804-4b11-90bb-d60b6b93e308\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 18:37:02 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"115\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"88365159-5804-4b11-90bb-d60b6b93e308\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "Interactions dataset ARN = arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85652/INTERACTIONS\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset_type = \"INTERACTIONS\"\n",
    "    interactions_dataset_name = \"retaildemostore-products-interactions-ds-\"+token\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = interactions_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        schemaArn = interactions_schema_arn\n",
    "    )\n",
    "\n",
    "    interactions_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this dataset, seemingly')\n",
    "    paginator = personalize.get_paginator('list_datasets')\n",
    "    for paginate_result in paginator.paginate(datasetGroupArn = dataset_group_arn):\n",
    "        for dataset in paginate_result['datasets']:\n",
    "            if dataset['name'] == interactions_dataset_name:\n",
    "                interactions_dataset_arn = dataset['datasetArn']\n",
    "                break\n",
    "                \n",
    "print(f'Interactions dataset ARN = {interactions_dataset_arn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for datasets to become active\n",
    "\n",
    "It can take a minute for the datasets to be created. Let's wait for all three to become active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one dataset is still in progress\n",
      "Dataset arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85652/INTERACTIONS successfully completed\n",
      "Dataset arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85652/USERS successfully completed\n",
      "Dataset arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85652/ITEMS successfully completed\n",
      "All datasets have completed\n",
      "CPU times: user 20.8 ms, sys: 122 µs, total: 20.9 ms\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset_arns = [ items_dataset_arn, users_dataset_arn, interactions_dataset_arn ]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    for dataset_arn in reversed(dataset_arns):\n",
    "        response = personalize.describe_dataset(\n",
    "            datasetArn = dataset_arn\n",
    "        )\n",
    "        status = response[\"dataset\"][\"status\"]\n",
    "\n",
    "        if status == \"ACTIVE\":\n",
    "            print(f'Dataset {dataset_arn} successfully completed')\n",
    "            dataset_arns.remove(dataset_arn)\n",
    "        elif status == \"CREATE FAILED\":\n",
    "            print(f'Dataset {dataset_arn} failed')\n",
    "            if response['dataset'].get('failureReason'):\n",
    "                print('   Reason: ' + response['dataset']['failureReason'])\n",
    "            dataset_arns.remove(dataset_arn)\n",
    "\n",
    "    if len(dataset_arns) > 0:\n",
    "        print('At least one dataset is still in progress')\n",
    "        time.sleep(15)\n",
    "    else:\n",
    "        print(\"All datasets have completed\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Datasets to Personalize\n",
    "\n",
    "So far in this chapter we have created schemas in Personalize that define the columns in our CSVs. Then we created a datset group and defined three datasets in Personalize that will receive our data. In the following steps we will create import jobs with Personalize that will import the datasets from our S3 bucket into our dataset group. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Import Jobs\n",
    "\n",
    "With the permissions in place to allow Personalize to access our CSV files, let's create three import jobs to import each file into its respective dataset. Each import job can take roughly 10 minutes to complete so we'll create all three import jobs and then wait for them all to complete. This allows them to import in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Users Dataset Import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset-import-job/import-job-users-95f739a1\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"9d35b449-b3e7-47b9-83a8-c5ee1524f91c\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 18:37:18 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"113\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"9d35b449-b3e7-47b9-83a8-c5ee1524f91c\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import_job_suffix = str(uuid.uuid4())[:8]\n",
    "\n",
    "users_create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"import-job-users-\" + import_job_suffix,\n",
    "    datasetArn = users_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, users_filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "users_dataset_import_job_arn = users_create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(users_create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Items Dataset Import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset-import-job/import-job-items-95f739a1\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"25411ad7-ebb1-43e6-9be5-bc7c65d8b712\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 18:37:18 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"113\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"25411ad7-ebb1-43e6-9be5-bc7c65d8b712\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "items_create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"import-job-items-\" + import_job_suffix,\n",
    "    datasetArn = items_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, items_basic_filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "items_dataset_import_job_arn = items_create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(items_create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Interactions Dataset Import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset-import-job/import-job-interactions-95f739a1\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"c273d7d8-3bd7-4e1f-ad21-b4e5c87a04c1\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 18:37:18 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"120\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"c273d7d8-3bd7-4e1f-ad21-b4e5c87a04c1\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "interactions_create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"import-job-interactions-\" + import_job_suffix,\n",
    "    datasetArn = interactions_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, interactions_filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "interactions_dataset_import_job_arn = interactions_create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(interactions_create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for Import Jobs to Complete\n",
    "\n",
    "It can take up to 10 minutes for the import jobs to complete, while you're waiting you can learn more about Datasets and Schemas here: https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html\n",
    "\n",
    "We will wait for all three import jobs to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Items Import Job to Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one dataset import job still in progress\n",
      "At least one dataset import job still in progress\n",
      "At least one dataset import job still in progress\n",
      "At least one dataset import job still in progress\n",
      "Import job arn:aws:personalize:us-east-1:402114309305:dataset-import-job/import-job-interactions-95f739a1 successfully completed\n",
      "At least one dataset import job still in progress\n",
      "At least one dataset import job still in progress\n",
      "Import job arn:aws:personalize:us-east-1:402114309305:dataset-import-job/import-job-items-95f739a1 successfully completed\n",
      "Import job arn:aws:personalize:us-east-1:402114309305:dataset-import-job/import-job-users-95f739a1 successfully completed\n",
      "All import jobs have ended\n",
      "CPU times: user 190 ms, sys: 8.1 ms, total: 198 ms\n",
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import_job_arns = [ users_dataset_import_job_arn, items_dataset_import_job_arn, interactions_dataset_import_job_arn ]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    for job_arn in reversed(import_job_arns):\n",
    "        import_job_response = personalize.describe_dataset_import_job(\n",
    "            datasetImportJobArn = job_arn\n",
    "        )\n",
    "        status = import_job_response[\"datasetImportJob\"]['status']\n",
    "\n",
    "        if status == \"ACTIVE\":\n",
    "            print(f'Import job {job_arn} successfully completed')\n",
    "            import_job_arns.remove(job_arn)\n",
    "        elif status == \"CREATE FAILED\":\n",
    "            print(f'Import job {job_arn} failed')\n",
    "            if import_job_response[\"datasetImportJob\"].get('failureReason'):\n",
    "                print('   Reason: ' + import_job_response[\"datasetImportJob\"]['failureReason'])\n",
    "            import_job_arns.remove(job_arn)\n",
    "\n",
    "    if len(import_job_arns) > 0:\n",
    "        print('At least one dataset import job still in progress')\n",
    "        time.sleep(60)\n",
    "    else:\n",
    "        print(\"All import jobs have ended\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 Summary - What have we accomplished?\n",
    "\n",
    "In this chapter we created schemas in Amazon Personalize that mapped to the dataset CSVs we introduced in chapter 2. We also created a dataset group in Personalize as well as Datasets to represent our CSVs. Finally, we created dataset import jobs in Personalize to load the three datasets into Personalize.\n",
    "\n",
    "In the next chapter we will create the a custom solution and train a solution version. This is where the machine learning models are trained and deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Create a custom solution in Amazon Personalize\n",
    "\n",
    "In this chapter we are going to create a Solution in Amazon Personalize. A Solution consists of a Personalize Recipe (an algorithm), parameters, and all of its Solution Versions (ie: trained models). \n",
    "\n",
    "## Chapter 4 Objectives\n",
    "\n",
    "In this chapter we will accomplish the following steps.\n",
    "\n",
    "- Create custom solution and solution version for the following use case:\n",
    "    - **Item Attribute Affinity**: user segmentation model that recommends users for item categories/attributes.\n",
    "\n",
    "This portion should take about 60 minutes to complete. However, most of the time will be waiting for model training job to complete. While we are waiting, this notebook will review a way to confirm the columns that are being used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Custom Solution\n",
    "\n",
    "With our three datasets imported into our dataset group, we can now turn to creating solutions. \n",
    "\n",
    "We simply need to create a solution and solution version using the item-attribute-affinity recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of the recipe\n",
    "\n",
    "[Item-Attribute-Affinity:](https://docs.aws.amazon.com/personalize/latest/dg/item-attribute-affinity-recipe.html)\n",
    "> The Item-Attribute-Affinity (aws-item-attribute-affinity) recipe is a USER_SEGMENTATION recipe that creates a user segment (group of users) for each item attribute that you specify. Use Item-Attribute-Affinity to learn more about your users and take actions based on their respective user segments.\n",
    "\n",
    "> For example, you might want to create a marketing campaign for your retail application based on user preferences for shoe types in your catalog. Item-Attribute-Affinity would create a user segment for each shoe type based data in your Interactions and Items datasets. You could use this to promote different shoes to different user segments based on the likelihood that they will take an action (for example, click a shoe or purchase a shoe). Other uses might include promoting different movie genres to different users or identifying prospective job applicant based on job type.\n",
    "\n",
    "\n",
    "Note: This demonstration only uses one recipe, however there many more than that available. If you are interested, you can visit the official documentation to read more about all the [predefined recipes](https://docs.aws.amazon.com/personalize/latest/dg/working-with-predefined-recipes.html) Amazon Personalize has to offer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a User Segmentation Recipe\n",
    "item_attribute_affinity_recipe_arn = 'arn:aws:personalize:::recipe/aws-item-attribute-affinity'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Solution and Solution Version\n",
    "\n",
    "With our recipe defined, we can now create our solution and solution version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below creates a solution using the item-attribute-affinity recipe and our dataset group that we created in the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"solutionArn\": \"arn:aws:personalize:us-east-1:402114309305:solution/my-original-solution-85652\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"dbf600ff-feb2-4cef-97ce-a0633bee738e\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 18:51:05 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"96\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"dbf600ff-feb2-4cef-97ce-a0633bee738e\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "solution_version_arn = None\n",
    "solution_arn = None\n",
    "solution_name = \"my-original-solution-\"+token\n",
    "\n",
    "\n",
    "try:\n",
    "    create_solution_response = personalize.create_solution(\n",
    "        name = solution_name,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        recipeArn = item_attribute_affinity_recipe_arn\n",
    "    )\n",
    "\n",
    "    solution_arn = create_solution_response['solutionArn']\n",
    "    print(json.dumps(create_solution_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this solution, seemingly')\n",
    "    paginator = personalize.get_paginator('list_solutions')\n",
    "    for paginate_result in paginator.paginate(datasetGroupArn = dataset_group_arn):\n",
    "        for solution in paginate_result['solutions']:\n",
    "            if solution['name'] == solution_name:\n",
    "                solution_arn = solution['solutionArn']\n",
    "                print(f'Item Attribute Affinity solution ARN = {solution_arn}')\n",
    "                \n",
    "                response = personalize.list_solution_versions(\n",
    "                    solutionArn = solution_arn,\n",
    "                    maxResults = 100\n",
    "                )\n",
    "                if len(response['solutionVersions']) > 0:\n",
    "                    solution_version_arn = response['solutionVersions'][-1]['solutionVersionArn']\n",
    "                    print(f'Will use most recent solution version for this solution: {solution_version_arn}')\n",
    "                    \n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Item Attribute Affinity Solution Version\n",
    "Next we can create a solution version for the solution. This is where the model is trained for this custom solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"solutionVersionArn\": \"arn:aws:personalize:us-east-1:402114309305:solution/my-original-solution-85652/ad6aea89\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"099c3ef4-a9c0-4025-84ef-60fb333deae9\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 18:51:05 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"112\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"099c3ef4-a9c0-4025-84ef-60fb333deae9\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if not solution_version_arn:\n",
    "    create_solution_version_response = personalize.create_solution_version(\n",
    "        solutionArn = solution_arn\n",
    "    )\n",
    "\n",
    "    solution_version_arn = create_solution_version_response['solutionVersionArn']\n",
    "    print(json.dumps(create_solution_version_response, indent=2))\n",
    "else:\n",
    "    print(f'Solution version {solution_version_arn} already exists; not creating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm the training columns\n",
    "\n",
    "While the solution version is being trained in the background, let's double check that this solution will be trained on the columns of the schema we created earlier. \n",
    "\n",
    "The most reliable method of making sure that the intended columns are being used by the solution requires checking via the Amazon Personalize service page within the AWS Management Console. Visit the `Solutions and recipes` menu option under the `Custom Resources` tab. Select the solution that we just created. Click the `Solution Configuration` dropdown option, and you will be able to see the `Columns for training` that are used by your solution. You will notice that the Item dataset columns used for training were: `ITEM_ID`, `PRICE`, and `CATEGORY_L1`.\n",
    "\n",
    "You may also want to confirm the schema of the item dataset. The code cell below does that programmatically. Do note however, the columns displayed in a schema do not neccessarily have to be the columns that a solution uses for training (For example, when creating a solution, you can optionally choose to *exclude* any columns that are contained within a schema, from training. Though since we are not explicitly exlcuding any of the columns in the dataset, the columns showed in the schema are the ones that our Personalize Solution will be trained on).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"items-schema-85652\",\n",
      "  \"schemaArn\": \"arn:aws:personalize:us-east-1:402114309305:schema/items-schema-85652\",\n",
      "  \"schema\": \"{\\\"type\\\": \\\"record\\\", \\\"name\\\": \\\"Items\\\", \\\"namespace\\\": \\\"com.amazonaws.personalize.schema\\\", \\\"fields\\\": [{\\\"name\\\": \\\"ITEM_ID\\\", \\\"type\\\": \\\"string\\\"}, {\\\"name\\\": \\\"PRICE\\\", \\\"type\\\": \\\"float\\\"}, {\\\"name\\\": \\\"CATEGORY_L1\\\", \\\"type\\\": \\\"string\\\", \\\"categorical\\\": true}], \\\"version\\\": \\\"1.0\\\"}\",\n",
      "  \"creationDateTime\": \"2023-10-27 18:36:32.456000+00:00\",\n",
      "  \"lastUpdatedDateTime\": \"2023-10-27 18:36:32.456000+00:00\",\n",
      "  \"domain\": \"ECOMMERCE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "confirm_schema = personalize.describe_schema(schemaArn=items_schema_arn)\n",
    "\n",
    "print(json.dumps(confirm_schema['schema'], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for Solution Versions to Complete\n",
    "\n",
    "It can take roughly 40 minutes for the solution version to be created. During this process a model is being trained and tested with the data contained within your datasets. The duration of training jobs can increase based on the size of the dataset, training parameters and a selected recipe. In the cell below we will wait for the solution versions to finish.\n",
    "\n",
    "While you are waiting for this process to complete you can learn more about [custom solutions](https://docs.aws.amazon.com/personalize/latest/dg/training-deploying-solutions.html).\n",
    "\n",
    "Additionally, though they are out of scope of this notebook, here you can read about [Personalize Recommenders](https://docs.aws.amazon.com/personalize/latest/dg/creating-recommenders.html). Recommenders are required for real-time jobs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for the custom solution version to become active\n",
    "\n",
    "The following cell waits for the solution version for the item attribute affinity use case to become active. We *need* to make sure it is active before proceeding to the next Chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version arn:aws:personalize:us-east-1:402114309305:solution/my-original-solution-85652/ad6aea89 successfully completed\n",
      "{\n",
      "  \"solutionVersion\": {\n",
      "    \"name\": \"my-original-solution-85652/ad6aea89\",\n",
      "    \"solutionVersionArn\": \"arn:aws:personalize:us-east-1:402114309305:solution/my-original-solution-85652/ad6aea89\",\n",
      "    \"solutionArn\": \"arn:aws:personalize:us-east-1:402114309305:solution/my-original-solution-85652\",\n",
      "    \"performHPO\": false,\n",
      "    \"performAutoML\": false,\n",
      "    \"recipeArn\": \"arn:aws:personalize:::recipe/aws-item-attribute-affinity\",\n",
      "    \"datasetGroupArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset-group/retaildemostore-products-DSG-85652\",\n",
      "    \"trainingHours\": 4.212,\n",
      "    \"trainingMode\": \"FULL\",\n",
      "    \"status\": \"ACTIVE\",\n",
      "    \"creationDateTime\": \"2023-10-27 18:51:05.917000+00:00\",\n",
      "    \"lastUpdatedDateTime\": \"2023-10-27 19:30:37.539000+00:00\"\n",
      "  },\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"6fa5772f-5bec-4dd8-9ef1-292f9036fe8e\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 19:31:11 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"633\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"6fa5772f-5bec-4dd8-9ef1-292f9036fe8e\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "Solution version has completed\n",
      "CPU times: user 1.07 s, sys: 79.6 ms, total: 1.15 s\n",
      "Wall time: 40min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "solution_version_arn\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    soln_ver_response = personalize.describe_solution_version(\n",
    "        solutionVersionArn = solution_version_arn\n",
    "    )\n",
    "    status = soln_ver_response[\"solutionVersion\"][\"status\"]\n",
    "\n",
    "    if status == \"ACTIVE\":\n",
    "        print(f'Solution version {solution_version_arn} successfully completed')\n",
    "        print(json.dumps(soln_ver_response, indent=2, default=str))\n",
    "        print(\"Solution version has completed\")\n",
    "        break\n",
    "    elif status == \"CREATE FAILED\":\n",
    "        print(f'Solution version {solution_version_arn} failed')\n",
    "        if soln_ver_response[\"solutionVersion\"].get('failureReason'):\n",
    "            print('   Reason: ' + soln_ver_response[\"solutionVersion\"]['failureReason'])\n",
    "        break\n",
    "    else:\n",
    "        print('Solution version is still in progress')\n",
    "        time.sleep(60)    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 Summary - What have we accomplished?\n",
    "\n",
    "In this chapter we created a solution version for item attribute affinity use case. We also confirmed, via the console, that the solution was only using the Items columns: `ITEM_ID`, `PRICE`, and `CATEGORY_L1`.\n",
    "\n",
    "In the next chapter we will perform run a batch job on this solution version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Run A Batch Segmentation Job on your Solution Version\n",
    "\n",
    "In this chapter, we will prepare and execute a batch job for the solution version that we created previously. The purpose of doing this is to obtain a baseline that we can use to compare the to the output of the solution version for the updated schema solution in the latter portion of this notebook.\n",
    "\n",
    "The batch job for the item attribute affinity solution will return an audience of users who have an affinity for different product categories.\n",
    "\n",
    "We will wait for the job to finish executing. Afterwards, we'll inspect the outputs.\n",
    "\n",
    "This chapter will take about 20 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the size input and outputs for our job\n",
    "\n",
    "If you want, you can set the size of the input job by changing the value of the variable 'x' in the following code cell.\n",
    "\n",
    "A default value of 5 has been pre-populated for you. If the value of x equal 5, this means we will randomly select 5 USER_ID values as input for the user personalization job.\n",
    "\n",
    "Similarly, you can set the size of the output by changing the value of the variable 'y'. A default value of 10 has been pre-populated for you. This means our model will return 10 ITEM_ID recommendations for each USER_ID input.\n",
    "\n",
    "You can decrease or increase the values of 'x' and 'y' if you want. Just be aware that larger values for x and y means the inference job will take slightly longer to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the size of the input and output variables\n",
    "x = 5\n",
    "y = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input file for batch segment job\n",
    "\n",
    "Next we will prepare a user segmentation input file by randomly selecting a product from the catalog.\n",
    "\n",
    "First, let's consider the format of the job input file. Below is a sample of the input file for an item attribute affinity job that builds 3 user segments looking for users for a Video On Demand application that are interested in both comedies and action movies, users just interested in comedies, and users interested in both horror and action movies:\n",
    "\n",
    "```javascript\n",
    "{\"itemAttributes\": \"ITEMS.genres = \\\"Comedy\\\" AND ITEMS.genres = \\\"Action\\\"\"}\n",
    "{\"itemAttributes\": \"ITEMS.genres = \\\"Comedy\\\"\"}\n",
    "{\"itemAttributes\": \"ITEMS.genres = \\\"Horror\\\" AND ITEMS.genres = \\\"Action\\\"\"}\n",
    "```\n",
    "\n",
    "For our job, we will select a few categories and use each category name as the item attribute we want to use to group users.\n",
    "\n",
    "#### Here are the steps required to run this job:\n",
    "    - Randomly select x attributes (used as inputs for our batch job)\n",
    "    - Generate the input file\n",
    "    - Upload it to S3\n",
    "    - Create and start a Batch Segment Job using our Solution Version\n",
    "    - Wait for the Segment Job to complete\n",
    "    - Download the Segment job output from S3\n",
    "    - Inspect the Segment Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below selects the attributes we want to generate customer segments for, creates the input file for the batch job, and uploads it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available categories:\n",
      "['accessories' 'apparel' 'beauty' 'books' 'electronics' 'floral'\n",
      " 'footwear' 'furniture' 'groceries' 'homedecor' 'housewares' 'instruments'\n",
      " 'jewelry' 'outdoors' 'seasonal' 'tools' 'food service' 'cold dispensed'\n",
      " 'salty snacks' 'hot dispensed']\n",
      "Randomly selected categories:\n",
      "['tools' 'housewares' 'floral' 'food service' 'furniture']\n",
      "\n",
      "Previewing input file... \n",
      "{\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"tools\\\"\"}\n",
      "{\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"housewares\\\"\"}\n",
      "{\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"floral\\\"\"}\n",
      "{\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"food service\\\"\"}\n",
      "{\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"furniture\\\"\"}\n",
      "\n",
      "\n",
      "File was uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "categories = items_basic_df['CATEGORY_L1'].unique()\n",
    "print(\"Available categories:\")\n",
    "print(categories)\n",
    "\n",
    "# Randomly select x categories\n",
    "sample_categories = numpy.random.choice(categories, x, False)\n",
    "print(\"Randomly selected categories:\")\n",
    "print(sample_categories)\n",
    "\n",
    "# Generate the input file\n",
    "job_input_filename = 'job_input.json'\n",
    "with open(job_input_filename, 'w') as json_input:\n",
    "    for category in sample_categories:\n",
    "        # Write line that specifies the query for users with an affinity for the CATEGORY_L1 field\n",
    "        json_input.write(f'{{\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\\\"{category}\\\\\"\"}}\\n')\n",
    "\n",
    "# Confirm the file matches the required format:\n",
    "# One very important characteristic of the job input file is that the itemAttributes query expression \n",
    "# for each segment must be fully defined in a single line.\n",
    "print(\"\\nPreviewing input file... \")\n",
    "!head -n 5 $job_input_filename\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Upload job input file to S3\n",
    "s3_input_key = \"batch-job-inputs/item-attribute-affinity-job/\" + job_input_filename\n",
    "s3.upload_file(job_input_filename, bucket_name, s3_input_key)\n",
    "\n",
    "if s3_input_key in [object['Key'] for object in s3.list_objects(Bucket=bucket_name)['Contents']]:\n",
    "    print('File was uploaded successfully!')\n",
    "else:\n",
    "    print('File was not uploaded!')\n",
    "\n",
    "\n",
    "# We need to define an input location in our s3 bucket where the segmention job gets its input, \n",
    "# and an output location where the segmentation job writes its output.\n",
    "s3_input_path = \"s3://\" + bucket_name + \"/\" + s3_input_key\n",
    "s3_output_path = \"s3://\" + bucket_name + \"/batch-job-outputs/item-attribute-affinity-job/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create batch segment job\n",
    "\n",
    "Finally, we're ready to create a batch segment job. There are several required parameters including a name for the job, the solution version ARN for the item attribute affinity model, the IAM role that Personalize needs to be able to access the job input file and write the output file, and the job input and output locations. These parameters are required inputs for all batch jobs in Amazon Personalize.\n",
    "\n",
    "We're also optionally specifying that we only want the top y users in our user segment.\n",
    "\n",
    "The user segmentation job can take several minutes to complete. Even though our input file only specifies a few input lines, there is a certain amount of fixed overhead required for Personalize to spin up the compute resources needed to execute the job. This overhead is amortized for larger input files that generate many user segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"batchSegmentJobArn\": \"arn:aws:personalize:us-east-1:402114309305:batch-segment-job/retaildemostore-item-attribute-affinity-job-85652\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"80b67830-c788-400c-b4ff-2666b8d8a5ea\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 19:32:50 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"135\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"80b67830-c788-400c-b4ff-2666b8d8a5ea\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create and start a Batch Segment Job using our latest Solution Version\n",
    "response = personalize.create_batch_segment_job (\n",
    "    solutionVersionArn = solution_version_arn,\n",
    "    jobName = \"retaildemostore-item-attribute-affinity-job-\" + token,\n",
    "    roleArn = role_arn,\n",
    "    jobInput = {\"s3DataSource\": {\"path\": s3_input_path}},\n",
    "    jobOutput = {\"s3DataDestination\":{\"path\": s3_output_path}},\n",
    "    numResults = y\n",
    ")\n",
    "item_attribute_affinity_job_arn = response['batchSegmentJobArn']\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the Item Attribute Affinity Job to complete and inspect its output\n",
    "\n",
    "After you finish the batch jobs, run the cell below. The cell below will wait for the Item Attribute Affinity job to finish, download the output, and display its first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Import Started on:  07:32:50 PM\n",
      "DatasetSegmentJob: CREATE PENDING\n",
      "DatasetSegmentJob: CREATE IN_PROGRESS\n",
      "DatasetSegmentJob: CREATE IN_PROGRESS\n",
      "DatasetSegmentJob: CREATE IN_PROGRESS\n",
      "DatasetSegmentJob: CREATE IN_PROGRESS\n",
      "DatasetSegmentJob: CREATE IN_PROGRESS\n",
      "DatasetSegmentJob: CREATE IN_PROGRESS\n",
      "DatasetSegmentJob: CREATE IN_PROGRESS\n",
      "DatasetSegmentJob: ACTIVE\n",
      "Import Completed on:  07:40:51 PM\n",
      "{\"input\": {\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"tools\\\"\"}, \"output\": {\"usersList\": [\"2131\",\"2504\",\"4680\",\"4712\",\"3965\",\"3867\",\"517\",\"321\",\"1177\",\"2056\"]}, \"error\": null}\n",
      "{\"input\": {\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"housewares\\\"\"}, \"output\": {\"usersList\": [\"3976\",\"1018\",\"4503\",\"673\",\"1656\",\"456\",\"3609\",\"4288\",\"3948\",\"1135\"]}, \"error\": null}\n",
      "{\"input\": {\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"floral\\\"\"}, \"output\": {\"usersList\": [\"1928\",\"68\",\"161\",\"2123\",\"1691\",\"3167\",\"5035\",\"142\",\"4509\",\"3254\"]}, \"error\": null}\n",
      "{\"input\": {\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"food service\\\"\"}, \"output\": {\"usersList\": [\"5679\",\"5338\",\"5559\",\"5304\",\"5696\",\"5503\",\"5689\",\"5318\",\"5366\",\"5809\"]}, \"error\": null}\n",
      "{\"input\": {\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"furniture\\\"\"}, \"output\": {\"usersList\": [\"3297\",\"1773\",\"97\",\"4110\",\"3660\",\"2930\",\"3557\",\"2217\",\"438\",\"1742\"]}, \"error\": null}\n",
      "CPU times: user 235 ms, sys: 26.2 ms, total: 261 ms\n",
      "Wall time: 8min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "current_time = datetime.now()\n",
    "print(\"\\nImport Started on: \", current_time.strftime(\"%I:%M:%S %p\"))\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "\n",
    "while time.time() < max_time:\n",
    "    response = personalize.describe_batch_segment_job(\n",
    "        batchSegmentJobArn = item_attribute_affinity_job_arn\n",
    "    )\n",
    "    status = response[\"batchSegmentJob\"]['status']\n",
    "    print(\"DatasetSegmentJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "    \n",
    "current_time = datetime.now()\n",
    "print(\"Import Completed on: \", current_time.strftime(\"%I:%M:%S %p\"))\n",
    "\n",
    "job_output_file = job_input_filename + \".out\"\n",
    "export_name = 'batch-job-outputs/item-attribute-affinity-job/' + job_output_file\n",
    "s3.download_file(bucket_name, export_name, job_output_file)\n",
    "\n",
    "!head -n 5 $job_output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the input item attribute query expressions are echoed in the output file but we also have `output` and `error` elements for each user segment. The `output` element has a `usersList` array that contains the user IDs for the segment. If there were any errors enountered while generating a segment, details will be included in the `error` element for the segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 5 Summary:\n",
    "\n",
    "In this chapter, we ran a batch segmentation job for the item attribute affinity solution.\n",
    "\n",
    "We waited for the inference job to finish and then inspected its outputs.\n",
    "\n",
    "#### Next Steps (out of scope of this notebook)\n",
    "Now that we have user segments created, what can we do with them? The most obvious choice is to use these outputs in outbound marketing tools. \n",
    "\n",
    "For example, you can create a promotion around a particular product category where you're looking to target users who would have the highest probability of being interested in the promotion. For instance, you may want to send \"football\" related marketing communications during Superbowl week. You would use the item attribute affinity model for this. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5 complete\n",
    "\n",
    "Congratulations! You have completed the batch segmentation portion of the demonstration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: Overview of Steps Involved for Changing Item Schemas\n",
    "\n",
    "Suppose we want to start collecting addtional data for our items dataset. Fortunately, Amazon Personalize supports updating Item Schemas to add new columns. \n",
    "\n",
    "In fact, recently, [Amazon Personalize made it easier to add columns to your existing datasets](https://aws.amazon.com/about-aws/whats-new/2023/07/amazon-personalize-add-columns-existing-datasets/). \n",
    "\n",
    "> To add a column to an existing dataset, simply select your dataset in the Personalize console and click “Replace Schema.” Add the new columns and import new data. The new columns will then be available for filtering. To use the new columns for training, simply clone your existing Solution, select the new columns for training, and train a new Solution version.\n",
    "\n",
    "This chapter will guide you through the steps involved in programmatically updating the schema of your items dataset.\n",
    "This chapter will take about 60 minutes, but most of that time will be spent waiting for data import jobs and solution version training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is a list of steps required to change the schema of your items:\n",
    "- Step 1: Create a new schema (include the new columns. All new fields must support 'null' data)\n",
    "- Step 2: Update the Items Dataset in Amazon Personalize\n",
    "- Step 3: Import the New Data (10 minutes)\n",
    "- Step 4: Create a new Solution for the Updated Data\n",
    "- Step 5: Create a new Solution Version (40 minutes)\n",
    "\n",
    "Once we perform these steps, we will then run the same batch job on the new solution version and see the new output (Chapter 7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Create a new schema that reflects schema of the updated dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:us-east-1:402114309305:schema/import-job-updated-items-schema-85652\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"b77f4cd4-30aa-4a6e-a589-772691f68614\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 19:40:51 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"103\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"b77f4cd4-30aa-4a6e-a589-772691f68614\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Note: All new fields must support 'null' data. See more here: https://docs.aws.amazon.com/personalize/latest/dg/updating-dataset-schema.html.\n",
    "\n",
    "updated_items_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Items\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PRICE\",\n",
    "            \"type\": \"float\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CATEGORY_L1\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CATEGORY_L2\",\n",
    "            \"type\": [\"string\", \"null\"],\n",
    "            \"categorical\": True,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PRODUCT_DESCRIPTION\",\n",
    "            \"type\": [\"string\", \"null\"],\n",
    "            \"textual\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENDER\",\n",
    "            \"type\": [\"string\", \"null\"],\n",
    "            \"categorical\": True,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PROMOTED\",\n",
    "            \"type\": [\"string\", \"null\"],\n",
    "        },\n",
    "    ],\n",
    "    \"version\": \"1.1\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    updated_items_schema_name = 'import-job-updated-items-schema-'+token\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = updated_items_schema_name,\n",
    "        domain = 'ECOMMERCE',\n",
    "        schema = json.dumps(updated_items_schema)\n",
    "    )\n",
    "    updated_items_schema_arn = create_schema_response['schemaArn']\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this schema, seemingly')\n",
    "    paginator = personalize.get_paginator('list_schemas')\n",
    "    for paginate_result in paginator.paginate():\n",
    "        for schema in paginate_result['schemas']:\n",
    "            if schema['name'] == updated_items_schema_name:\n",
    "                updated_items_schema_arn = schema['schemaArn']\n",
    "                print(f\"Using existing schema: {updated_items_schema_arn}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Update the Items Dataset in Amazon Personalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARN of the updated dataset: arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85652/ITEMS\n",
      "\n",
      "Import Started on:  07:40:51 PM\n",
      "DatasetSegmentJob: UPDATE PENDING\n",
      "DatasetSegmentJob: UPDATE PENDING\n",
      "DatasetSegmentJob: UPDATE IN_PROGRESS\n",
      "DatasetSegmentJob: UPDATE IN_PROGRESS\n",
      "DatasetSegmentJob: UPDATE IN_PROGRESS\n",
      "DatasetSegmentJob: UPDATE IN_PROGRESS\n",
      "DatasetSegmentJob: UPDATE IN_PROGRESS\n",
      "DatasetSegmentJob: UPDATE IN_PROGRESS\n",
      "DatasetSegmentJob: ACTIVE\n",
      "Updated dataset:\n",
      "\n",
      "{\n",
      "  \"dataset\": {\n",
      "    \"name\": \"retaildemostore-products-items-ds-85652\",\n",
      "    \"datasetArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85652/ITEMS\",\n",
      "    \"datasetGroupArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset-group/retaildemostore-products-DSG-85652\",\n",
      "    \"datasetType\": \"ITEMS\",\n",
      "    \"schemaArn\": \"arn:aws:personalize:us-east-1:402114309305:schema/import-job-updated-items-schema-85652\",\n",
      "    \"status\": \"ACTIVE\",\n",
      "    \"creationDateTime\": \"2023-10-27 18:37:02.857000+00:00\",\n",
      "    \"lastUpdatedDateTime\": \"2023-10-27 19:42:40.791000+00:00\",\n",
      "    \"latestDatasetUpdate\": {\n",
      "      \"schemaArn\": \"arn:aws:personalize:us-east-1:402114309305:schema/import-job-updated-items-schema-85652\",\n",
      "      \"status\": \"ACTIVE\",\n",
      "      \"creationDateTime\": \"2023-10-27 19:40:51.836000+00:00\",\n",
      "      \"lastUpdatedDateTime\": \"2023-10-27 19:42:40.001000+00:00\"\n",
      "    }\n",
      "  },\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"3b885310-29ae-47fe-b8be-64cd55673dec\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 19:42:52 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"717\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"3b885310-29ae-47fe-b8be-64cd55673dec\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "Import Completed on:  07:42:52 PM\n"
     ]
    }
   ],
   "source": [
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize/client/update_dataset.html\n",
    "updated_items_dataset_arn = personalize.update_dataset(\n",
    "    datasetArn = items_dataset_arn, # Pass the arn of the dataset you want to update\n",
    "    schemaArn = updated_items_schema_arn # Pass the new schema arn\n",
    ")\n",
    "\n",
    "updated_items_dataset_arn = updated_items_dataset_arn['datasetArn']\n",
    "print(f'ARN of the updated dataset: {updated_items_dataset_arn}')\n",
    "\n",
    "\n",
    "# Wait for the dataset to be updated. It can take a minute.\n",
    "current_time = datetime.now()\n",
    "print(\"\\nImport Started on: \", current_time.strftime(\"%I:%M:%S %p\"))\n",
    "\n",
    "max_time = time.time() + 1*60*60 # 1 hour\n",
    "\n",
    "while time.time() < max_time:\n",
    "    dataset_response = personalize.describe_dataset(datasetArn = updated_items_dataset_arn)\n",
    "    status = dataset_response['dataset']['status']\n",
    "    print(\"DatasetSegmentJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        print(\"Updated dataset:\\n\")\n",
    "        # TODO: View the describe_dataset response. It should have information about the update.\n",
    "        print(json.dumps(dataset_response, indent=2, default=str))\n",
    "        break\n",
    "        \n",
    "    time.sleep(15)\n",
    "    \n",
    "current_time = datetime.now()\n",
    "print(\"\\nImport Completed on: \", current_time.strftime(\"%I:%M:%S %p\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Import the new data\n",
    "\n",
    "After you associate the new schema with your dataset, you must import the new data.\n",
    "You can use the CreateDatasetImportJob API call to import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset-import-job/updated_items_dataset_import_job85652\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"bd04e60b-a312-471a-9319-44637973cc91\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 19:42:52 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"125\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"bd04e60b-a312-471a-9319-44637973cc91\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "Dataset import job:CREATE PENDING\n",
      "Dataset import job:CREATE IN_PROGRESS\n",
      "Dataset import job:CREATE IN_PROGRESS\n",
      "Dataset import job:CREATE IN_PROGRESS\n",
      "Dataset import job:CREATE IN_PROGRESS\n",
      "Dataset import job:CREATE IN_PROGRESS\n",
      "Import job arn:aws:personalize:us-east-1:402114309305:dataset-import-job/updated_items_dataset_import_job85652 successfully completed\n"
     ]
    }
   ],
   "source": [
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize/client/create_dataset_import_job.html\n",
    "updated_items_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"updated-items-dataset-import-job-\"+token,\n",
    "    datasetArn = updated_items_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, items_filename) # Pass in the new dataset. See note below\n",
    "    },\n",
    "    roleArn = role_arn,\n",
    "    importMode = \"FULL\"\n",
    ")\n",
    "# Note: notice how we are passing the updated items dataset (the dataset that contains the additional columns)\n",
    "# Thus, Personalize will now use all of the columns in the dataset for training. \n",
    "\n",
    "updated_items_dataset_import_job_arn = updated_items_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(updated_items_dataset_import_job_response, indent=2))\n",
    "\n",
    "\n",
    "# Wait for the data to finish importing. It can take up to 10 minutes.\n",
    "max_time = time.time() + 1*60*60 # 1 hours\n",
    "\n",
    "while time.time() < max_time:\n",
    "    import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = updated_items_dataset_import_job_arn\n",
    "    )\n",
    "    status = import_job_response[\"datasetImportJob\"]['status']\n",
    "\n",
    "    if status == \"ACTIVE\":\n",
    "        print(f'Import job {updated_items_dataset_import_job_arn} successfully completed')\n",
    "        break\n",
    "    elif status == \"CREATE FAILED\":\n",
    "        print(f'Import job {updated_items_dataset_import_job_arn} failed')\n",
    "        if import_job_response[\"datasetImportJob\"].get('failureReason'):\n",
    "            print('   Reason: ' + import_job_response[\"datasetImportJob\"]['failureReason'])\n",
    "    else: # status == CREATE PENDING or CREATE IN_PROGRESS\n",
    "        print('Dataset import job:' + status)\n",
    "        time.sleep(60)\n",
    "\n",
    "# After importing the data, the new columns are available for filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Create a new Solution for the updated data.\n",
    "\n",
    "Once the new dataset is finished importing, you will need to create a new solution.\n",
    "In the request to create the new solution, you can pass in the same DatasetGroup Arn as before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"solutionArn\": \"arn:aws:personalize:us-east-1:402114309305:solution/my-updated-solution-85652\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"93c56fea-25a5-489f-b56e-3f8055a13ecc\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 19:48:53 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"95\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"93c56fea-25a5-489f-b56e-3f8055a13ecc\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Create solution\n",
    "\n",
    "updated_solution_version_arn = None\n",
    "updated_solution_arn = None\n",
    "updated_solution_name = \"my-updated-solution-\"+token\n",
    "\n",
    "\n",
    "try:\n",
    "    # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize/client/create_solution.html\n",
    "    create_solution_response = personalize.create_solution(\n",
    "        name = updated_solution_name,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        recipeArn = item_attribute_affinity_recipe_arn\n",
    "    )\n",
    "\n",
    "    updated_solution_arn = create_solution_response['solutionArn']\n",
    "    print(json.dumps(create_solution_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this solution, seemingly')\n",
    "    paginator = personalize.get_paginator('list_solutions')\n",
    "    for paginate_result in paginator.paginate(datasetGroupArn = dataset_group_arn):\n",
    "        for solution in paginate_result['solutions']:\n",
    "            if solution['name'] == updated_solution_name:\n",
    "                updated_solution_arn = solution['solutionArn']\n",
    "                print(f'Item Attribute Affinity solution ARN = {updated_solution_arn}')\n",
    "                \n",
    "                response = personalize.list_solution_versions(\n",
    "                    solutionArn = updated_solution_arn,\n",
    "                    maxResults = 100\n",
    "                )\n",
    "                if len(response['solutionVersions']) > 0:\n",
    "                    updated_solution_version_arn = response['solutionVersions'][-1]['solutionVersionArn']\n",
    "                    print(f'Will use most recent solution version for this solution: {updated_solution_version_arn}')\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Create a new Solution Version from the new Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"solutionVersionArn\": \"arn:aws:personalize:us-east-1:402114309305:solution/my-updated-solution-85652/0a2037c2\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"e35aa636-d62e-4b26-98e1-60f4281b73d6\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 19:48:53 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"111\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"e35aa636-d62e-4b26-98e1-60f4281b73d6\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if not updated_solution_version_arn:\n",
    "    create_solution_version_response = personalize.create_solution_version(\n",
    "        solutionArn = updated_solution_arn\n",
    "    )\n",
    "\n",
    "    updated_solution_version_arn = create_solution_version_response['solutionVersionArn']\n",
    "    print(json.dumps(create_solution_version_response, indent=2))\n",
    "else:\n",
    "    print(f'Solution version {updated_solution_version_arn} already exists; not creating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the new solution version to finish training\n",
    "This can take around 40 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "Solution version arn:aws:personalize:us-east-1:402114309305:solution/my-updated-solution-85652/0a2037c2 successfully completed\n",
      "CPU times: user 1.09 s, sys: 65.6 ms, total: 1.15 s\n",
      "Wall time: 41min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "while time.time() < max_time:\n",
    "    updated_soln_ver_response = personalize.describe_solution_version(\n",
    "        solutionVersionArn = updated_solution_version_arn\n",
    "    )\n",
    "    status = updated_soln_ver_response[\"solutionVersion\"][\"status\"]\n",
    "\n",
    "    if status == \"ACTIVE\":\n",
    "        print(f'Solution version {updated_solution_version_arn} successfully completed')\n",
    "        break\n",
    "    elif status == \"CREATE FAILED\":\n",
    "        print(f'Solution version {updated_solution_version_arn} failed')\n",
    "        if updated_soln_ver_response[\"solutionVersion\"].get('failureReason'):\n",
    "            print('   Reason: ' + updated_soln_ver_response[\"solutionVersion\"]['failureReason'])\n",
    "        break\n",
    "    else:\n",
    "        print('At least one solution version is still in progress')\n",
    "        time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 7.0: Submit the batch segmentation job to our new Solution Version\n",
    "\n",
    "In this chapter, we will:\n",
    "- Step 1: Submit a batch segmentation job to our new solution version. For consistency, we will use the same sample input that we generated previously.\n",
    "- Wait for the batch job to complete.\n",
    "- Step 2: Compare the output of the new solution version to the previous solution versions' output. We should expect to see slight differences in outputs between the before-and-after solution versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Previewing input file item-attribute-affinity solution... \n",
      "{\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"tools\\\"\"}\n",
      "{\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"housewares\\\"\"}\n",
      "{\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"floral\\\"\"}\n",
      "{\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"food service\\\"\"}\n",
      "{\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"furniture\\\"\"}\n",
      "\n",
      "\n",
      "{\n",
      "  \"batchSegmentJobArn\": \"arn:aws:personalize:us-east-1:402114309305:batch-segment-job/retaildemostore-updated-item-attribute-affinity-job-85652\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"a140d94e-096f-4251-9062-bee7961070de\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 20:30:00 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"143\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"a140d94e-096f-4251-9062-bee7961070de\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "### Step 1) Submit batch segmentation job. \n",
    "\n",
    "# Review the sample input file from earlier. We will use this same file to perform a batch inference job using our new solution version.\n",
    "\n",
    "# Create a copy of the sample input file for the new solution's batch inference job & rename it\n",
    "job_input_filename_updated = 'job_input_updated.json'\n",
    "!cp $job_input_filename $job_input_filename_updated\n",
    "\n",
    "# Preview input file\n",
    "print(\"\\nPreviewing input file item-attribute-affinity solution... \")\n",
    "!head -n 5 $job_input_filename_updated\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Create and start a Batch Segmentation Job using our latest Solution Version\n",
    "\n",
    "s3_input_key_updated = \"batch-job-inputs/item-attribute-affinity-job/\" + job_input_filename_updated\n",
    "\n",
    "s3.upload_file(job_input_filename_updated, bucket_name, s3_input_key_updated)\n",
    "\n",
    "s3_input_path_updated = \"s3://\" + bucket_name + \"/\" + s3_input_key_updated\n",
    "\n",
    "response = personalize.create_batch_segment_job (\n",
    "    solutionVersionArn = updated_solution_version_arn,\n",
    "    jobName = \"retaildemostore-updated-item-attribute-affinity-job-\" + token,\n",
    "    roleArn = role_arn,\n",
    "    jobInput = {\"s3DataSource\": {\"path\": s3_input_path_updated}},\n",
    "    jobOutput = {\"s3DataDestination\":{\"path\": s3_output_path}},\n",
    "    numResults = y\n",
    ")\n",
    "job_updated_arn = response['batchSegmentJobArn']\n",
    "print(json.dumps(response, indent=2, default=str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Started on:  08:30:00 PM\n",
      "DatasetSegmentJob arn:aws:personalize:us-east-1:402114309305:batch-segment-job/retaildemostore-updated-item-attribute-affinity-job-85652: CREATE PENDING\n",
      "Job still in progress\n",
      "DatasetSegmentJob arn:aws:personalize:us-east-1:402114309305:batch-segment-job/retaildemostore-updated-item-attribute-affinity-job-85652: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetSegmentJob arn:aws:personalize:us-east-1:402114309305:batch-segment-job/retaildemostore-updated-item-attribute-affinity-job-85652: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetSegmentJob arn:aws:personalize:us-east-1:402114309305:batch-segment-job/retaildemostore-updated-item-attribute-affinity-job-85652: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetSegmentJob arn:aws:personalize:us-east-1:402114309305:batch-segment-job/retaildemostore-updated-item-attribute-affinity-job-85652: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetSegmentJob arn:aws:personalize:us-east-1:402114309305:batch-segment-job/retaildemostore-updated-item-attribute-affinity-job-85652: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetSegmentJob arn:aws:personalize:us-east-1:402114309305:batch-segment-job/retaildemostore-updated-item-attribute-affinity-job-85652: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetSegmentJob arn:aws:personalize:us-east-1:402114309305:batch-segment-job/retaildemostore-updated-item-attribute-affinity-job-85652: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetSegmentJob arn:aws:personalize:us-east-1:402114309305:batch-segment-job/retaildemostore-updated-item-attribute-affinity-job-85652: ACTIVE\n",
      "Job has ended on:  08:38:01 PM\n",
      "CPU times: user 227 ms, sys: 13.4 ms, total: 240 ms\n",
      "Wall time: 8min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Wait for the batch jobs to complete. This can take around 10 minutes.\n",
    "\n",
    "current_time = datetime.now()\n",
    "print(\"Import Started on: \", current_time.strftime(\"%I:%M:%S %p\"))\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    resp = personalize.describe_batch_segment_job(batchSegmentJobArn = job_updated_arn)\n",
    "    status = resp[\"batchSegmentJob\"]['status']\n",
    "    print(\"DatasetSegmentJob {arn}: {status}\".format(arn=job_updated_arn, status=status))\n",
    "\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        current_time = datetime.now()\n",
    "        print(\"Job has ended on: \", current_time.strftime(\"%I:%M:%S %p\"))\n",
    "        break\n",
    "    else:\n",
    "        print('Job still in progress')\n",
    "        time.sleep(60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the output files of the segmenation jobs from S3:\n",
    "\n",
    "job_output_file_updated = job_input_filename_updated + \".out\"\n",
    "export_name_updated = 'batch-job-outputs/item-attribute-affinity-job/' + job_output_file_updated\n",
    "s3.download_file(bucket_name, export_name_updated, job_output_file_updated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2) Compare the outputs of the segmentation jobs.\n",
    "\n",
    "Now lets inspect the output of this job and compare it to the output of the job that we ran on the solution version which used the original items schema.\n",
    "The code block below provides a side-by-side comparison of the two outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting outputs of original-schema & new-schema solution versions:\n",
      "Original schema output:\n",
      "{\"input\": {\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"tools\\\"\"}, \"output\": {\"usersList\": [\"2131\",\"2504\",\"4680\",\"4712\",\"3965\",\"3867\",\"517\",\"321\",\"1177\",\"2056\"]}, \"error\": null}\n",
      "{\"input\": {\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"housewares\\\"\"}, \"output\": {\"usersList\": [\"3976\",\"1018\",\"4503\",\"673\",\"1656\",\"456\",\"3609\",\"4288\",\"3948\",\"1135\"]}, \"error\": null}\n",
      "{\"input\": {\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"floral\\\"\"}, \"output\": {\"usersList\": [\"1928\",\"68\",\"161\",\"2123\",\"1691\",\"3167\",\"5035\",\"142\",\"4509\",\"3254\"]}, \"error\": null}\n",
      "{\"input\": {\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"food service\\\"\"}, \"output\": {\"usersList\": [\"5679\",\"5338\",\"5559\",\"5304\",\"5696\",\"5503\",\"5689\",\"5318\",\"5366\",\"5809\"]}, \"error\": null}\n",
      "{\"input\": {\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"furniture\\\"\"}, \"output\": {\"usersList\": [\"3297\",\"1773\",\"97\",\"4110\",\"3660\",\"2930\",\"3557\",\"2217\",\"438\",\"1742\"]}, \"error\": null}\n",
      "\n",
      "New schema output:\n",
      "{\"input\": {\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"tools\\\"\"}, \"output\": {\"usersList\": [\"4619\",\"4680\",\"2131\",\"321\",\"4712\",\"3965\",\"172\",\"2832\",\"5143\",\"5167\"]}, \"error\": null}\n",
      "{\"input\": {\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"housewares\\\"\"}, \"output\": {\"usersList\": [\"1135\",\"3976\",\"1524\",\"1656\",\"1018\",\"4503\",\"3948\",\"4288\",\"4835\",\"4221\"]}, \"error\": null}\n",
      "{\"input\": {\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"floral\\\"\"}, \"output\": {\"usersList\": [\"68\",\"3501\",\"2123\",\"5035\",\"1280\",\"2044\",\"604\",\"962\",\"3395\",\"3167\"]}, \"error\": null}\n",
      "{\"input\": {\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"food service\\\"\"}, \"output\": {\"usersList\": [\"5679\",\"5338\",\"5559\",\"5304\",\"5318\",\"5689\",\"5696\",\"5366\",\"5809\",\"5920\"]}, \"error\": null}\n",
      "{\"input\": {\"itemAttributes\": \"ITEMS.CATEGORY_L1 = \\\"furniture\\\"\"}, \"output\": {\"usersList\": [\"4110\",\"4118\",\"3660\",\"2217\",\"2211\",\"1406\",\"1773\",\"2839\",\"3078\",\"3467\"]}, \"error\": null}\n"
     ]
    }
   ],
   "source": [
    "print('Inspecting outputs of original-schema & new-schema solution versions:')\n",
    "print('Original schema output:')\n",
    "!head -n 5 $job_output_file\n",
    "\n",
    "print('\\nNew schema output:')\n",
    "!head -n 5 $job_output_file_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the outputs:\n",
    "\n",
    "Look at the outputs in the previous code block.\n",
    "Do you notice any changes in the outputs?\n",
    "\n",
    "We should expect to see slight differences in outputs between the before-and-after solution versions.\n",
    "Any changes you see is a representation of the fact that we are able to create a new solution version after updating the item schema! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Evaluate the metrics of the two solution versions\n",
    "\n",
    "Amazon Personalize provides [offline metrics](https://docs.aws.amazon.com/personalize/latest/dg/working-with-training-metrics.html#working-with-training-metrics-metrics) for solution versions that allow you to evaluate the accuracy of the model before you deploy the model in your application. Metrics can also be used to view the effects of modifying a custom solution's hyperparameters or to compare the metrics between two solutions versions.\n",
    "\n",
    "Let's retrieve and compare the metrics for the solution versions we just created in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics of the solution version with the original schema & dataset:\n",
      "{\n",
      "  \"coverage\": 0.5869,\n",
      "  \"hits_at_1_percent\": 11.459,\n",
      "  \"recall_at_1_percent\": 0.6866\n",
      "}\n",
      "\n",
      "\n",
      "Metrics of the solution version with the updated schema & dataset:\n",
      "{\n",
      "  \"coverage\": 0.5826,\n",
      "  \"hits_at_1_percent\": 11.67,\n",
      "  \"recall_at_1_percent\": 0.6959\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get metrics for the solution version that used the original schema\n",
    "get_orig_solution_metrics_response = personalize.get_solution_metrics(\n",
    "    solutionVersionArn = solution_version_arn\n",
    ")\n",
    "print(\"Metrics of the solution version with the original schema & dataset:\")\n",
    "print(json.dumps(get_orig_solution_metrics_response['metrics'], indent=2))\n",
    "\n",
    "# Get metrics for the solution version that used the updated schema & dataset\n",
    "get_updated_solution_metrics_response = personalize.get_solution_metrics(\n",
    "    solutionVersionArn = updated_solution_version_arn\n",
    ")\n",
    "print(\"\\n\\nMetrics of the solution version with the updated schema & dataset:\")\n",
    "print(json.dumps(get_updated_solution_metrics_response['metrics'], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the metrics of the two solution versions are slightly different. This can serve as additional confirmation that the new solution version used a different dataset for training.\n",
    "\n",
    "\n",
    "However, the most reliable method of making sure that the new columns are being used by this new solution version requires checking via the Amazon Personalize service page within the AWS Management Console. Visit the `Solutions and recipes` menu option under the `Custom Resources` tab. Select the new solution we just created. Click the `Solution Configuration` dropdown option, and you will be able to see the `Columns for training` that are used by your solution. Recall, for the original solution, we previously observed that the Item dataset's columns for training were: `ITEM_ID`, `PRICE`, and `CATEGORY_L1`.\n",
    "\n",
    "Looking at the updated solution. Notice how the Item dataset columns used for training are: `ITEM_ID`, `PRICE`, `CATEGORY_L1`, `CATEGORY_L2`, `PRODUCT_DESCRIPTION`, and `GENDER`.\n",
    "\n",
    "This is confirmation that the new solution used these additional new columns for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: \n",
    "\n",
    "In this notebook, we walked through the process for updating your item schemas when using Amazon Personalize. Although we used the item-attribute-affinity in our example, this workflow also applies to the other recipe types in Amazon Personalize.\n",
    "\n",
    "This demonstration required some set up. To recap those steps, we:\n",
    "1. Set Up Amazon S3 Bucket, IAM Policies, and IAM Roles\n",
    "2. Fetched and Inspect the Datasets\n",
    "3. Created Schemas and Imported Datasets in Amazon Personalize\n",
    "4. Created an e-commerce custom solution in Amazon Personalize\n",
    "5. Ran a Batch Segmentation Job our Solution Version to obtain a baseline for comparison.\n",
    "\n",
    "To Recap, the steps we performed for updating a schema, we:\n",
    "1. Updated the schema (via the CreateSchema API)\n",
    "2. Updated the dataset that corresponded to the updated schema (via the UpdateDataset API)\n",
    "3. Imported our new data/columns (via the CreateDatasetImportJob API)\n",
    "4. Created new a Solution (via the CreateSolution API).\n",
    "5. Trained a new solution version (via the CreateSolutionVersion API) that uses updated schema and new dataset.\n",
    "6. Submited the batch job to our new solution version.\n",
    "7. Inspected the output of our job & compared it to the output from the original solution version.\n",
    "8. Compared the before-and-after metrics of the two solution versions\n",
    "9. Confirmed the columns for training for each Solution via the AWS Console.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Cleanup\n",
    "\n",
    "This chapter will walk through deleting all of the resources created throughout this notebook.\n",
    "\n",
    "First, we will delete the S3 and IAM resources. \n",
    "\n",
    "Then, we will delete the Amazon Personalize resources.\n",
    "\n",
    "Amazon Personalize resources have to deleted in a specific sequence* to avoid dependency errors.\n",
    "The order in which you should delete resources in Amazon Personalize are: recommenders and campaigns, then solutions, then event trackers, then filters, then datasets and dataset schemas, and finally, the dataset group. \n",
    "\n",
    "To declutter this notebook, we will be leveraging a utility module written in python that provides an orderly delete process for deleting all resources in each dataset group.\n",
    "\n",
    "This section should take about 15 minutes, though most of this time will be spent waiting for the Personalize resources to finish deleting.\n",
    "\n",
    "*: Note, we didn't use some of these resource types (such as recommenders and campaigns) in this notebook. The list is just for your knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emptying and Deleting the S3 bucket\n",
    "\n",
    "NOTE: THE FOLLOWING CODE WILL DELETE ALL OF THE OBJECTS, INCLUDING THE CSVs & BATCH JOB FILES. \n",
    "If you dont want to delete the S3 bucket, DONT run the code block below.\n",
    "\n",
    "Alternatively, if you want to delete the bucket, consider directly downloading the files (eg: via the Console or CLI) to \n",
    "persist your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting batch-job-inputs/item-attribute-affinity-job/job_input.json...\n",
      "Deleting batch-job-inputs/item-attribute-affinity-job/job_input_updated.json...\n",
      "Deleting batch-job-outputs/item-attribute-affinity-job/_CHECK...\n",
      "Deleting batch-job-outputs/item-attribute-affinity-job/job_input.json.out...\n",
      "Deleting batch-job-outputs/item-attribute-affinity-job/job_input_updated.json.out...\n",
      "Deleting interactions.csv...\n",
      "Deleting items.csv...\n",
      "Deleting items_basic.csv...\n",
      "Deleting users.csv...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'VEYHS36PKSZWN6DR',\n",
       "  'HostId': 'uMk1PA58khnv9mONIXEpgSXKyOgXWOrEdsJPZtMp+Ea3N9eWuWNJ8kXBEYHrXyNMgBGnnoSf8nOeiLnApDnFyneYkn3woTCD',\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'uMk1PA58khnv9mONIXEpgSXKyOgXWOrEdsJPZtMp+Ea3N9eWuWNJ8kXBEYHrXyNMgBGnnoSf8nOeiLnApDnFyneYkn3woTCD',\n",
       "   'x-amz-request-id': 'VEYHS36PKSZWN6DR',\n",
       "   'date': 'Fri, 27 Oct 2023 20:38:03 GMT',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List objects in the bucket and delete them\n",
    "objects = s3.list_objects_v2(Bucket=bucket_name)\n",
    "\n",
    "if 'Contents' in objects:\n",
    "    for obj in objects['Contents']:\n",
    "        print(f'Deleting {obj[\"Key\"]}...')\n",
    "        s3.delete_object(Bucket=bucket_name, Key=obj['Key'])\n",
    "\n",
    "# Delete the bucket\n",
    "s3.delete_bucket(Bucket=bucket_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete the IAM Execution Role and Policy\n",
    "\n",
    "Now, lets delete the IAM Policy and IAM Role that we created for the Personalize Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting: PersonalizeRole-85652\n",
      "Deleting: arn:aws:iam::402114309305:policy/PersonalizePolicy-85652\n",
      "Bucket and IAM role and policy deleted successfully!\n"
     ]
    }
   ],
   "source": [
    "iam.detach_role_policy(RoleName=role_name, PolicyArn=policy_arn)\n",
    "\n",
    "print(\"Deleting: \" + role_name)\n",
    "iam.delete_role(RoleName=role_name)\n",
    "\n",
    "print(\"Deleting: \" + policy_arn)\n",
    "iam.delete_policy(PolicyArn=policy_arn)\n",
    "\n",
    "# Check if the IAM role and policy were deleted\n",
    "if role_name in [role['RoleName'] for role in iam.list_roles()['Roles']]:\n",
    "    print('Role was not deleted!')\n",
    "    \n",
    "if policy_arn in [policy['Arn'] for policy in iam.list_policies()['Policies']]:\n",
    "    print('Policy was not deleted!')\n",
    "    \n",
    "print('Bucket and IAM role and policy deleted successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up deletion script\n",
    "\n",
    "Next we will download a helper script that will simplify the cleanup process. If you want to look at the underlying code, the helper script can be found in the [retail-demo-store github repo](https://github.com/aws-samples/retail-demo-store/blob/b80137c6edb2c975c50221fcaba46b6abadd7b99/src/aws-lambda/personalize-pre-create-resources/delete_dataset_groups.py).\n",
    "\n",
    "Note: Under the hood, this script uses the 'delete_*' Personalize API boto3 commands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 18958  100 18958    0     0   119k      0 --:--:-- --:--:-- --:--:--  119k\n"
     ]
    }
   ],
   "source": [
    "# Download the helper script from the github repo.\n",
    "!curl -O https://raw.githubusercontent.com/aws-samples/retail-demo-store/b80137c6edb2c975c50221fcaba46b6abadd7b99/src/aws-lambda/personalize-pre-create-resources/delete_dataset_groups.py\n",
    "\n",
    "# Import the module\n",
    "import delete_dataset_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up logging for deletion of Personalize resources\n",
    "The following code cell ensures we import and set up the native python logging module. Our resource deletion script requires this so that we can provide information about the deletion status of Personalize resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "\n",
    "delete_dataset_groups.logger.setLevel(logging.INFO)\n",
    "delete_dataset_groups.logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Amazon Personalize Resources\n",
    "\n",
    "Now we can delete the active dataset groups. This can take up to 10 minutes depending on the resources within your dataset group. The function below will log its progress until finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active dataset groups that need to be deleted: retaildemostore-products-DSG-85652\n",
      "\n",
      "Dataset Group ARN: arn:aws:personalize:us-east-1:402114309305:dataset-group/retaildemostore-products-DSG-85652\n",
      "All recommenders have been deleted or none exist for dataset group\n",
      "All campaigns have been deleted or none exist for dataset group\n",
      "Deleting solution: arn:aws:personalize:us-east-1:402114309305:solution/my-updated-solution-85652\n",
      "Deleting solution: arn:aws:personalize:us-east-1:402114309305:solution/my-original-solution-85652\n",
      "Waiting for 2 solution(s) to be deleted\n",
      "Waiting for 2 solution(s) to be deleted\n",
      "Waiting for 2 solution(s) to be deleted\n",
      "Waiting for 1 solution(s) to be deleted\n",
      "All solutions have been deleted or none exist for dataset group\n",
      "All event trackers have been deleted or none exist for dataset group\n",
      "All filters have been deleted or none exist for dataset group\n",
      "Deleting dataset arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85652/ITEMS\n",
      "Deleting dataset arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85652/INTERACTIONS\n",
      "Deleting dataset arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85652/USERS\n",
      "Waiting for 3 dataset(s) to be deleted\n",
      "Waiting for 2 dataset(s) to be deleted\n",
      "Waiting for 1 dataset(s) to be deleted\n",
      "All datasets have been deleted or none exist for dataset group\n",
      "Deleting schema arn:aws:personalize:us-east-1:402114309305:schema/import-job-updated-items-schema-85652\n",
      "Deleting schema arn:aws:personalize:us-east-1:402114309305:schema/interactions-schema-85652\n",
      "Deleting schema arn:aws:personalize:us-east-1:402114309305:schema/users-schema-85652\n",
      "All schemas used exclusively by datasets have been deleted or none exist for dataset group\n",
      "Deleting dataset group arn:aws:personalize:us-east-1:402114309305:dataset-group/retaildemostore-products-DSG-85652\n",
      "Waiting for dataset group to be deleted\n",
      "Waiting for dataset group to be deleted\n",
      "Waiting for dataset group to be deleted\n",
      "Dataset group arn:aws:personalize:us-east-1:402114309305:dataset-group/retaildemostore-products-DSG-85652 has been fully deleted\n",
      "Dataset group retaildemostore-products-DSG-85652 fully deleted\n",
      "CPU times: user 178 ms, sys: 25.2 ms, total: 203 ms\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(f'Active dataset groups that need to be deleted: {dataset_group_name}\\n')\n",
    "\n",
    "delete_dataset_groups.delete_dataset_groups(\n",
    "    dataset_group_names = [ dataset_group_name ], \n",
    "    wait_for_resources = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the original item schema\n",
    "# The external helper script doesnt delete obsolete schemas, so we must do this ourselves\n",
    "response = personalize.delete_schema(schemaArn = items_schema_arn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete local files\n",
    "If you want to retain these files, dont run the following cell block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm delete_dataset_groups.py\n",
    "\n",
    "# Delete input & output files from original solution version\n",
    "!rm job_input.json\n",
    "!rm job_input.json.out\n",
    "\n",
    "# Delete input & output files from retrained solution version with new schema & dataset\n",
    "!rm job_input_updated.json\n",
    "!rm job_input_updated.json.out\n",
    "\n",
    "# Delete the basic items dataset csv\n",
    "!rm {items_basic_filename}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Complete\n",
    "\n",
    "All resources created by this Notebook have been deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final note:\n",
    "If you are running this notebook on Amazon Sagemaker, don't forget to `stop` or `terminate` your sagemaker instance so that you don't incur additional costs.\n",
    "Afterwards, feel free to delete the execution role of this Sagemaker notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congrats on completing this Personalize Demonstration! \n",
    "If you are further interested in learning how you can leverage Amazon Personalize to power your business's ML-powered recommendation services, refer to the [AWS Documentation on Amazon Personlize](https://docs.aws.amazon.com/personalize/latest/dg/what-is-personalize.html).\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

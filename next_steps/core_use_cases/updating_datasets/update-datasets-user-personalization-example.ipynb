{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Example for Adding New Items and Interactions for User-Personalization Solutions in Amazon Personalize\n",
    "\n",
    "\n",
    "This notebook explains the process for *updating* your *datasets* when using Amazon Personalize--specifically adding new *items* and *interactions for those new items*. Commentary in this notebook is centered around the *User Personalization* recipe. \n",
    "\n",
    "This notebook is inspired by the official AWS [Retail Demo Store Workshop](https://github.com/aws-samples/retail-demo-store). The main difference between this notebook and the Retail Demo Store workshop, is that the latter mainly focuses on real-time recommendations, whereas this notebook demonstrates how updating your datasets affects user-personalization solutions & item recommendations.\n",
    "\n",
    "\n",
    "## Notebook overview\n",
    "\n",
    "### Core content:\n",
    "\n",
    "Before we can demonstrate schema changes, we will need to do some set-up. This set up portion is documented in the first 5 chapters of this notebook.\n",
    "\n",
    "After we perform the set up, we will then walk through the process of changing your dataset. \n",
    "This is what the remaining chapters go over.\n",
    "\n",
    "Table of Contents:\n",
    "\n",
    "------------Set up------------\n",
    "- **Chapter 1**: Set Up Amazon S3 Bucket, IAM Policies, and IAM Roles (_5 minutes_)\n",
    "- **Chapter 2**: Fetch, Inspect, and Trim the Datasets (_5 minutes_)\n",
    "- **Chapter 3**: Create Schemas and Import Datasets in Amazon Personalize (_15 minutes_)\n",
    "- **Chapter 4**: Create an e-commerce custom solution in Amazon Personalize (_60 minutes_)\n",
    "- **Chapter 5**: Gather Metrics and Run a Batch Inference Job on your Solution Version (_20 minutes_)\n",
    "\n",
    "------------Updating Datasets------------\n",
    "- **Chapter 6**: Overview of the Steps involved for your Updating Datasets for User-Personalization Solutions (_100 minutes_)\n",
    "- **Chapter 6 Step 1**: Fetch, modify, and inspect the updated the Items & Interactions Datasets \n",
    "- **Chapter 6 Step 2**: Update the Items & Interactions Datasets (via the the CreateDatasetImportJob API)\n",
    "- **Chapter 6 Step 3**: Re-Run the Batch Inference Job on your Solution Version to trigger an auto-update\n",
    "- **Chapter 6 Step 4**: Gather metrics of the auto-updated solution version\n",
    "- **Chapter 6 Step 5**: Perform A Full Retraining (Create a Solution Version)\n",
    "- **Chapter 6 Step 6**: Run the Batch Inference Job on the new Solution Version\n",
    "- **Chapter 6 Step 7**: Gather metrics of the fully-retrained solution version\n",
    "\n",
    "\n",
    "------------Analysis------------\n",
    "- **Chapter 7**: Analysis (_5 minutes_)\n",
    "- **Chapter 7 Step 1**: Compare the outputs of the original, auto-updated, and fully retrained solution versions. \n",
    "    - All three should be different.\n",
    "    \n",
    "- **Chapter 7 Step 2**: Compare the metrics across the two solution versions (original, auto-updated, fully-trained)\n",
    "    - Since the solution version was fully retrained with additional data, we should expect the metrics to be slightly different.\n",
    "    \n",
    "------------Clean up------------\n",
    "- **Chapter 8**: Clean up (_15 minutes_)\n",
    "\n",
    "\n",
    "#### Relevant Information:\n",
    "\n",
    "- This notebook was developed and tested in the us-east-1 Region.\n",
    "\n",
    "- To ensure a reliable run, please don't *concurrently* run multiple copies of this notebook on the same Sagemaker Notebook Instance. If you want to concurrently run multiple copies of this notebook, run each notebook in its own environment/instance. \n",
    "\n",
    "- Updating datasets for user-segmentation-based solutions is similar. The difference is, for user-segmentation, a new solution version *must* be trained before changes in the datasets have an impact in your recommendations. This is in contrast to user-personalization-based solutions, where recent interactions and new items are factored in your inference jobs. This is useful for \"cold\"-starting new items. However, to use those new items & interactions beyond cold starts, occasionally training a new solution version is recommended. This will make that new data more impactful in your recommendations for user-personalization solutions.\n",
    "\n",
    "- After you finish running this notebook, please run the code cells in the `Clean up` chapter of this notebook (final chapter). This will prevent incurring additional costs.\n",
    "\n",
    "- The purpose of this notebook is to demonstrate a *high-level* implementation of an end-to-end Personalize Workflow for updating datasets. As such, the code within this notebook has not been tested for a production environment and for the sake of brevity, not all security best practices may have been implemented. For additional information to secure your Personalize-dependent workloads, refer to the [Security in Amazon Personalize](https://docs.aws.amazon.com/personalize/latest/dg/security.html) section of the Amazon Personalize documentation.\n",
    "\n",
    "- This notebook will be using the python programming language and the AWS SDK for python (referred to as boto3). Even if you are not fluent in python, the code cells should be reasonably intuitive. In practice, you can use any programming language supported by the AWS SDK to complete the same steps from this notebook in your application environment. Visit the [official boto3 documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) for more information about the AWS SDK for Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Amazon Personalize\n",
    "\n",
    "[Amazon Personalize](https://aws.amazon.com/personalize/) makes it easy for customers to develop applications with a wide array of personalization use cases, including real time product recommendations and customized direct marketing. Amazon Personalize brings the same machine learning technology used by Amazon.com to everyone for use in their applications â€“ with no machine learning experience required. Amazon Personalize customers pay for what they use, with no minimum fees or upfront commitment. You can start using Amazon Personalize with a simple three step process, which only takes a few clicks in the AWS console, or a set of simple API calls. First, point Amazon Personalize to user data, catalog data, and activity stream of views, clicks, purchases, etc. in Amazon S3 or upload using a simple API call. Second, with a single click in the console or an API call, train a custom private recommendation model for your data. Third, retrieve personalized recommendations for any user by creating a recommender, campaign, or batch job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Set Up Amazon S3 Bucket, IAM Policies, and IAM Roles\n",
    "\n",
    "In this Chapter, we are going to focus setting up our Amazon S3 bucket, and initializing the proper IAM Policies & Roles required to run this workflow.\n",
    "\n",
    "This chapter will take about 5 minutes.\n",
    "\n",
    "### Update dependencies\n",
    "\n",
    "To get started, we need to perform a bit of setup. First, we need to ensure that a current version of botocore is locally installed. The botocore library is used by boto3, the AWS SDK library for python. We need a current version to be able to access some of the newer Amazon Personalize features.\n",
    "\n",
    "The following cell will update pip and install the latest botocore library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (24.0)\n",
      "Collecting botocore\n",
      "  Using cached botocore-1.34.60-py3-none-any.whl.metadata (5.7 kB)\n",
      "Using cached botocore-1.34.60-py3-none-any.whl (12.0 MB)\n",
      "Installing collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.60\n",
      "    Uninstalling botocore-1.34.60:\n",
      "      Successfully uninstalled botocore-1.34.60\n",
      "Successfully installed botocore-1.34.60\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade --no-deps --force-reinstall botocore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "\n",
    "Next we need to import some dependencies/libraries needed to complete this part of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import uuid\n",
    "from botocore.exceptions import ClientError\n",
    "import numpy\n",
    "from io import StringIO\n",
    "import botocore\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create clients\n",
    "\n",
    "Next we need to create the AWS service clients needed for this demonstration.\n",
    "\n",
    "- **personalize**: this client is used to create resources in Amazon Personalize\n",
    "- **s3**: this client is used to access S3 commands and resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup clients\n",
    "personalize = boto3.client('personalize')\n",
    "s3 = boto3.Session().client('s3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up our Amazon S3 bucket\n",
    "\n",
    "For simplicity, we will use this bucket to store our input data, output data, helper scripts, and other files. \n",
    "Though, in a production environment, you may want to store these assets seperately/in seperate buckets.\n",
    "\n",
    "To ensure a consistent naming convention throughout this notebook, we generate a random number for the 'token' variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an epoch timestamp w/ precision to the nearest millisecond to present a pseduo-randomly generated value for token. \n",
    "# Alternatively, enter your own *lowercase alphanumeric* string of 5 characters here. The 'token` is used for naming aws resources. \n",
    "token = str(round(time.time()*1000))[-5:]\n",
    "print(f'The value of your token is:\"{token}\".')\n",
    "\n",
    "# Bucket name *must* contain the substring 'Personalize' or 'personalize'. \n",
    "#  This is to ensure compliance with the execution role of this Sagemaker Notebook instance.\n",
    "bucket_name = 'personalize-update-items-dataset-example-' + token\n",
    "\n",
    "# Creates a bucket in us-east-1\n",
    "# Reference: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/create_bucket.html\n",
    "bucket = s3.create_bucket(\n",
    "    Bucket=bucket_name\n",
    ")\n",
    "\n",
    "# The Bucket Policy we need to attach to the bucket in order to allow Amazon Personalize to access it.\n",
    "bucket_policy = {\n",
    "    'Version': '2012-10-17',\n",
    "    'Id': 'PersonalizeS3BucketAccessPolicy',\n",
    "    'Statement': [\n",
    "        {\n",
    "            'Sid': 'PersonalizeS3BucketAccessPolicy',\n",
    "            'Effect': 'Allow',\n",
    "            'Principal': {\n",
    "                'Service': 'personalize.amazonaws.com'\n",
    "            },\n",
    "            'Action': [\n",
    "                's3:GetObject',\n",
    "                's3:ListBucket',\n",
    "                's3:PutObject'\n",
    "            ],\n",
    "            'Resource': [\n",
    "                f'arn:aws:s3:::{bucket_name}',\n",
    "                f'arn:aws:s3:::{bucket_name}/*'\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the policy to a JSON string and attach it to the bucket\n",
    "bucket_policy = json.dumps(bucket_policy)\n",
    "s3.put_bucket_policy(Bucket=bucket_name, Policy=bucket_policy)\n",
    "\n",
    "\n",
    "# prints out the bucket\n",
    "print('Bucket: {}'.format(bucket['Location']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Amazon IAM Permissions for the Personalize Service\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to a bucket policy that allows Amazon Personalize access, we also need to explicitly grant the Amazon Personalize service those permissions within an IAM Role. This will enable the Personalize service to fetch and write data to Amazon S3. We use a custom-made customer-managed IAM policy to ensure we are abiding by the [Principle of Least Privilege best security practice](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating IAM Role...\n",
      "Created IAM Role. IAM Role ARN: arn:aws:iam::402114309305:role/PersonalizeRole-85968\n",
      "Creating IAM Policy...\n",
      "Created IAM Policy. Policy ARN: arn:aws:iam::402114309305:policy/PersonalizePolicy-85968\n",
      "Attached policy to Role\n"
     ]
    }
   ],
   "source": [
    "# Set up IAM for Personalize\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "# role_name must begin with the substring 'PersonalizeRole' to ensure compliance with the Execution Role of this Sagemaker Notebook instance.\n",
    "role_name = 'PersonalizeRole-'+token\n",
    "\n",
    "print(\"Creating IAM Role...\")\n",
    "role_arn = iam.create_role(\n",
    "    RoleName=role_name,\n",
    "    # Allow Amazon Personalize to assume this role\n",
    "    AssumeRolePolicyDocument=json.dumps({\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}))\n",
    "role_arn = role_arn['Role']['Arn']\n",
    "\n",
    "print(\"Created IAM Role. IAM Role ARN: \" + role_arn)\n",
    "\n",
    "\n",
    "# Create the IAM policy for Personalize\n",
    "personalize_policy_doc = {\n",
    "    'Version': '2012-10-17',\n",
    "    'Id': 'PersonalizeS3BucketAccessPolicy-'+token,\n",
    "    'Statement': [\n",
    "        {\n",
    "            'Sid': 'PersonalizeS3BucketAccessPolicy',\n",
    "            'Action': [\n",
    "                's3:GetObject',\n",
    "                's3:ListBucket',\n",
    "                's3:PutObject'\n",
    "            ],\n",
    "            'Resource': [\n",
    "                f'arn:aws:s3:::{bucket_name}',\n",
    "                f'arn:aws:s3:::{bucket_name}/*'\n",
    "            ],\n",
    "            'Effect': 'Allow'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "personalize_policy_doc = json.dumps(personalize_policy_doc)\n",
    "\n",
    "# role_name must begin with the substring 'PersonalizePolicy' to ensure compliance with the Execution Role of this Sagemaker Notebook instance.\n",
    "iam_policy_name = 'PersonalizePolicy-'+token\n",
    "\n",
    "print(\"Creating IAM Policy...\")\n",
    "policy_response = iam.create_policy(\n",
    "    PolicyName=iam_policy_name,\n",
    "    PolicyDocument=personalize_policy_doc,\n",
    "    Description='Policy to allow Personalize access to our S3 bucket'\n",
    ")\n",
    "\n",
    "# get arn of the policy\n",
    "policy_arn = policy_response['Policy']['Arn']\n",
    "policy_version = policy_response['Policy']['DefaultVersionId']\n",
    "print(\"Created IAM Policy. Policy ARN: \" + policy_arn)\n",
    "\n",
    "# Attach the policy to the role\n",
    "iam.attach_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyArn=policy_arn\n",
    ")\n",
    "print(\"Attached policy to Role\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Fetch and Inspect the Datasets\n",
    "\n",
    "Amazon Personalize provides predefined recipes, based on common use cases, for training models. A recipe is a machine learning algorithm that you use with settings, or hyperparameters, and the data you provide to train an Amazon Personalize model. The data you provide to train a model are organized into separate datasets by the type of data being provided. A collection of datasets are organized into a dataset group. The three dataset types supported by Personalize are items, users, and interactions. Depending on the recipe type you choose, a different combination of dataset types are required. For all recipe types, an interactions dataset is required. Interactions represent how users interact with items. For example, viewing a product, watching a video, listening to a recording, or reading an article. In this notebook, we will be using the user personalization recipe, a recipe that can use all three dataset types.\n",
    "\n",
    "In this chapter, you will:\n",
    "    \n",
    "    - copy public datasets to your private S3 bucket,\n",
    "    - Load the datasets into this notebook environment,\n",
    "    - Inspect the datasets so you have an understanding of the data\n",
    "    \n",
    "This chapter will take about 5 minutes.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some context on 'Items' datasets\n",
    "\n",
    "When training models in Amazon Personalize, we can provide structured and unstructured metadata about our items. This data helps improve the relevance of recommendations and is particularly useful when recommending new/cold items added to your catalog. \n",
    "\n",
    "Optional reading: For this notebook we will be creating 'custom solutions' for our use cases. Additionally, Personalize also has retail domain recommenders. This construct, which was released at re:Invent 2021 is used for real-time inferences. You can read more about them in the [Personalize blog](https://aws.amazon.com/blogs/machine-learning/amazon-personalize-announces-recommenders-optimized-for-retail-and-media-entertainment/).\n",
    "\n",
    "The retail domain recommenders stipulate some [reserved fields/columns](https://docs.aws.amazon.com/personalize/latest/dg/ECOMMERCE-datasets-and-schemas.html) that we must conform to. For example, some columns that Personalize supports for an `Items` dataset include `ITEM_ID`, `PRICE`, `CATEGORY_L1`, `CATEGORY_L2`, `PRODUCT_DESCRIPTION`, and `GENDER`. Personalize will automatically apply a natural language processing (NLP) machine learning model to the product description column to extract features from the text. The product's unique identifier is required. For items, at least one metadata column (such as price or level-1 category) is also required. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to CSV and upload to S3 bucket\n",
    "\n",
    "For this notebook, we will be using publicly available datasets. These datasets are part of the [Retail Demo Store](https://github.com/aws-samples/retail-demo-store) project and are provided as a public download. \n",
    "\n",
    "The following cell will copy the csv datasets from the download URL to the local volume and then upload to our private s3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-12 01:36:06--  https://code.retaildemostore.retail.aws.dev/csvs/users.csv\n",
      "Resolving code.retaildemostore.retail.aws.dev (code.retaildemostore.retail.aws.dev)... 18.165.83.69, 18.165.83.109, 18.165.83.129, ...\n",
      "Connecting to code.retaildemostore.retail.aws.dev (code.retaildemostore.retail.aws.dev)|18.165.83.69|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 58912 (58K) [text/csv]\n",
      "Saving to: â€˜users.csvâ€™\n",
      "\n",
      "100%[======================================>] 58,912      --.-K/s   in 0.001s  \n",
      "\n",
      "2024-03-12 01:36:06 (43.4 MB/s) - â€˜users.csvâ€™ saved [58912/58912]\n",
      "\n",
      "Finishing copying users.csv to personalize-update-items-dataset-example-06196\n",
      "--2024-03-12 01:36:06--  https://code.retaildemostore.retail.aws.dev/csvs/items.csv\n",
      "Resolving code.retaildemostore.retail.aws.dev (code.retaildemostore.retail.aws.dev)... 18.165.83.120, 18.165.83.129, 18.165.83.109, ...\n",
      "Connecting to code.retaildemostore.retail.aws.dev (code.retaildemostore.retail.aws.dev)|18.165.83.120|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 789018 (771K) [text/csv]\n",
      "Saving to: â€˜items.csvâ€™\n",
      "\n",
      "100%[======================================>] 789,018     --.-K/s   in 0.007s  \n",
      "\n",
      "2024-03-12 01:36:06 (102 MB/s) - â€˜items.csvâ€™ saved [789018/789018]\n",
      "\n",
      "Finishing copying items.csv to personalize-update-items-dataset-example-06196\n",
      "--2024-03-12 01:36:07--  https://code.retaildemostore.retail.aws.dev/csvs/interactions.csv\n",
      "Resolving code.retaildemostore.retail.aws.dev (code.retaildemostore.retail.aws.dev)... 18.165.83.69, 18.165.83.120, 18.165.83.129, ...\n",
      "Connecting to code.retaildemostore.retail.aws.dev (code.retaildemostore.retail.aws.dev)|18.165.83.69|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42447843 (40M) [text/csv]\n",
      "Saving to: â€˜interactions.csvâ€™\n",
      "\n",
      "100%[======================================>] 42,447,843   210MB/s   in 0.2s   \n",
      "\n",
      "2024-03-12 01:36:07 (210 MB/s) - â€˜interactions.csvâ€™ saved [42447843/42447843]\n",
      "\n",
      "Finishing copying interactions.csv to personalize-update-items-dataset-example-06196\n"
     ]
    }
   ],
   "source": [
    "users_filename, items_filename, interactions_filename = \"users.csv\", \"items.csv\", \"interactions.csv\"\n",
    "\n",
    "# copy the datasets from the public s3 bucket to our private s3 bucket\n",
    "for file in [users_filename, items_filename, interactions_filename]:\n",
    "    !wget https://code.retaildemostore.retail.aws.dev/csvs/{file}\n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(file).upload_file(file)\n",
    "    print(f'Finishing copying {file} to {bucket_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will download our datasets from our private s3 bucket into this notebook environment, and load them into a [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n",
    "\n",
    "Finally, we will display the first few rows of each dataset just so we have a sense of its dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Users:-----------\n",
      "   USER_ID  AGE GENDER\n",
      "0        1   31      M\n",
      "1        2   58      F\n",
      "2        3   43      M\n",
      "3        4   38      M\n",
      "4        5   24      M\n",
      "\n",
      "Items:-----------\n",
      "                                ITEM_ID   PRICE  CATEGORY_L1 CATEGORY_L2  \\\n",
      "0  6579c22f-be2b-444c-a52b-0116dd82df6c   90.99  accessories    backpack   \n",
      "1  2e852905-c6f4-47db-802c-654013571922  123.99  accessories    backpack   \n",
      "2  4ec7ff5c-f70f-4984-b6c4-c7ef37cc0c09   87.99  accessories    backpack   \n",
      "3  7977f680-2cf7-457d-8f4d-afa0aa168cb9  125.99  accessories    backpack   \n",
      "4  b5649d7c-4651-458d-a07f-912f253784ce  141.99  accessories    backpack   \n",
      "\n",
      "                                 PRODUCT_DESCRIPTION GENDER PROMOTED  \n",
      "0           This tan backpack is nifty for traveling      F        N  \n",
      "1                       Pale pink backpack for women      F        N  \n",
      "2  This gainsboro backpack for women is first-rat...      F        N  \n",
      "3  This gray backpack for women is first-rate for...      F        N  \n",
      "4                     Peru-orange backpack for women      F        N  \n",
      "\n",
      "Interactions:-----------\n",
      "                                ITEM_ID USER_ID EVENT_TYPE   TIMESTAMP  \\\n",
      "0  b93b7b15-9bb3-407c-b80b-517e7c45e090    3156       View  1690552936   \n",
      "1  b93b7b15-9bb3-407c-b80b-517e7c45e090    3156       View  1690552941   \n",
      "2  3946f4c8-1b5b-4161-b794-70b33affb671    2122       View  1690552959   \n",
      "3  3946f4c8-1b5b-4161-b794-70b33affb671    2122       View  1690552969   \n",
      "4  e9daa7cd-8230-4544-9f07-86fa84d7c3c1    2485       View  1690552979   \n",
      "\n",
      "  DISCOUNT  \n",
      "0       No  \n",
      "1       No  \n",
      "2       No  \n",
      "3       No  \n",
      "4       No  \n"
     ]
    }
   ],
   "source": [
    "# Users Dataset\n",
    "get_users_csv_response = s3.get_object(Bucket=bucket_name, Key=users_filename)\n",
    "users_csv_content = get_users_csv_response['Body'].read().decode('utf-8')\n",
    "\n",
    "# Create a pandas DataFrame from the CSV content\n",
    "users_df = pd.read_csv(StringIO(users_csv_content))\n",
    "print('\\nUsers:-----------')\n",
    "print(users_df.head())  # Inspect the first few rows of the DataFrame\n",
    "\n",
    "\n",
    "# Items Dataset\n",
    "get_items_csv_response = s3.get_object(Bucket=bucket_name, Key=items_filename)\n",
    "items_csv_content = get_items_csv_response['Body'].read().decode('utf-8')\n",
    "\n",
    "# Create a pandas DataFrame from the CSV content\n",
    "items_df = pd.read_csv(StringIO(items_csv_content))\n",
    "print('\\nItems:-----------')\n",
    "print(items_df.head())  # Inspect the first few rows of the DataFrame\n",
    "\n",
    "\n",
    "# Interactions Dataset\n",
    "get_interactions_csv_response = s3.get_object(Bucket=bucket_name, Key=interactions_filename)\n",
    "interactions_csv_content = get_interactions_csv_response['Body'].read().decode('utf-8')\n",
    "\n",
    "# Create a pandas DataFrame from the CSV content\n",
    "interactions_df = pd.read_csv(StringIO(interactions_csv_content))\n",
    "\n",
    "interactions_df['USER_ID'] = interactions_df.USER_ID.astype(str)\n",
    "interactions_df['TIMESTAMP'] = interactions_df.TIMESTAMP.astype(int)\n",
    "print('\\nInteractions:-----------')\n",
    "print(interactions_df.head())  # Inspect the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional reading: Inspection of our user & interactions input data\n",
    "\n",
    "Similar to the items dataset, we have provided metadata on our users when training models in Personalize. For this notebook we have included each user's age and gender. For more information about requirements for the users dataset, refer to the [aws documentation](https://docs.aws.amazon.com/personalize/latest/dg/ECOMMERCE-users-dataset.html).\n",
    "\n",
    "\n",
    "Additionally, take a look at the first few lines of the interactions file. Note: \n",
    "\n",
    "- An EVENT_TYPE column which can be used to train different Personalize campaigns & custom solutions, and can also be used to filter on recommendations. To simulate a real-world site, most of the EVENT_TYPE events are views, whereas a much smaller proportion is add to cart, checkout, and purchase events.\n",
    "- The custom DISCOUNT column which is a contextual metadata field, that a Personalize user personalization solution can take into account to predict on the best next product based the user's propensity to interact with discount products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim down the Items and Interactions dataset\n",
    "\n",
    "Now, we will modify the `items dataset` and `interactions dataset` to remove (at random) half of the items, and all interactions associated with those items. Then, we will create a solution version using these trimmed down datasets. In the second part of the notebook, we will simulate updating the datasets by using the full-version of the datasets with all of the items and interactions.\n",
    "\n",
    "Specifically, we will:\n",
    "- randomly select half the ItemIDs from the original dataset,\n",
    "- remove rows associated with ItemIDs from the Items dataset,\n",
    "- From the interactions dataset, remove all interactions associated with our randomly chosen ItemIDs.\n",
    "- These writes will be performed on a *copy* of the orginal datasets. These `trimmed` CSVs will be uploaded to s3. We will then use these trimmed CSVs when we create our first solution version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim the items dataset, upload it to S3, and preview it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique ITEM_ID values: 2465.\n",
      "\n",
      "Number of ITEM_ID values to remove: 1232.\n",
      "\n",
      "Removing the following ITEM_ID values... : ['6579c22f-be2b-444c-a52b-0116dd82df6c'\n",
      " '2e852905-c6f4-47db-802c-654013571922'\n",
      " '4ec7ff5c-f70f-4984-b6c4-c7ef37cc0c09' ...\n",
      " '575c0ac0-5494-4c64-a886-a9c0cf8b779a'\n",
      " '7000f6e7-41f7-4957-878a-ccc42a39ca59'\n",
      " '9c1a2048-7aac-4565-b836-d8d4f726322c'].\n",
      "\n",
      "Copying trimmed Items records to items_trimmed.csv...\n",
      "\n",
      "File 'items_trimmed.csv' has been uploaded to S3 bucket 'personalize-update-items-dataset-example-85968' with key 'items_trimmed.csv'.\n",
      "\n",
      " Trimmed Items dataset preview:-----------\n",
      "                                 ITEM_ID   PRICE  CATEGORY_L1 CATEGORY_L2  \\\n",
      "0   6579c22f-be2b-444c-a52b-0116dd82df6c   90.99  accessories    backpack   \n",
      "4   b5649d7c-4651-458d-a07f-912f253784ce  141.99  accessories    backpack   \n",
      "5   296d144e-7f86-464b-9c5a-f545257f1700  144.99  accessories    backpack   \n",
      "12  0c47dade-1ec0-483a-9ab4-1b87604bdaf8  106.99  accessories    backpack   \n",
      "13  f995ec8d-237c-4513-8bfa-9aee210f097c  122.99  accessories    backpack   \n",
      "\n",
      "                                  PRODUCT_DESCRIPTION GENDER PROMOTED  \n",
      "0            This tan backpack is nifty for traveling      F        N  \n",
      "4                      Peru-orange backpack for women      F        N  \n",
      "5   This black backpack for women is first-class f...      F        N  \n",
      "12                            Pink backpack for women      F        Y  \n",
      "13  This dark olive green backpack for women is ou...      F        N  \n",
      "\n",
      "Number of unique ITEM_ID values in 'items_trimmed.csv': 1233\n"
     ]
    }
   ],
   "source": [
    "# Trim Items Dataset:\n",
    "\n",
    "items_trimmed_filename = 'items_trimmed.csv'\n",
    "\n",
    "# Obtain the unique ITEM_ID values\n",
    "unique_item_ids = items_df['ITEM_ID'].unique()\n",
    "print(f\"Number of unique ITEM_ID values: {len(unique_item_ids)}.\\n\")\n",
    "\n",
    "num_of_ItemIDs_to_remove = int(0.5 * len(unique_item_ids))\n",
    "print(f\"Number of ITEM_ID values to remove: {num_of_ItemIDs_to_remove}.\\n\")\n",
    "\n",
    "# Randomly select 50% of the unique \"ITEM_ID\" values (these represent new ITEM_IDs during the dataset update portion of the notebook)\n",
    "selected_item_ids = numpy.random.choice(unique_item_ids, num_of_ItemIDs_to_remove, replace=False)\n",
    "print(f\"Removing the following ITEM_ID values... : {unique_item_ids}.\\n\")\n",
    "\n",
    "\n",
    "# Create a new DataFrame with rows where \"ITEM_ID\" is not in the selected_item_ids\n",
    "items_trimmed_df = items_df[~items_df[\"ITEM_ID\"].isin(selected_item_ids)]\n",
    "\n",
    "# Write the updated data to items_trimmed_filename\n",
    "print(f\"Copying trimmed Items records to {items_trimmed_filename}...\\n\")\n",
    "items_trimmed_df.to_csv(items_trimmed_filename, index=False)\n",
    "\n",
    "# Upload the modified CSV file to an S3 bucket\n",
    "s3.upload_file(items_trimmed_filename, bucket_name, items_trimmed_filename)\n",
    "\n",
    "print(f\"File '{items_trimmed_filename}' has been uploaded to S3 bucket '{bucket_name}' with key '{items_trimmed_filename}'.\")\n",
    "\n",
    "print('\\n Trimmed Items dataset preview:-----------')\n",
    "print(items_trimmed_df.head())  # Inspect the first few rows of the DataFrame\n",
    "\n",
    "# Validate: Number of unique ITEM_ID values in the trimmed csv should be half of what it was before\n",
    "print(f\"\\nNumber of unique ITEM_ID values in '{items_trimmed_filename}': {len(items_trimmed_df['ITEM_ID'].unique())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim the Interactions dataset, upload it to S3, and preview it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed rows with 'ITEM_ID' matching selected_item_ids and wrote the filtered data to interactions_trimmed.csv.\n",
      "\n",
      "File 'interactions_trimmed.csv' has been uploaded to S3 bucket 'personalize-update-items-dataset-example-85968' with key 'interactions_trimmed.csv'.\n",
      "\n",
      "Number of rows in interactions.csv: 675004 \n",
      "Number of rows in interactions_trimmed.csv: 328442 \n",
      "\n",
      " Trimmed Interactions dataset preview:-----------\n",
      "                                 ITEM_ID USER_ID EVENT_TYPE   TIMESTAMP  \\\n",
      "2   3946f4c8-1b5b-4161-b794-70b33affb671    2122       View  1690552959   \n",
      "3   3946f4c8-1b5b-4161-b794-70b33affb671    2122       View  1690552969   \n",
      "4   e9daa7cd-8230-4544-9f07-86fa84d7c3c1    2485       View  1690552979   \n",
      "5   e9daa7cd-8230-4544-9f07-86fa84d7c3c1    2485       View  1690552994   \n",
      "12  e7af1dbd-4ab2-4201-b70f-1a52e4ea9250     810       View  1690553072   \n",
      "\n",
      "   DISCOUNT  \n",
      "2        No  \n",
      "3        No  \n",
      "4        No  \n",
      "5        No  \n",
      "12       No  \n"
     ]
    }
   ],
   "source": [
    "# Trim Interactions dataset:\n",
    "\n",
    "interactions_trimmed_filename = 'interactions_trimmed.csv'\n",
    "\n",
    "# Create a new DataFrame without rows where \"ITEM_ID\" is in selected_item_ids\n",
    "interactions_trimmed_df = interactions_df[~interactions_df['ITEM_ID'].isin(selected_item_ids)]\n",
    "\n",
    "# Write the trimmed interactions data to a new CSV file\n",
    "interactions_trimmed_df.to_csv(interactions_trimmed_filename, index=False)\n",
    "\n",
    "print(f\"Removed rows with 'ITEM_ID' matching selected_item_ids and wrote the filtered data to {interactions_trimmed_filename}.\\n\")\n",
    "\n",
    "# Upload the modified CSV file to an S3 bucket\n",
    "s3.upload_file(interactions_trimmed_filename, bucket_name, interactions_trimmed_filename)\n",
    "\n",
    "print(f\"File '{interactions_trimmed_filename}' has been uploaded to S3 bucket '{bucket_name}' with key '{interactions_trimmed_filename}'.\")\n",
    "\n",
    "# Validate: Compare the number of rows in interactions.csv & interactions_trimmed.csv. \n",
    "# The latter should have fewer rows (approximately half) than the original interactions csv.\n",
    "print(f\"\\nNumber of rows in {interactions_filename}: {len(interactions_df)} \")\n",
    "print(f\"Number of rows in {interactions_trimmed_filename}: {len(interactions_trimmed_df)} \")\n",
    "\n",
    "print('\\n Trimmed Interactions dataset preview:-----------')\n",
    "print(interactions_trimmed_df.head())  # Inspect the first few rows of the DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 Summary - What have we accomplished?\n",
    "\n",
    "In this chapter, we fetched pre-prepared sample datasets for each dataset type (items, users, and interactions) and uploaded them to the Amazon S3 bucket for later use.\n",
    "\n",
    "We then inspected the three dataset types that will be used to train models and create custom solutions in Amazon Personalize.\n",
    "\n",
    "Finally, we trimmed the items and interactions dataset so simulate a baseline dataset (In the latter half of this notebook, we will use the trimmed out items and interactions as our *new* data).\n",
    "\n",
    "In the next chapter we will start creating resources in Amazon Personalize to receive our dataset files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Create Schemas and Import Datasets into Amazon Personalize\n",
    "\n",
    "\n",
    "In this chapter we are going to create an Amazon Personalize dataset group and import our three datasets into Amazon Personalize.\n",
    "\n",
    "## Chapter 3 Objectives\n",
    "\n",
    "In this chapter we will accomplish the following steps. This chapter should take about 15 minutes to complete.\n",
    "\n",
    "- Create schema resources in Amazon Personalize that define the layout of our three dataset files (CSVs) created in the prior chapter\n",
    "- Create a dataset group in Amazon Personalize that will be used to receive our datasets\n",
    "- Create a dataset in the Personalize dataset group for the three dataset types and schemas\n",
    "    - Items: information about the products in the Retail Demo Store\n",
    "    - Users: information about the users in the Retail Deme Store\n",
    "    - Interactions: user-item interactions representing typical storefront behavior such as viewing products, adding products to a shopping cart, purchasing products, and so on\n",
    "- Create dataset import jobs to import each of the three datasets into Personalize\n",
    "\n",
    "Note: We will be using the trimmed versions of the items and interactions datasets here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Amazon Personalize\n",
    "\n",
    "Now that we've prepared our three datasets and uploaded them to S3 we'll need to configure the Amazon Personalize service to understand our data so that it can be used to train models for generating recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Schemas for Datasets\n",
    "\n",
    "Amazon Personalize requires a schema for each dataset so it can map the columns in our CSVs to fields for model training. Each schema is declared in JSON using the [Apache Avro](https://avro.apache.org/) format.\n",
    "\n",
    "Let's define and create schemas in Personalize for our datasets.\n",
    "\n",
    "Note that categorical fields include an additional attribute of `\"categorical\": true` and the textual field has an additional attribute of `\"textual\": true`. Categorical fields are those where one or more values can be specified for the field value (i.e. enumerated values). For example, one or more category names/codes for the `CATEGORY_L1` field. A textual field indicates that Personalize should apply a natural language processing (NLP) model to the field's value to extract model features from unstructured text. In this case, we're using the product description as the textual field. You can only have one textual field in the items dataset. Finally, you will notice that the `PROMOTED` field does _not_ have `categorical` or `textual` specified. In this case, the `PROMOTED` column will not be included as a feature in the model but can be used for filtering (out of scope of this notebook).\n",
    "\n",
    "Another detail to note is that when we call the [CreateSchema](https://docs.aws.amazon.com/personalize/latest/dg/API_CreateSchema.html) API, we pass an optional `domain` parameter with a value of `ECOMMERCE`. This tells Personalize that we are creating a schema for Retail/E-commerce domain. We will do this for all three schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Users Dataset Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:us-east-1:402114309305:schema/retaildemostore-products-users-schema-85968\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"816766fd-6e8b-456c-88c8-1659794d109c\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 15:34:53 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"109\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"816766fd-6e8b-456c-88c8-1659794d109c\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "users_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Users\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"AGE\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENDER\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True,\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    users_schema_name = 'retaildemostore-products-users-schema-'+token\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = users_schema_name,\n",
    "        domain = \"ECOMMERCE\",\n",
    "        schema = json.dumps(users_schema)\n",
    "    )\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "    users_schema_arn = create_schema_response['schemaArn']\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this schema, seemingly')\n",
    "    paginator = personalize.get_paginator('list_schemas')\n",
    "    for paginate_result in paginator.paginate():\n",
    "        for schema in paginate_result['schemas']:\n",
    "            if schema['name'] == users_schema_name:\n",
    "                users_schema_arn = schema['schemaArn']\n",
    "                print(f\"Using existing schema: {users_schema_arn}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Items Datsaset Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:us-east-1:402114309305:schema/retaildemostore-products-items-schema-85968\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"92f02ed4-41d3-4d6c-9970-d402353f0feb\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 15:34:53 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"109\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"92f02ed4-41d3-4d6c-9970-d402353f0feb\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "items_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Items\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PRICE\",\n",
    "            \"type\": \"float\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CATEGORY_L1\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True,\n",
    "        },\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    items_schema_name = 'retaildemostore-products-items-schema-'+token\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = items_schema_name,\n",
    "        domain = 'ECOMMERCE',\n",
    "        schema = json.dumps(items_schema)\n",
    "    )\n",
    "    items_schema_arn = create_schema_response['schemaArn']\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this schema, seemingly')\n",
    "    paginator = personalize.get_paginator('list_schemas')\n",
    "    for paginate_result in paginator.paginate():\n",
    "        for schema in paginate_result['schemas']:\n",
    "            if schema['name'] == items_schema_name:\n",
    "                items_schema_arn = schema['schemaArn']\n",
    "                print(f\"Using existing schema: {items_schema_arn}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactions Dataset Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:us-east-1:402114309305:schema/retaildemostore-products-interactions-schema-85968\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"3f22e3fd-704e-4be9-95a7-c4c7e4049a1e\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 15:34:53 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"116\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"3f22e3fd-704e-4be9-95a7-c4c7e4049a1e\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "interactions_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"EVENT_TYPE\",  # \"View\", \"Purchase\", etc.\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DISCOUNT\",  # This is the contextual metadata - \"Yes\" or \"No\".\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    interactions_schema_name = 'retaildemostore-products-interactions-schema-'+token\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = interactions_schema_name,\n",
    "        domain = \"ECOMMERCE\",\n",
    "        schema = json.dumps(interactions_schema)\n",
    "    )\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "    interactions_schema_arn = create_schema_response['schemaArn']\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this schema, seemingly')\n",
    "    paginator = personalize.get_paginator('list_schemas')\n",
    "    for paginate_result in paginator.paginate():\n",
    "        for schema in paginate_result['schemas']:\n",
    "            if schema['name'] == interactions_schema_name:\n",
    "                interactions_schema_arn = schema['schemaArn']\n",
    "                print(f\"Using existing schema: {interactions_schema_arn}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Dataset Group\n",
    "\n",
    "Next we need to create the dataset group that will contain our three datasets. This is one of many Personalize operations that are asynchronous. That is, we call an API to create a resource and have to wait for it to become active."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset Group\n",
    "\n",
    "Note that we are also passing `ECOMMERCE` for the `domain` parameter here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetGroupArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset-group/retaildemostore-products-DSG-85968\",\n",
      "  \"domain\": \"ECOMMERCE\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"6dd03be3-95fb-45a4-995e-84472dd344e8\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 15:34:53 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"134\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"6dd03be3-95fb-45a4-995e-84472dd344e8\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "DatasetGroupArn = arn:aws:personalize:us-east-1:402114309305:dataset-group/retaildemostore-products-DSG-85968\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset_group_name = 'retaildemostore-products-DSG-'+token\n",
    "    create_dataset_group_response = personalize.create_dataset_group(\n",
    "        name = dataset_group_name,\n",
    "        domain = 'ECOMMERCE'\n",
    "    )\n",
    "    dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "    print(json.dumps(create_dataset_group_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this dataset group, seemingly')\n",
    "    paginator = personalize.get_paginator('list_dataset_groups')\n",
    "    for paginate_result in paginator.paginate():\n",
    "        for dataset_group in paginate_result['datasetGroups']:\n",
    "            if dataset_group['name'] == dataset_group_name:\n",
    "                dataset_group_arn = dataset_group['datasetGroupArn']\n",
    "                break\n",
    "                \n",
    "print(f'DatasetGroupArn = {dataset_group_arn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Group to Have ACTIVE Status\n",
    "This should take about a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetGroup: CREATE PENDING\n",
      "DatasetGroup: CREATE PENDING\n",
      "DatasetGroup: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the three Datasets in Personalize\n",
    "Next we will create the datasets in Personalize for our three dataset types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Users Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85968/USERS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"c43ee3ba-cd86-4120-9001-b7e66d4e69da\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 15:35:23 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"108\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"c43ee3ba-cd86-4120-9001-b7e66d4e69da\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "Users dataset ARN = arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85968/USERS\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset_type = \"USERS\"\n",
    "    users_dataset_name = \"retaildemostore-products-users-ds-\"+token\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = users_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        schemaArn = users_schema_arn\n",
    "    )\n",
    "\n",
    "    users_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this dataset, seemingly')\n",
    "    paginator = personalize.get_paginator('list_datasets')\n",
    "    for paginate_result in paginator.paginate(datasetGroupArn = dataset_group_arn):\n",
    "        for dataset in paginate_result['datasets']:\n",
    "            if dataset['name'] == users_dataset_name:\n",
    "                users_dataset_arn = dataset['datasetArn']\n",
    "                break\n",
    "                \n",
    "print(f'Users dataset ARN = {users_dataset_arn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Items Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85968/ITEMS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"bb3cead9-afd2-447a-a4b5-3424fbf221f8\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 15:35:23 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"108\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"bb3cead9-afd2-447a-a4b5-3424fbf221f8\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "Items dataset ARN = arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85968/ITEMS\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset_type = \"ITEMS\"\n",
    "    items_dataset_name = \"retaildemostore-products-items-ds-\"+token\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = items_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        schemaArn = items_schema_arn\n",
    "    )\n",
    "\n",
    "    items_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this dataset, seemingly')\n",
    "    paginator = personalize.get_paginator('list_datasets')\n",
    "    for paginate_result in paginator.paginate(datasetGroupArn = dataset_group_arn):\n",
    "        for dataset in paginate_result['datasets']:\n",
    "            if dataset['name'] == items_dataset_name:\n",
    "                items_dataset_arn = dataset['datasetArn']\n",
    "                break\n",
    "                \n",
    "print(f'Items dataset ARN = {items_dataset_arn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Interactions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85968/INTERACTIONS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"1b23cf9b-508b-406b-8151-547a89892fff\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 15:35:23 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"115\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"1b23cf9b-508b-406b-8151-547a89892fff\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "Interactions dataset ARN = arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85968/INTERACTIONS\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset_type = \"INTERACTIONS\"\n",
    "    interactions_dataset_name = \"retaildemostore-products-interactions-ds-\"+token\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = interactions_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        schemaArn = interactions_schema_arn\n",
    "    )\n",
    "\n",
    "    interactions_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this dataset, seemingly')\n",
    "    paginator = personalize.get_paginator('list_datasets')\n",
    "    for paginate_result in paginator.paginate(datasetGroupArn = dataset_group_arn):\n",
    "        for dataset in paginate_result['datasets']:\n",
    "            if dataset['name'] == interactions_dataset_name:\n",
    "                interactions_dataset_arn = dataset['datasetArn']\n",
    "                break\n",
    "                \n",
    "print(f'Interactions dataset ARN = {interactions_dataset_arn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for datasets to become active\n",
    "\n",
    "It can take a minute for the datasets to be created. Let's wait for all three to become active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one dataset is still in progress\n",
      "At least one dataset is still in progress\n",
      "Dataset arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85968/INTERACTIONS successfully completed\n",
      "Dataset arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85968/USERS successfully completed\n",
      "Dataset arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85968/ITEMS successfully completed\n",
      "All datasets have completed\n",
      "CPU times: user 25.3 ms, sys: 1.32 ms, total: 26.6 ms\n",
      "Wall time: 30.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset_arns = [ items_dataset_arn, users_dataset_arn, interactions_dataset_arn ]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    for dataset_arn in reversed(dataset_arns):\n",
    "        response = personalize.describe_dataset(\n",
    "            datasetArn = dataset_arn\n",
    "        )\n",
    "        status = response[\"dataset\"][\"status\"]\n",
    "\n",
    "        if status == \"ACTIVE\":\n",
    "            print(f'Dataset {dataset_arn} successfully completed')\n",
    "            dataset_arns.remove(dataset_arn)\n",
    "        elif status == \"CREATE FAILED\":\n",
    "            print(f'Dataset {dataset_arn} failed')\n",
    "            if response['dataset'].get('failureReason'):\n",
    "                print('   Reason: ' + response['dataset']['failureReason'])\n",
    "            dataset_arns.remove(dataset_arn)\n",
    "\n",
    "    if len(dataset_arns) > 0:\n",
    "        print('At least one dataset is still in progress')\n",
    "        time.sleep(15)\n",
    "    else:\n",
    "        print(\"All datasets have completed\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Datasets to Personalize\n",
    "\n",
    "So far in this chapter we have created schemas in Personalize that define the columns in our CSVs. Then we created a dataset group and three datasets in Personalize that will receive our data. In the following steps we will create import jobs with Personalize that will import the datasets from our S3 bucket into our dataset group. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Import Jobs\n",
    "\n",
    "With the permissions in place to allow Personalize to access our CSV files, let's create three import jobs to import each file into its respective dataset. Each import job can take roughly 10 minutes to complete so we'll start the import jobs and then wait for them all to complete. This allows these import jobs to run in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Users Dataset Import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset-import-job/retaildemostore-products-users-899a5333\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"a2a18d02-7368-491d-ae52-55d4e62c763f\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 15:35:54 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"127\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"a2a18d02-7368-491d-ae52-55d4e62c763f\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import_job_suffix = str(uuid.uuid4())[:8]\n",
    "\n",
    "users_create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"retaildemostore-products-users-\" + import_job_suffix,\n",
    "    datasetArn = users_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, users_filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "users_dataset_import_job_arn = users_create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(users_create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Items Dataset Import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset-import-job/retaildemostore-products-items-899a5333\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"b04addd2-87ca-449d-adb5-510676e24038\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 15:35:54 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"127\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"b04addd2-87ca-449d-adb5-510676e24038\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "items_create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"retaildemostore-products-items-\" + import_job_suffix,\n",
    "    datasetArn = items_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, items_trimmed_filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "items_dataset_import_job_arn = items_create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(items_create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Interactions Dataset Import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset-import-job/retaildemostore-products-interactions-899a5333\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"ab7a2040-9748-4624-a814-7c56657b6ccf\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 15:35:54 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"134\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"ab7a2040-9748-4624-a814-7c56657b6ccf\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "interactions_create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"retaildemostore-products-interactions-\" + import_job_suffix,\n",
    "    datasetArn = interactions_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, interactions_trimmed_filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "interactions_dataset_import_job_arn = interactions_create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(interactions_create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for Import Jobs to Complete\n",
    "\n",
    "It can take up to 10 minutes for the import jobs to complete, while you're waiting you can learn more about Datasets and Schemas here: https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html\n",
    "\n",
    "We will wait for all three import jobs to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Items Import Job to Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one dataset import job still in progress\n",
      "At least one dataset import job still in progress\n",
      "At least one dataset import job still in progress\n",
      "At least one dataset import job still in progress\n",
      "Import job arn:aws:personalize:us-east-1:402114309305:dataset-import-job/retaildemostore-products-interactions-899a5333 successfully completed\n",
      "At least one dataset import job still in progress\n",
      "At least one dataset import job still in progress\n",
      "Import job arn:aws:personalize:us-east-1:402114309305:dataset-import-job/retaildemostore-products-items-899a5333 successfully completed\n",
      "Import job arn:aws:personalize:us-east-1:402114309305:dataset-import-job/retaildemostore-products-users-899a5333 successfully completed\n",
      "All import jobs have ended\n",
      "CPU times: user 183 ms, sys: 4.76 ms, total: 188 ms\n",
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import_job_arns = [ users_dataset_import_job_arn, items_dataset_import_job_arn, interactions_dataset_import_job_arn ]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    for job_arn in reversed(import_job_arns):\n",
    "        import_job_response = personalize.describe_dataset_import_job(\n",
    "            datasetImportJobArn = job_arn\n",
    "        )\n",
    "        status = import_job_response[\"datasetImportJob\"]['status']\n",
    "\n",
    "        if status == \"ACTIVE\":\n",
    "            print(f'Import job {job_arn} successfully completed')\n",
    "            import_job_arns.remove(job_arn)\n",
    "        elif status == \"CREATE FAILED\":\n",
    "            print(f'Import job {job_arn} failed')\n",
    "            if import_job_response[\"datasetImportJob\"].get('failureReason'):\n",
    "                print('   Reason: ' + import_job_response[\"datasetImportJob\"]['failureReason'])\n",
    "            import_job_arns.remove(job_arn)\n",
    "\n",
    "    if len(import_job_arns) > 0:\n",
    "        print('At least one dataset import job still in progress')\n",
    "        time.sleep(60)\n",
    "    else:\n",
    "        print(\"All import jobs have ended\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 Summary - What have we accomplished?\n",
    "\n",
    "In this chapter we created schemas in Amazon Personalize that mapped to the dataset CSVs we introduced in chapter 2. We also created a dataset group in Personalize as well as Datasets to represent our CSVs. Finally, we created dataset import jobs in Personalize to load the three datasets into Personalize.\n",
    "\n",
    "In the next chapter we will create the a custom solution and train a solution version. This is where the machine learning models are trained and deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Create a custom solution in Amazon Personalize\n",
    "\n",
    "In this chapter we are going to create a Solution in Amazon Personalize. A Solution consists of a Personalize Recipe (an algorithm), parameters, and all of its Solution Versions (ie: trained models). \n",
    "\n",
    "## Chapter 4 Objectives\n",
    "\n",
    "In this chapter we will accomplish the following steps.\n",
    "\n",
    "- Create custom solution and solution version for the following use case:\n",
    "    - **User Personalization**: An item recommendation model that recommends specific items to your users.\n",
    "\n",
    "This portion should take about 60 minutes to complete. However, most of the time will be waiting for model training job to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Custom Solution\n",
    "\n",
    "With our three datasets imported into our dataset group, we can now turn to creating solutions. \n",
    "\n",
    "We simply need to create a solution and solution version using the user-personalization recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of the recipe\n",
    "\n",
    "[User-Personalization:](https://docs.aws.amazon.com/personalize/latest/dg/native-recipe-new-item-USER_PERSONALIZATION.html)\n",
    "\n",
    "> The User-Personalization (aws-user-personalization) recipe is optimized for all personalized recommendation scenarios. It predicts the items that a user will interact with based on Interactions, Items, and Users datasets. When recommending items, it uses automatic item exploration.\n",
    "\n",
    "> With exploration, recommendations include some items that would be typically less likely to be recommended for the user, such as new items, items with few interactions, or items less relevant for the user based on their previous behavior. This improves item discovery and engagement when you have a fast-changing catalog, or when new items, such as news articles or promotions, are more relevant to users when fresh.\n",
    "\n",
    "\n",
    "\n",
    "Note: This notebook only uses one recipe, however there many more than that available. If you are interested, you can visit the official documentation to read more about all the [predefined recipes](https://docs.aws.amazon.com/personalize/latest/dg/working-with-predefined-recipes.html) Amazon Personalize has to offer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item Recommendation Recipe:\n",
    "user_personalization_recipe_arn = 'arn:aws:personalize:::recipe/aws-user-personalization'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Solution and Solution Version\n",
    "\n",
    "With our recipe defined, we can now create our solution and solution version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below creates a solution using the user-personalization recipe and our dataset group that we created in the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"solutionArn\": \"arn:aws:personalize:us-east-1:402114309305:solution/retaildemostore-user-personalization-solution-85968\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"dcfe7878-eb97-42f7-a350-b1328d15ba56\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 15:41:56 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"121\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"dcfe7878-eb97-42f7-a350-b1328d15ba56\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_personalization_solution_name = \"retaildemostore-user-personalization-solution-\"+token\n",
    "user_personalization_solution_arn = None\n",
    "user_personalization_solution_version_arn = None\n",
    "\n",
    "try:\n",
    "    create_solution_response = personalize.create_solution(\n",
    "        name = user_personalization_solution_name,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        recipeArn = user_personalization_recipe_arn\n",
    "    )\n",
    "\n",
    "    user_personalization_solution_arn = create_solution_response['solutionArn']\n",
    "    print(json.dumps(create_solution_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You aready created this solution, seemingly')\n",
    "    paginator = personalize.get_paginator('list_solutions')\n",
    "    for paginate_result in paginator.paginate(datasetGroupArn = dataset_group_arn):\n",
    "        for solution in paginate_result['solutions']:\n",
    "            if solution['name'] == user_personalization_solution_name:\n",
    "                user_personalization_solution_arn = solution['solutionArn']\n",
    "                print(f'Ranking solution ARN = {user_personalization_solution_arn}')\n",
    "                \n",
    "                response = personalize.list_solution_versions(\n",
    "                    solutionArn = user_personalization_solution_arn,\n",
    "                    maxResults = 100\n",
    "                )\n",
    "                if len(response['solutionVersions']) > 0:\n",
    "                    user_personalization_solution_version_arn = response['solutionVersions'][-1]['solutionVersionArn']\n",
    "                    print(f'Will use most recent solution version for this solution: {user_personalization_solution_version_arn}')\n",
    "                    \n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create User Personalization Solution Version\n",
    "\n",
    "Next we can create a solution version for the solution. This is where the model is trained for this custom solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"solutionVersionArn\": \"arn:aws:personalize:us-east-1:402114309305:solution/retaildemostore-user-personalization-solution-85968/83cc95fc\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"2a3a328d-5b88-49e7-a7dc-27628f323951\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 15:41:56 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"137\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"2a3a328d-5b88-49e7-a7dc-27628f323951\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if not user_personalization_solution_version_arn:\n",
    "    create_solution_version_response = personalize.create_solution_version(\n",
    "        solutionArn = user_personalization_solution_arn\n",
    "    )\n",
    "\n",
    "    user_personalization_solution_version_arn = create_solution_version_response['solutionVersionArn']\n",
    "    print(json.dumps(create_solution_version_response, indent=2))\n",
    "else:\n",
    "    print(f'Solution version {user_personalization_solution_version_arn} already exists; not creating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for Solution Versions to Complete\n",
    "\n",
    "It can take roughly 40 minutes for the solution version to be created. During this process a model is being trained and tested with the data contained within your datasets. The duration of training jobs can increase based on the size of the dataset, training parameters and a selected recipe. In the cell below we will wait for the solution version to finish.\n",
    "\n",
    "While you are waiting for this process to complete you can learn more about [custom solutions](https://docs.aws.amazon.com/personalize/latest/dg/training-deploying-solutions.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for the custom solution version to become active\n",
    "\n",
    "The following cell waits for the solution version for the user personalization use case to become active. We *need* to make sure it is active before proceeding to the next Chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version is still in progress\n",
      "Solution version arn:aws:personalize:us-east-1:402114309305:solution/retaildemostore-user-personalization-solution-85968/83cc95fc successfully completed\n",
      "{\n",
      "  \"solutionVersion\": {\n",
      "    \"name\": \"retaildemostore-user-personalization-solution-85968/83cc95fc\",\n",
      "    \"solutionVersionArn\": \"arn:aws:personalize:us-east-1:402114309305:solution/retaildemostore-user-personalization-solution-85968/83cc95fc\",\n",
      "    \"solutionArn\": \"arn:aws:personalize:us-east-1:402114309305:solution/retaildemostore-user-personalization-solution-85968\",\n",
      "    \"performHPO\": false,\n",
      "    \"performAutoML\": false,\n",
      "    \"recipeArn\": \"arn:aws:personalize:::recipe/aws-user-personalization\",\n",
      "    \"datasetGroupArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset-group/retaildemostore-products-DSG-85968\",\n",
      "    \"solutionConfig\": {},\n",
      "    \"trainingHours\": 1.073,\n",
      "    \"trainingMode\": \"FULL\",\n",
      "    \"status\": \"ACTIVE\",\n",
      "    \"creationDateTime\": \"2023-10-27 15:41:56.283000+00:00\",\n",
      "    \"lastUpdatedDateTime\": \"2023-10-27 16:02:04.818000+00:00\"\n",
      "  },\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"692287ea-13ac-47ce-aeb0-e7cec25e17b2\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 16:02:59 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"750\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"692287ea-13ac-47ce-aeb0-e7cec25e17b2\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "Solution version has completed\n",
      "CPU times: user 540 ms, sys: 23 ms, total: 563 ms\n",
      "Wall time: 21min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "user_personalization_solution_version_arn\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    soln_ver_response = personalize.describe_solution_version(\n",
    "        solutionVersionArn = user_personalization_solution_version_arn\n",
    "    )\n",
    "    status = soln_ver_response[\"solutionVersion\"][\"status\"]\n",
    "\n",
    "    if status == \"ACTIVE\":\n",
    "        print(f'Solution version {user_personalization_solution_version_arn} successfully completed')\n",
    "        print(json.dumps(soln_ver_response, indent=2, default=str))\n",
    "        print(\"Solution version has completed\")\n",
    "        break\n",
    "    elif status == \"CREATE FAILED\":\n",
    "        print(f'Solution version {user_personalization_solution_version_arn} failed')\n",
    "        if soln_ver_response[\"solutionVersion\"].get('failureReason'):\n",
    "            print('   Reason: ' + soln_ver_response[\"solutionVersion\"]['failureReason'])\n",
    "        break\n",
    "    else:\n",
    "        print('Solution version is still in progress')\n",
    "        time.sleep(60)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 Summary - What have we accomplished?\n",
    "\n",
    "In this chapter we created a solution using the user-personalization recipe. We also created (or trained) a solution version.\n",
    "\n",
    "In the next chapter we will perform a batch job on the newly-trained solution version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Run A Batch Segmentation Job on your Solution Version\n",
    "\n",
    "In this chapter, we will prepare and execute a batch job for the solution version that we created previously. The purpose of doing this is to obtain a baseline that we can use to compare the to the output of the solution version that uses the updated datasets later on in this notebook.\n",
    "\n",
    "The batch job for the user personalization solution will return a group of items for each of the inputted users. These items will represent the items that the user is most likely to purchase.\n",
    "\n",
    "We will wait for the job to finish executing. Afterwards, we'll inspect the outputs.\n",
    "\n",
    "This chapter will take about 20 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare some file names that we will use throughout the rest of this notebook.\n",
    "note: 'orig' is shorthard for 'original', 'au' is short for 'auto-updated', and 'fr' is short for 'fully retrained'.\n",
    "Since are using the same input for all jobs, the contents of these files will be identical, \n",
    "but we need to create seperate copies of the input files due to the API behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_job_input_filename = \"orig_job_input.json\"\n",
    "au_job_input_filename = \"au_job_input.json\"\n",
    "fr_job_input_filename = \"fr_job_input.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the size input and outputs for our job\n",
    "\n",
    "If you want, you can set the size of the input job by changing the value of the variable 'x' in the following code cell.\n",
    "\n",
    "A default value of 5 has been pre-populated for you. If the value of x equal 5, this means we will randomly select 5 `USER_ID` values as input for the user personalization job.\n",
    "\n",
    "Similarly, you can set the size of the output by changing the value of the variable 'y'. A default value of 10 has been pre-populated for you. This means our model will return 10 `ITEM_ID` recommendations for each `USER_ID` input.\n",
    "\n",
    "You can decrease or increase the values of 'x' and 'y' if you want. Just be aware that larger values for x and y means the inference job will take slightly longer to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the size of the input and output variables\n",
    "x = 5\n",
    "y = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input file for batch inference job\n",
    "\n",
    "Next we will prepare an input file for our batch inference job.\n",
    "\n",
    "The input file for batch jobs against User Personalization Solutions requires a list of userIds. For each user, our solution version will return a list of personalized item recommendations for that user.\n",
    "Along with each item recommendation, our model will also return a score that represents their liklihood of interacting with the item.\n",
    "\n",
    "Below is a sample of the input file for a user personalization job that builds item recommendations for 3 users.\n",
    "\n",
    "```javascript\n",
    "{\"userId\": \"4\"}\n",
    "{\"userId\": \"5\"}\n",
    "{\"userId\": \"6\"}\n",
    "```\n",
    "\n",
    "For our job, we will *randomly* select *'x'* UserIDs that we want item recommendations for.\n",
    "\n",
    "#### Here are the steps required to run this job:\n",
    "    - Randomly select x user id's (used as inputs for our batch job)\n",
    "    - Generate the input file\n",
    "    - Upload it to S3\n",
    "    - Create and start a Batch Inference Job using our Solution Version\n",
    "    - Wait for the inference Job to complete\n",
    "    - Download the inference job output from S3\n",
    "    - Inspect the job output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below selects the users, generate the input file for the batch job, and uploads it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected users :\n",
      "[5023 5866 4016 2755 2896]\n",
      "\n",
      "Previewing input file... \n",
      "{\"userId\": \"5023\"}\n",
      "{\"userId\": \"5866\"}\n",
      "{\"userId\": \"4016\"}\n",
      "{\"userId\": \"2755\"}\n",
      "{\"userId\": \"2896\"}\n",
      "\n",
      "\n",
      "File was uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# - Randomly select x users\n",
    "users = users_df['USER_ID'].unique()\n",
    "sample_users = numpy.random.choice(users, x, False)\n",
    "\n",
    "print(\"Randomly selected users :\")\n",
    "print(sample_users)\n",
    "\n",
    "# - Generate the input file\n",
    "with open(orig_job_input_filename, 'w') as json_input:\n",
    "    for user_id in sample_users:\n",
    "        # Write line that specifies the specific user\n",
    "        json_input.write(f'{{\"userId\": \"{user_id}\"}}\\n')\n",
    "\n",
    "# Confirm the file matches the required format:\n",
    "print(\"\\nPreviewing input file... \")\n",
    "!head -n 5 $orig_job_input_filename\n",
    "print('\\n')\n",
    "\n",
    "# Upload job input file to S3\n",
    "s3_input_key_orig = \"batch-job-input/\" + orig_job_input_filename\n",
    "\n",
    "s3.upload_file(orig_job_input_filename, bucket_name, s3_input_key_orig)\n",
    "if s3_input_key_orig in [object['Key'] for object in s3.list_objects(Bucket=bucket_name)['Contents']]:\n",
    "    print('File was uploaded successfully!')\n",
    "else:\n",
    "    print('File was not uploaded!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the batch inference job\n",
    "\n",
    "Finally, we're ready to submit a batch inference job. There are several required parameters including a name for the job, the solution version ARN for the user-personalization model, the IAM role that Personalize needs to be able to access the job input file and write the output file, and the job input and output locations. These parameters are required inputs for all batch jobs in Amazon Personalize.\n",
    "\n",
    "We're also optionally specifying that we only want *'y'* item recommendations per user.\n",
    "\n",
    "The inference job can take several minutes to complete. Even though our input file only specifies a few input lines, there is a certain amount of fixed overhead required for Personalize to spin up the compute resources needed to execute the job. This overhead is amortized for larger input files that generate many item recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"batchInferenceJobArn\": \"arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-orig-85968\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"14a32661-ef8f-4fa9-9030-7f0cffae3982\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 16:02:59 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"141\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"14a32661-ef8f-4fa9-9030-7f0cffae3982\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create and submit a Batch Segment Job using our latest Solution Version\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize/client/create_batch_inference_job.html\n",
    "\n",
    "# We need to define an input location in our s3 bucket where the segmention job gets its input, \n",
    "# and an output location where the segmentation job writes its output.\n",
    "s3_input_path_orig = \"s3://\" + bucket_name + \"/\" + s3_input_key_orig\n",
    "s3_output_path_orig = \"s3://\" + bucket_name + \"/batch-job-outputs/original/\"\n",
    "\n",
    "response = personalize.create_batch_inference_job (\n",
    "    solutionVersionArn = user_personalization_solution_version_arn,\n",
    "    jobName = \"retaildemostore-user-personalization-job-orig-\" + token,\n",
    "    roleArn = role_arn,\n",
    "    jobInput = {\"s3DataSource\": {\"path\": s3_input_path_orig }},\n",
    "    jobOutput = {\"s3DataDestination\":{\"path\": s3_output_path_orig }},\n",
    "    numResults = y\n",
    ")\n",
    "user_personalization_job_arn = response['batchInferenceJobArn']\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather metrics of the solution version\n",
    "\n",
    "Amazon Personalize provides [offline metrics](https://docs.aws.amazon.com/personalize/latest/dg/working-with-training-metrics.html#working-with-training-metrics-metrics) that allow you to evaluate the accuracy of the model before you deploy the it in your application. Metrics can also be used to view the effects of modifying a custom solution's hyperparameters or to compare the metrics between solution versions.\n",
    "\n",
    "While we are waiting for the job to complete, let's quickly grab the metrics of our solution version.\n",
    "\n",
    "We will use these metrics as a baseline to compare the metrics of the future solution versions to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"coverage\": 0.9968,\n",
      "  \"mean_reciprocal_rank_at_25\": 0.8459,\n",
      "  \"normalized_discounted_cumulative_gain_at_10\": 0.7873,\n",
      "  \"normalized_discounted_cumulative_gain_at_25\": 0.8089,\n",
      "  \"normalized_discounted_cumulative_gain_at_5\": 0.7661,\n",
      "  \"precision_at_10\": 0.1194,\n",
      "  \"precision_at_25\": 0.0541,\n",
      "  \"precision_at_5\": 0.2189\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "get_original_solution_metrics_response = personalize.get_solution_metrics(\n",
    "    solutionVersionArn = user_personalization_solution_version_arn\n",
    ")\n",
    "\n",
    "print(json.dumps(get_original_solution_metrics_response['metrics'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the Job to complete and inspect its output\n",
    "\n",
    "Run the cell below. The cell below will wait for the user personalization batch inference job to finish, download the output, and display its first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference Job Started on:  04:02:59 PM\n",
      "DatasetInferenceJob: CREATE PENDING\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: ACTIVE\n",
      "Inference Job Completed on:  04:16:01 PM\n",
      "{\"input\":{\"userId\":\"4016\"},\"output\":{\"recommendedItems\":[\"ccdf737c-c4fd-4c78-abd2-d5ef0428ef20\",\"425cc876-3935-4e87-ad8d-77f42b0b6a75\",\"6be08307-1ec0-44dc-b436-5d489a8010e8\",\"2c1b34d6-0f3d-463d-be76-226cb87bdc6d\",\"89c4eeb4-c146-4434-a9f1-6943b4b552dc\",\"5a94b7d5-b210-44b3-9287-c8b0b5488a15\",\"61b1ad14-4e70-4029-ba55-d17bbf4ab62b\",\"8f8f015a-4166-4e9e-ac0b-6d980614ca5d\",\"eecbee28-73a3-425d-84e8-516c326e399c\",\"1daacea7-7d46-464a-8326-ed81951fecab\"],\"scores\":[0.8449224,0.0186943,0.0148416,0.0100115,0.008885,0.0055925,0.0055068,0.0053057,0.0040184,0.0038556]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"2755\"},\"output\":{\"recommendedItems\":[\"3630053e-3962-4549-bcce-402c3a980557\",\"6d488475-1d67-4076-96b1-8e706709a847\",\"b947ee58-a7e7-40bf-9926-42a445f3480f\",\"90ccfbb9-4538-4951-af8d-4f728578b237\",\"d537d92a-23fe-4673-a697-795652ff10c8\",\"78080d05-b078-441f-b245-54b2a2dec872\",\"61840d6a-6ba2-4ece-a644-6db6a3377b1c\",\"2e95f6fc-6be7-46cf-9e50-8c35313c2768\",\"b630250c-41f3-4f14-865c-c1dc12e448ac\",\"d2d8147f-0f24-42c3-bcbe-a232bab7e94d\"],\"scores\":[0.5338279,0.0922309,0.0577862,0.0209987,0.0206773,0.0148014,0.0141705,0.0129197,0.0120157,0.0077951]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"5866\"},\"output\":{\"recommendedItems\":[\"5afced84-ed2d-4520-a06d-dcfeab382e52\",\"6cc0deb8-4a56-4148-a2ab-677277522c80\",\"e1146e90-3274-4ad6-a6a2-0170f0f8d597\",\"575c0ac0-5494-4c64-a886-a9c0cf8b779a\",\"24c62ad2-6977-4f69-be75-e37d897c1434\",\"4496471c-b098-4915-9a1a-8b9e60043737\",\"9c1a2048-7aac-4565-b836-d8d4f726322c\",\"ccb407b1-7620-4303-8521-fea86c51f503\",\"8cd7ffe0-a8a6-45b1-8d1f-bf731c9cd17b\",\"aa564ee3-67ef-4428-8ad9-fe785a0fff63\"],\"scores\":[0.5773515,0.2774695,0.1156833,0.0155479,0.0066222,0.0059881,3.773E-4,7.17E-5,6.78E-5,3.1E-5]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"2896\"},\"output\":{\"recommendedItems\":[\"1dd4c2da-d174-43b1-8d40-fadc666c26c9\",\"e99c24df-ebe9-429e-8c69-cd80132b87b3\",\"e06f53cd-7776-41ce-9f7a-a88986192e24\",\"75cb828e-ccc5-41ff-9bdd-9ac3dc7740aa\",\"153b2374-36e3-466c-b08c-1078b839cd9b\",\"2f2995da-4768-478a-a4ae-906b76d8c6fe\",\"2a0a5c7b-ca68-4abf-9798-18ffb706832b\",\"e8e48eb7-0b66-4087-b280-1c3a62804a5c\",\"e5816ea5-3ce2-4b86-9530-9b221b357b43\",\"17ab5081-8414-4cab-9003-033ec02b44da\"],\"scores\":[0.6146742,0.0403633,0.0256617,0.0107649,0.0107413,0.0106864,0.0100017,0.007999,0.0077459,0.0065279]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"5023\"},\"output\":{\"recommendedItems\":[\"c6dd0909-46f3-4cf9-a059-fbdff93198dd\",\"441c2a65-4b68-4864-b014-04a9bd9fe08a\",\"84d6c26d-9760-49d8-854b-0a22becd8241\",\"9257351d-59f7-481a-86c4-30dea451afa2\",\"635be5a7-3345-46f9-aa0d-419a6652b0f2\",\"72ae72f3-e7f0-4f03-b8eb-12e78c77741d\",\"d4cf35dd-b543-4b4f-9efb-c2de473c3fed\",\"4994caee-f0b7-4ce8-a4df-d542ce1d9bda\",\"0e3eb8f1-8f23-41fd-9f45-8e7747a5eb37\",\"fe96a096-a0b6-4b20-a332-e11db6c0c7b0\"],\"scores\":[0.3460568,0.2430023,0.1881302,0.025838,0.023981,0.0178926,0.0130894,0.0092487,0.0080664,0.0077484]},\"error\":null}\n",
      "CPU times: user 366 ms, sys: 32.5 ms, total: 399 ms\n",
      "Wall time: 13min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "current_time = datetime.now()\n",
    "print(\"\\nInference Job Started on: \", current_time.strftime(\"%I:%M:%S %p\"))\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "\n",
    "while time.time() < max_time:\n",
    "    response = personalize.describe_batch_inference_job(\n",
    "        batchInferenceJobArn = user_personalization_job_arn\n",
    "    )\n",
    "    status = response[\"batchInferenceJob\"]['status']\n",
    "    print(\"DatasetInferenceJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "    \n",
    "current_time = datetime.now()\n",
    "print(\"Inference Job Completed on: \", current_time.strftime(\"%I:%M:%S %p\"))\n",
    "\n",
    "# - Download the Inference job output from S3\n",
    "job_output_file_orig = orig_job_input_filename + \".out\"\n",
    "export_name_orig = 'batch-job-outputs/original/' + job_output_file_orig\n",
    "s3.download_file(bucket_name, export_name_orig, job_output_file_orig)\n",
    "\n",
    "# - Inspect the Inference Job\n",
    "!head -n 5 $job_output_file_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the input userId is echoed in the output file but we also have `output` and `error` elements for each inference. The `output` element has a `recommendedItems` array that contains the Item IDs for the inference. If there were any errors enountered while generating an inference, details will be included in the `error` element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 5 Summary:\n",
    "\n",
    "In this chapter, we ran a batch inference job for our original solution version.\n",
    "\n",
    "We waited for the inference job to finish and then inspected its outputs. We also gathered the metrics of the solution version for future comparison.\n",
    "\n",
    "#### Potential Next Steps (out of scope of this notebook)\n",
    "Now that we have these item recommendations created, what can we do with them? The most obvious choice is to use these outputs in outbound marketing tools! \n",
    "\n",
    "For example, you can send marketing emails to users containing information about the items they are most likely to purchase (You would use the user personalization model for this). This might be useful if you are seeking to maximize sales or number of purchases. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5 complete\n",
    "\n",
    "Congratulations! You have completed the batch segmentation portion of the notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: Overview of Steps Involved for Updating Datasets for User-Personalization Solutions\n",
    "\n",
    "Suppose we want to update the data in our datasets. This could mean adding new items or capturing new interactions. Fortunately, Amazon Personalize supports adding new rows to your datasets, whether its new Items, new interactions, or new users!\n",
    "\n",
    "In fact, for user-personalization-backed solutions, [Amazon Personalize manages some aspects of dataset updates for you](https://docs.aws.amazon.com/personalize/latest/dg/native-recipe-new-item-USER_PERSONALIZATION.html#automatic-updates)!\n",
    "\n",
    ">  For batch item recommendations, Amazon Personalize *updates* the solution version you specify in the batch inference job when the solution version is the latest for your solution.\n",
    "\n",
    "> With each update, Amazon Personalize *updates* the solution version to consider any *new items* through *exploration*. And it uses any new *interactions* data, including impressions data, to determine *what items to include or not include in exploration*. This is *not* a full retraining; you should still train a new solution version weekly with trainingMode set to FULL so the model can learn from your users' behavior and any item metadata.\n",
    "\n",
    "> There is no cost for automatic updates.\n",
    "\n",
    "(italics added above for emphasis)\n",
    "\n",
    "This chapter will guide you through the process of updating your datasets. \n",
    "\n",
    "This entire chapter will take about 100 minutes, but most of that time will be spent waiting for data import jobs and solution version training.\n",
    "\n",
    "\n",
    "Note: For demonstration purposes, this notebook only reviews updates to items dataset and interactions dataset. Adding new users is a supported feature in Amazon Personalize, but isn't shown in this notebook. For more information how adding new users, refer to [this aws documentation](https://docs.aws.amazon.com/personalize/latest/dg/getting-batch-recommendations.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is a high-level overview of this chapter.\n",
    "Due to its length, this chapter is sub-divided into steps. \n",
    "\n",
    "- **Chapter 6 Step 1**: First, we will fetch, modify, and inspect the updated the Items & Interactions Datasets \n",
    "\n",
    "- **Chapter 6 Step 2**: Next, we will update the Items & Interactions Datasets (via the the CreateDatasetImportJob API)\n",
    "\n",
    "- **Chapter 6 Step 3**: Then, we will re-run the Batch Inference Job on our Solution Version & inpect the updated output. \n",
    "    - When we submit the batch job, our original solution version will automatically update. It will now be able to recommend those new items as 'cold' items. We will inspect the output of the original solution version (Output A) and the output of the auto-updated solution version (Output B). We should expect some slight differences between the outputs.\n",
    "\n",
    "- **Chapter 6 Step 4**: Gather metrics of the auto-updated solution version. \n",
    "    - We will fetch the metrics of the solution version to see if they have changed. Since the underlying solution version was only updated (but not retrained), we should expect the metrics to remain the same.\n",
    "\n",
    "- **Chapter 6 Step 5**: Fully retrain the solution versoin\n",
    "    - If we want our new Items and Interactions data to have more influence in our solution version, then we will need to *fully retrain* the solution version. The full retraining will allow our solution version to use the new items and new interactions at part of its *meaningful* recommendations for inference jobs.\n",
    "\n",
    "- **Chapter 6 Step 6**: Run the Batch Inference Job on the fully re-trained Solution Version & Inspect its output (Output C).\n",
    "\n",
    "- **Chapter 6 Step 7**: Gather metrics of the solution version. We will use this in the analysis portion of the notebook.\n",
    "\n",
    "Afterwards, in Chapter 7, we will compare the outputs and metrics of the original, auto-updated, and fully-retrained solution versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: First, we will fetch, modify, and inspect the updated the Items & Interactions Datasets\n",
    "\n",
    "To make the before-and-after comparisons easier for us, we will slightly modify the original items and interactions datasets. All of the *new* ITEM_ID values that we will import into Amazon Personalize will contain the string \"new_\" as a prefix. This way, you will be able to see whether a specific item recommendation is a *new* item or an *original* item. \n",
    "\n",
    "Run the code cell below. Due to the amount of data processing, it should take about a minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previewing the updated Items dataset:\n",
      "                                    ITEM_ID   PRICE  CATEGORY_L1 CATEGORY_L2  \\\n",
      "0      6579c22f-be2b-444c-a52b-0116dd82df6c   90.99  accessories    backpack   \n",
      "1  new_2e852905-c6f4-47db-802c-654013571922  123.99  accessories    backpack   \n",
      "2  new_4ec7ff5c-f70f-4984-b6c4-c7ef37cc0c09   87.99  accessories    backpack   \n",
      "3  new_7977f680-2cf7-457d-8f4d-afa0aa168cb9  125.99  accessories    backpack   \n",
      "4      b5649d7c-4651-458d-a07f-912f253784ce  141.99  accessories    backpack   \n",
      "5      296d144e-7f86-464b-9c5a-f545257f1700  144.99  accessories    backpack   \n",
      "6  new_7d3e7f5b-8ac8-49a9-a960-8a24773a8280  133.99  accessories    backpack   \n",
      "7  new_1d3ae532-f790-44ca-a8e8-f55aa9b66526   75.99  accessories    backpack   \n",
      "8  new_f6cd5dd2-d3ea-4858-844a-04879153e459   95.99  accessories    backpack   \n",
      "9  new_3491deff-c0fe-4065-abbc-72b507da84b2   80.99  accessories    backpack   \n",
      "\n",
      "                                 PRODUCT_DESCRIPTION GENDER PROMOTED  \n",
      "0           This tan backpack is nifty for traveling      F        N  \n",
      "1                       Pale pink backpack for women      F        N  \n",
      "2  This gainsboro backpack for women is first-rat...      F        N  \n",
      "3  This gray backpack for women is first-rate for...      F        N  \n",
      "4                     Peru-orange backpack for women      F        N  \n",
      "5  This black backpack for women is first-class f...      F        N  \n",
      "6                    Saddle brown backpack for women      F        N  \n",
      "7  This purple backpack for women is flawless for...      F        N  \n",
      "8  This black backpack for women is first-class f...      F        N  \n",
      "9                         Unicorn backpack for women      F        N  \n",
      "\n",
      "\n",
      "Previewing the updated Interactions dataset:\n",
      "                                    ITEM_ID USER_ID EVENT_TYPE   TIMESTAMP\n",
      "0  new_b93b7b15-9bb3-407c-b80b-517e7c45e090    3156       View  1690552936\n",
      "1  new_b93b7b15-9bb3-407c-b80b-517e7c45e090    3156       View  1690552941\n",
      "2      3946f4c8-1b5b-4161-b794-70b33affb671    2122       View  1690552959\n",
      "3      3946f4c8-1b5b-4161-b794-70b33affb671    2122       View  1690552969\n",
      "4      e9daa7cd-8230-4544-9f07-86fa84d7c3c1    2485       View  1690552979\n",
      "5      e9daa7cd-8230-4544-9f07-86fa84d7c3c1    2485       View  1690552994\n",
      "6  new_514cbd7d-71d8-4b75-8a67-448f40bab8e4    1790       View  1690553000\n",
      "7  new_514cbd7d-71d8-4b75-8a67-448f40bab8e4    1790       View  1690553009\n",
      "8  new_74c3fd9d-73b0-44ce-8bc5-202c03a2e8a6     777       View  1690553022\n",
      "9  new_74c3fd9d-73b0-44ce-8bc5-202c03a2e8a6     777       View  1690553037\n"
     ]
    }
   ],
   "source": [
    "# Helper function: Adds the prefix \"new_\" to new ITEM_ID values.\n",
    "# A new ITEM_ID is an ITEM_ID value that was removed from the original dataset.\n",
    "# Recall how we kept these in the selected_item_ids list.\n",
    "def add_prefix(item_id):\n",
    "    if item_id in selected_item_ids:\n",
    "        return 'new_' + item_id\n",
    "    else:\n",
    "        return item_id\n",
    "\n",
    "    \n",
    "# Modify & Inspect Items Datasets\n",
    "items_df['ITEM_ID'] = items_df['ITEM_ID'].apply(add_prefix) # Apply the function to the 'ITEM_ID' column\n",
    "items_df.to_csv('items.csv', index=False) # Save dataframe as csv\n",
    "print(\"Previewing the updated Items dataset:\")\n",
    "print(items_df.head(10)) # Inspect the updated dataset. Notice how some of the ITEM_ID values begin with \"new_\". \n",
    "# These are the new ITEM_ID values that we will import into Amazon Personalize when we update the datasets in the next step.\n",
    "\n",
    "\n",
    "# Modify & Inspect Interactions Datasets\n",
    "interactions_df['ITEM_ID'] = interactions_df['ITEM_ID'].apply(add_prefix)\n",
    "interactions_df.to_csv('interactions.csv', index=False)\n",
    "print(\"\\n\\nPreviewing the updated Interactions dataset:\")\n",
    "print(interactions_df[['ITEM_ID', 'USER_ID', 'EVENT_TYPE','TIMESTAMP']].head(10)) # Display only the most important rows for visual purposes\n",
    "# !head -n 10 $interactions_filename # If you want to view the file itself\n",
    "\n",
    "# Upload items.csv & interactions.csv to S3\n",
    "s3.upload_file(items_filename, bucket_name, items_filename)\n",
    "s3.upload_file(interactions_filename, bucket_name, interactions_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how some of the ITEM_ID values begin with the string \"new_\".\n",
    "These represent the new items that we will be adding to our dataset.\n",
    "The ITEM_ID values that do *not* begin with \"new_\" are old items from the original datasets.\n",
    "\n",
    "Likewise, we updated the interactions dataset such that the ITEM_IDs of all interactions associated with new ITEM_ID values also begin with \"new_\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Update the new Items and interactions data\n",
    "\n",
    "We will simulate an update of the items dataset and interactions dataset up importing the complete version of the datasets. Recall that we stored a copy of these datasets in S3, and that they contain *all* of the items and interactions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset-import-job/updated_items_dataset_import_job\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"54efcc34-ec16-4e93-acef-d0a63165c831\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 16:16:27 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"120\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"54efcc34-ec16-4e93-acef-d0a63165c831\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:402114309305:dataset-import-job/updated_interactions_dataset_import_job\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"9e2b983d-5f86-49c8-b4b8-5c007b48807e\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 16:16:27 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"127\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"9e2b983d-5f86-49c8-b4b8-5c007b48807e\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "At least one dataset import job still in progress\n",
      "At least one dataset import job still in progress\n",
      "At least one dataset import job still in progress\n",
      "At least one dataset import job still in progress\n",
      "At least one dataset import job still in progress\n",
      "At least one dataset import job still in progress\n",
      "Import job arn:aws:personalize:us-east-1:402114309305:dataset-import-job/updated_items_dataset_import_job successfully completed\n",
      "Import job arn:aws:personalize:us-east-1:402114309305:dataset-import-job/updated_items_dataset_import_job successfully completed\n",
      "All import jobs have ended\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Update the new Items and Interactions Satasets\n",
    "\n",
    "# You can use the CreateDatasetImportJob API call to import the data.\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize/client/create_dataset_import_job.html\n",
    "\n",
    "# Import the complete items dataset\n",
    "updated_items_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"updated_items_dataset_import_job\"+token,\n",
    "    datasetArn = items_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, items_filename) # Pass in the new dataset. See note below\n",
    "    },\n",
    "    roleArn = role_arn,\n",
    "    importMode = \"FULL\" # importMode='FULL' is used here because we are re-importing the entire dataset. Though if we had imported *just* the new data, we could have set ImportMode to 'INCREMENTAL'.\n",
    ")\n",
    "# Note: notice how we are passing the orginal items dataset (the dataset that contains all the items)\n",
    "\n",
    "updated_items_dataset_import_job_arn = updated_items_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(updated_items_dataset_import_job_response, indent=2))\n",
    "\n",
    "\n",
    "# Import the complete interactions dataset\n",
    "updated_interactions_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"updated_interactions_dataset_import_job\"+token,\n",
    "    datasetArn = interactions_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, interactions_filename) # Pass in the csv that contains all of the interactions.\n",
    "    },\n",
    "    roleArn = role_arn,\n",
    "    importMode = \"FULL\"\n",
    ")\n",
    "\n",
    "updated_interactions_dataset_import_job_arn = updated_items_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(updated_interactions_dataset_import_job_response, indent=2))\n",
    "\n",
    "\n",
    "import_job_arns = [updated_items_dataset_import_job_arn, updated_interactions_dataset_import_job_arn]\n",
    "\n",
    "# Wait for the data to finish importing. It can take up to 10 minutes.\n",
    "max_time = time.time() + 1*60*60 # 1 hours\n",
    "\n",
    "while time.time() < max_time:\n",
    "    for job_arn in reversed(import_job_arns):\n",
    "        import_job_response = personalize.describe_dataset_import_job(\n",
    "            datasetImportJobArn = job_arn\n",
    "        )\n",
    "        status = import_job_response[\"datasetImportJob\"]['status']\n",
    "\n",
    "        if status == \"ACTIVE\":\n",
    "            print(f'Import job {job_arn} successfully completed')\n",
    "            import_job_arns.remove(job_arn)\n",
    "        elif status == \"CREATE FAILED\":\n",
    "            print(f'Import job {job_arn} failed')\n",
    "            if import_job_response[\"datasetImportJob\"].get('failureReason'):\n",
    "                print('   Reason: ' + import_job_response[\"datasetImportJob\"]['failureReason'])\n",
    "            import_job_arns.remove(job_arn)\n",
    "    if len(import_job_arns) > 0: # CREATE PENDING or CREATE IN_PROGRESS\n",
    "        print('At least one dataset import job still in progress')\n",
    "        time.sleep(60)\n",
    "    else:\n",
    "        print(\"All import jobs have ended\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter 6 Step 3: Re-submit the Batch Inference Job on the solution version\n",
    "\n",
    "Now, we will resubmit the same batch job that we submitted in Chapter 5.\n",
    "Because we are using the User-personalization recipe, when we submit a batch inference job, our solution version will automatically update to consider the items and interactions data.\n",
    "This new items and interactions data will be used for cold-starting purposes. In chapter 7, when you compare the output of this job to the output of the previous job, you will notice a difference.\n",
    "\n",
    "According to [the AWS Documentation for the user personalization recipe](https://docs.aws.amazon.com/personalize/latest/dg/native-recipe-new-item-USER_PERSONALIZATION.html),\n",
    "> Just remember that Amazon Personalize automatically updates only your latest fully trained solution version, so the manually updated solution version won't be automatically updated in the future.\n",
    "\n",
    "\n",
    "Additionally, automatic update requirements for batch item recommendations include the following:\n",
    "\n",
    "> * The solution version you specify in the batch inference job must be the latest solution version for your solution.\n",
    "> * The solution version must be trained with trainingMode set to FULL (this is the default when creating a solution version).\n",
    "> * You must provide new item or interactions data since the last automatic update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File was uploaded successfully!\n",
      "{\n",
      "  \"batchInferenceJobArn\": \"arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"fe20dec8-8967-46be-a380-cc3fcf9743af\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 16:22:29 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"139\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"fe20dec8-8967-46be-a380-cc3fcf9743af\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "s3_input_key_au = \"batch-job-input/\" + au_job_input_filename\n",
    "s3_input_path_au = \"s3://\" + bucket_name + \"/\" + s3_input_key_au\n",
    "s3_output_path_au = \"s3://\" + bucket_name + \"/batch-job-outputs/auto-update/\"\n",
    "\n",
    "# Upload a copy of the input file to s3.\n",
    "!cp {orig_job_input_filename} {au_job_input_filename}\n",
    "\n",
    "s3.upload_file(au_job_input_filename, bucket_name, s3_input_key_au)\n",
    "if s3_input_key_au in [object['Key'] for object in s3.list_objects(Bucket=bucket_name)['Contents']]:\n",
    "    print('File was uploaded successfully!')\n",
    "else:\n",
    "    print('File was not uploaded!')\n",
    "\n",
    "response = personalize.create_batch_inference_job(\n",
    "    solutionVersionArn = user_personalization_solution_version_arn,\n",
    "    jobName = \"retaildemostore-user-personalization-job-au-\" + token,\n",
    "    roleArn = role_arn,\n",
    "    jobInput = {\"s3DataSource\": {\"path\": s3_input_path_au}},\n",
    "    jobOutput = {\"s3DataDestination\":{\"path\": s3_output_path_au}},\n",
    "    numResults = y\n",
    ")\n",
    "user_personalization_autoupdate_job_arn = response['batchInferenceJobArn']\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Job Started on:  04:22:29 PM\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE PENDING\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: CREATE IN_PROGRESS\n",
      "Job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-au-85968: ACTIVE\n",
      "Inference job has ended\n",
      "Inference Job Completed on:  04:41:31 PM\n",
      "CPU times: user 494 ms, sys: 30.1 ms, total: 524 ms\n",
      "Wall time: 19min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### Wait for the batch inference job to complete This can take around 15 minutes.\n",
    "current_time = datetime.now()\n",
    "print(\"Inference Job Started on: \", current_time.strftime(\"%I:%M:%S %p\"))\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    resp = personalize.describe_batch_inference_job(batchInferenceJobArn = user_personalization_autoupdate_job_arn)\n",
    "    status = resp[\"batchInferenceJob\"]['status']\n",
    "    print(\"DatasetInferenceJob {arn}: {status}\".format(arn=user_personalization_autoupdate_job_arn, status=status))\n",
    "\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        print(\"Inference job has ended\")\n",
    "        break\n",
    "    else:\n",
    "        print('Job still in progress')\n",
    "        time.sleep(60)\n",
    "    \n",
    "current_time = datetime.now()\n",
    "print(\"Inference Job Completed on: \", current_time.strftime(\"%I:%M:%S %p\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the output and inspect of the inference job\n",
    "\n",
    "Download the output of the inference job from our private S3 bucket.\n",
    "The following code cell does a side-by-side comparison of the outputs from the original solution version and the auto-updated solution version.\n",
    "You may notice some differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previewing output of the inference job that was run on the original solution version\n",
      "{\"input\":{\"userId\":\"4016\"},\"output\":{\"recommendedItems\":[\"ccdf737c-c4fd-4c78-abd2-d5ef0428ef20\",\"425cc876-3935-4e87-ad8d-77f42b0b6a75\",\"6be08307-1ec0-44dc-b436-5d489a8010e8\",\"2c1b34d6-0f3d-463d-be76-226cb87bdc6d\",\"89c4eeb4-c146-4434-a9f1-6943b4b552dc\",\"5a94b7d5-b210-44b3-9287-c8b0b5488a15\",\"61b1ad14-4e70-4029-ba55-d17bbf4ab62b\",\"8f8f015a-4166-4e9e-ac0b-6d980614ca5d\",\"eecbee28-73a3-425d-84e8-516c326e399c\",\"1daacea7-7d46-464a-8326-ed81951fecab\"],\"scores\":[0.8449224,0.0186943,0.0148416,0.0100115,0.008885,0.0055925,0.0055068,0.0053057,0.0040184,0.0038556]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"2755\"},\"output\":{\"recommendedItems\":[\"3630053e-3962-4549-bcce-402c3a980557\",\"6d488475-1d67-4076-96b1-8e706709a847\",\"b947ee58-a7e7-40bf-9926-42a445f3480f\",\"90ccfbb9-4538-4951-af8d-4f728578b237\",\"d537d92a-23fe-4673-a697-795652ff10c8\",\"78080d05-b078-441f-b245-54b2a2dec872\",\"61840d6a-6ba2-4ece-a644-6db6a3377b1c\",\"2e95f6fc-6be7-46cf-9e50-8c35313c2768\",\"b630250c-41f3-4f14-865c-c1dc12e448ac\",\"d2d8147f-0f24-42c3-bcbe-a232bab7e94d\"],\"scores\":[0.5338279,0.0922309,0.0577862,0.0209987,0.0206773,0.0148014,0.0141705,0.0129197,0.0120157,0.0077951]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"5866\"},\"output\":{\"recommendedItems\":[\"5afced84-ed2d-4520-a06d-dcfeab382e52\",\"6cc0deb8-4a56-4148-a2ab-677277522c80\",\"e1146e90-3274-4ad6-a6a2-0170f0f8d597\",\"575c0ac0-5494-4c64-a886-a9c0cf8b779a\",\"24c62ad2-6977-4f69-be75-e37d897c1434\",\"4496471c-b098-4915-9a1a-8b9e60043737\",\"9c1a2048-7aac-4565-b836-d8d4f726322c\",\"ccb407b1-7620-4303-8521-fea86c51f503\",\"8cd7ffe0-a8a6-45b1-8d1f-bf731c9cd17b\",\"aa564ee3-67ef-4428-8ad9-fe785a0fff63\"],\"scores\":[0.5773515,0.2774695,0.1156833,0.0155479,0.0066222,0.0059881,3.773E-4,7.17E-5,6.78E-5,3.1E-5]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"2896\"},\"output\":{\"recommendedItems\":[\"1dd4c2da-d174-43b1-8d40-fadc666c26c9\",\"e99c24df-ebe9-429e-8c69-cd80132b87b3\",\"e06f53cd-7776-41ce-9f7a-a88986192e24\",\"75cb828e-ccc5-41ff-9bdd-9ac3dc7740aa\",\"153b2374-36e3-466c-b08c-1078b839cd9b\",\"2f2995da-4768-478a-a4ae-906b76d8c6fe\",\"2a0a5c7b-ca68-4abf-9798-18ffb706832b\",\"e8e48eb7-0b66-4087-b280-1c3a62804a5c\",\"e5816ea5-3ce2-4b86-9530-9b221b357b43\",\"17ab5081-8414-4cab-9003-033ec02b44da\"],\"scores\":[0.6146742,0.0403633,0.0256617,0.0107649,0.0107413,0.0106864,0.0100017,0.007999,0.0077459,0.0065279]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"5023\"},\"output\":{\"recommendedItems\":[\"c6dd0909-46f3-4cf9-a059-fbdff93198dd\",\"441c2a65-4b68-4864-b014-04a9bd9fe08a\",\"84d6c26d-9760-49d8-854b-0a22becd8241\",\"9257351d-59f7-481a-86c4-30dea451afa2\",\"635be5a7-3345-46f9-aa0d-419a6652b0f2\",\"72ae72f3-e7f0-4f03-b8eb-12e78c77741d\",\"d4cf35dd-b543-4b4f-9efb-c2de473c3fed\",\"4994caee-f0b7-4ce8-a4df-d542ce1d9bda\",\"0e3eb8f1-8f23-41fd-9f45-8e7747a5eb37\",\"fe96a096-a0b6-4b20-a332-e11db6c0c7b0\"],\"scores\":[0.3460568,0.2430023,0.1881302,0.025838,0.023981,0.0178926,0.0130894,0.0092487,0.0080664,0.0077484]},\"error\":null}\n",
      "\n",
      "\n",
      "Previewing output of the inference job that was run on the auto-updated solution version\n",
      "{\"input\":{\"userId\":\"4016\"},\"output\":{\"recommendedItems\":[\"ccdf737c-c4fd-4c78-abd2-d5ef0428ef20\",\"425cc876-3935-4e87-ad8d-77f42b0b6a75\",\"6be08307-1ec0-44dc-b436-5d489a8010e8\",\"2c1b34d6-0f3d-463d-be76-226cb87bdc6d\",\"89c4eeb4-c146-4434-a9f1-6943b4b552dc\",\"5a94b7d5-b210-44b3-9287-c8b0b5488a15\",\"61b1ad14-4e70-4029-ba55-d17bbf4ab62b\",\"8f8f015a-4166-4e9e-ac0b-6d980614ca5d\",\"eecbee28-73a3-425d-84e8-516c326e399c\",\"1daacea7-7d46-464a-8326-ed81951fecab\"],\"scores\":[0.8114071,0.0179528,0.0142529,0.0096143,0.0085326,0.0053707,0.0052884,0.0050952,0.003859,0.0037027]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"2755\"},\"output\":{\"recommendedItems\":[\"3630053e-3962-4549-bcce-402c3a980557\",\"6d488475-1d67-4076-96b1-8e706709a847\",\"b947ee58-a7e7-40bf-9926-42a445f3480f\",\"90ccfbb9-4538-4951-af8d-4f728578b237\",\"d537d92a-23fe-4673-a697-795652ff10c8\",\"78080d05-b078-441f-b245-54b2a2dec872\",\"61840d6a-6ba2-4ece-a644-6db6a3377b1c\",\"2e95f6fc-6be7-46cf-9e50-8c35313c2768\",\"b630250c-41f3-4f14-865c-c1dc12e448ac\",\"d2d8147f-0f24-42c3-bcbe-a232bab7e94d\"],\"scores\":[0.4696172,0.081137,0.0508355,0.0184729,0.0181902,0.013021,0.012466,0.0113657,0.0105704,0.0068574]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"5866\"},\"output\":{\"recommendedItems\":[\"5afced84-ed2d-4520-a06d-dcfeab382e52\",\"6cc0deb8-4a56-4148-a2ab-677277522c80\",\"e1146e90-3274-4ad6-a6a2-0170f0f8d597\",\"575c0ac0-5494-4c64-a886-a9c0cf8b779a\",\"24c62ad2-6977-4f69-be75-e37d897c1434\",\"4496471c-b098-4915-9a1a-8b9e60043737\",\"new_7a619c82-a5da-4bc9-b6e6-64e93c51fb55\",\"new_25d7bbf6-7dd3-4912-93a7-4186ea417b54\",\"new_0987bfa1-0a23-4b90-8882-8a6e9bd91e24\",\"new_0790267c-c708-424d-81f5-46903a9c8444\"],\"scores\":[0.5705479,0.2741997,0.11432,0.0153647,0.0065442,0.0059175,0.0027483,0.0012491,0.0012491,0.0012491]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"2896\"},\"output\":{\"recommendedItems\":[\"1dd4c2da-d174-43b1-8d40-fadc666c26c9\",\"e99c24df-ebe9-429e-8c69-cd80132b87b3\",\"e06f53cd-7776-41ce-9f7a-a88986192e24\",\"75cb828e-ccc5-41ff-9bdd-9ac3dc7740aa\",\"153b2374-36e3-466c-b08c-1078b839cd9b\",\"2f2995da-4768-478a-a4ae-906b76d8c6fe\",\"2a0a5c7b-ca68-4abf-9798-18ffb706832b\",\"e8e48eb7-0b66-4087-b280-1c3a62804a5c\",\"e5816ea5-3ce2-4b86-9530-9b221b357b43\",\"17ab5081-8414-4cab-9003-033ec02b44da\"],\"scores\":[0.5493492,0.0360736,0.0229345,0.0096209,0.0095998,0.0095507,0.0089388,0.0071489,0.0069227,0.0058341]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"5023\"},\"output\":{\"recommendedItems\":[\"c6dd0909-46f3-4cf9-a059-fbdff93198dd\",\"441c2a65-4b68-4864-b014-04a9bd9fe08a\",\"84d6c26d-9760-49d8-854b-0a22becd8241\",\"9257351d-59f7-481a-86c4-30dea451afa2\",\"635be5a7-3345-46f9-aa0d-419a6652b0f2\",\"72ae72f3-e7f0-4f03-b8eb-12e78c77741d\",\"d4cf35dd-b543-4b4f-9efb-c2de473c3fed\",\"4994caee-f0b7-4ce8-a4df-d542ce1d9bda\",\"0e3eb8f1-8f23-41fd-9f45-8e7747a5eb37\",\"fe96a096-a0b6-4b20-a332-e11db6c0c7b0\"],\"scores\":[0.3333074,0.2340497,0.1811992,0.0248861,0.0230975,0.0172334,0.0126071,0.008908,0.0077692,0.0074629]},\"error\":null}\n"
     ]
    }
   ],
   "source": [
    "# Get the output and inspect of the inference job\n",
    "\n",
    "# - Download the Inference job output from S3\n",
    "job_output_file_au = au_job_input_filename + \".out\"\n",
    "export_name_au = 'batch-job-outputs/auto-update/' + job_output_file_au\n",
    "# print(batch_job_output_file_au)\n",
    "# print(export_name_au)\n",
    "\n",
    "s3.download_file(bucket_name, export_name_au, job_output_file_au)\n",
    "\n",
    "# - Inspect the Inference Job\n",
    "print(\"Previewing output of the inference job that was run on the original solution version\")\n",
    "!head -n 5 $job_output_file_orig\n",
    "\n",
    "# - Inspect the Inference Job\n",
    "print(\"\\n\\nPreviewing output of the inference job that was run on the auto-updated solution version\")\n",
    "!head -n 5 $job_output_file_au\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the output from the auto-updated solution version contains new items.\n",
    "This demonstrates that the auto-update feature for user-personalization solution version does indeed recommend those new items (for cold starts) without requiring a retraining of the underlying model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter 6 Step 4: View the metrics of the auto-updated solution version\n",
    "\n",
    "Lets quickly fetch the metrics of post auto-updated solution version. Since the underlying solution version was only *updated* (but not *retrained*), we should expect the metric of the original solution version & the auto-trained solution version to remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics of the original solution version:\n",
      "{\n",
      "  \"coverage\": 0.9968,\n",
      "  \"mean_reciprocal_rank_at_25\": 0.8459,\n",
      "  \"normalized_discounted_cumulative_gain_at_10\": 0.7873,\n",
      "  \"normalized_discounted_cumulative_gain_at_25\": 0.8089,\n",
      "  \"normalized_discounted_cumulative_gain_at_5\": 0.7661,\n",
      "  \"precision_at_10\": 0.1194,\n",
      "  \"precision_at_25\": 0.0541,\n",
      "  \"precision_at_5\": 0.2189\n",
      "}\n",
      "\n",
      "\n",
      "Metrics of the auto-updated solution version:\n",
      "{\n",
      "  \"coverage\": 0.9968,\n",
      "  \"mean_reciprocal_rank_at_25\": 0.8459,\n",
      "  \"normalized_discounted_cumulative_gain_at_10\": 0.7873,\n",
      "  \"normalized_discounted_cumulative_gain_at_25\": 0.8089,\n",
      "  \"normalized_discounted_cumulative_gain_at_5\": 0.7661,\n",
      "  \"precision_at_10\": 0.1194,\n",
      "  \"precision_at_25\": 0.0541,\n",
      "  \"precision_at_5\": 0.2189\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Display the metrics of the original/pre-updated solution version:\n",
    "print(\"Metrics of the original solution version:\")\n",
    "print(json.dumps(get_original_solution_metrics_response['metrics'], indent=2))\n",
    "\n",
    "# Get metrics for auto-updated solution version\n",
    "get_autoupdated_solution_metrics_response = personalize.get_solution_metrics(\n",
    "    solutionVersionArn = user_personalization_solution_version_arn\n",
    ")\n",
    "print(\"\\n\\nMetrics of the auto-updated solution version:\")\n",
    "print(json.dumps(get_autoupdated_solution_metrics_response['metrics'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter 6 Step 5: Fully re-train our solution version\n",
    "\n",
    "If we want our new Items and Interactions data to have more influence in our solution version, then we will need to *fully retrain* the solution version. The full retraining will allow our solution version to use the new items and new interactions at part of its *meaningful*, or *exploitative* (as opposed to just *exploratory*) recommendations for inference jobs. To fully retrain a solution version, use the CreateSolutionVersion command with the ```trainingMode``` flag set to ```FULL```.\n",
    "\n",
    "Note: Setting ```trainingMode=FULL``` will enable auto-updates to the new solution version.\n",
    "\n",
    "Some additional information on the trainingMode parameter (from [documentaion on Solution Versions](https://docs.aws.amazon.com/personalize/latest/dg/API_SolutionVersion.html)):\n",
    "\n",
    "The trainingMode parameter defines:\n",
    "> The scope of training to be performed when creating the solution version. The FULL option trains the solution version based on the entirety of the input solution's training data, while the UPDATE option processes only the data that has changed in comparison to the input solution. Choose UPDATE when you want to incrementally update your solution version instead of creating an entirely new one.\n",
    "\n",
    "As a side note:\n",
    "> The UPDATE option can only be used when you already have an active solution version created from the input solution using the FULL option and the input solution was trained with the User-Personalization recipe or the HRNN-Coldstart recipe.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"solutionVersionArn\": \"arn:aws:personalize:us-east-1:402114309305:solution/retaildemostore-user-personalization-solution-85968/859a76e2\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"35a7a2ad-025f-43a4-b1bf-de8856237ace\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 16:41:31 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"137\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"35a7a2ad-025f-43a4-b1bf-de8856237ace\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Fully train a new solution version\n",
    "retrained_user_personalization_solution_version_arn = None\n",
    "\n",
    "if not retrained_user_personalization_solution_version_arn:\n",
    "    create_solution_version_response = personalize.create_solution_version(\n",
    "        solutionArn = user_personalization_solution_arn,\n",
    "        trainingMode='FULL'\n",
    "    )\n",
    "\n",
    "    retrained_user_personalization_solution_version_arn = create_solution_version_response['solutionVersionArn']\n",
    "    print(json.dumps(create_solution_version_response, indent=2))\n",
    "else:\n",
    "    print(f'Solution version {retrained_user_personalization_solution_version_arn} already exists; not creating')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wait for the new solution version to finish training\n",
    "This can take around 40 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "At least one solution version is still in progress\n",
      "Solution version arn:aws:personalize:us-east-1:402114309305:solution/retaildemostore-user-personalization-solution-85968/859a76e2 successfully completed\n",
      "CPU times: user 622 ms, sys: 17.4 ms, total: 639 ms\n",
      "Wall time: 20min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    updated_soln_ver_response = personalize.describe_solution_version(\n",
    "        solutionVersionArn = retrained_user_personalization_solution_version_arn\n",
    "    )\n",
    "    status = updated_soln_ver_response[\"solutionVersion\"][\"status\"]\n",
    "\n",
    "    if status == \"ACTIVE\":\n",
    "        print(f'Solution version {retrained_user_personalization_solution_version_arn} successfully completed')\n",
    "        break\n",
    "    elif status == \"CREATE FAILED\":\n",
    "        print(f'Solution version {retrained_user_personalization_solution_version_arn} failed')\n",
    "        if updated_soln_ver_response[\"solutionVersion\"].get('failureReason'):\n",
    "            print('   Reason: ' + updated_soln_ver_response[\"solutionVersion\"]['failureReason'])\n",
    "        break\n",
    "    else:\n",
    "        print('At least one solution version is still in progress')\n",
    "        time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 6 Step 6: Submit the batch segmentation job to our new Solution Version\n",
    "\n",
    "Now that we have fully retrained a new solution version, we will:\n",
    "- Submit a batch segmentation job to our new *fully retrained* solution version. For consistency, we will use the same sample input that we used previously.\n",
    "- Wait for the batch job to complete.\n",
    "- Compare the output (Output C) with Output B.\n",
    "\n",
    "In chapter 7, we will perform a side-by-side comparison of outputs A, B, and C.\n",
    "\n",
    "Recall:\n",
    "- Output A = the output from the original solution version\n",
    "- Output B = the output from the solution version after it was auto-updated\n",
    "- Output C = the output from the fully retrained solution version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File was uploaded successfully!\n",
      "\n",
      "Previewing input file... \n",
      "{\"userId\": \"5023\"}\n",
      "{\"userId\": \"5866\"}\n",
      "{\"userId\": \"4016\"}\n",
      "{\"userId\": \"2755\"}\n",
      "{\"userId\": \"2896\"}\n",
      "\n",
      "\n",
      "{\n",
      "  \"batchInferenceJobArn\": \"arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-fr-85968\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"bb8bbd39-10b6-4883-a55e-aaaf6f249504\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 27 Oct 2023 17:01:35 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"139\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"bb8bbd39-10b6-4883-a55e-aaaf6f249504\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "### Step 6) Submit batch segmentation job. \n",
    "\n",
    "s3_input_key_fr = \"batch-job-input/\" + fr_job_input_filename\n",
    "s3_input_path_fr = \"s3://\" + bucket_name + \"/\" + s3_input_key_fr\n",
    "s3_output_path_fr = \"s3://\" + bucket_name + \"/batch-job-outputs/fully-retrained/\"\n",
    "\n",
    "# Upload a copy of the input file to s3.\n",
    "!cp {orig_job_input_filename} {fr_job_input_filename}\n",
    "s3.upload_file(fr_job_input_filename, bucket_name, s3_input_key_fr)\n",
    "if s3_input_key_fr in [object['Key'] for object in s3.list_objects(Bucket=bucket_name)['Contents']]:\n",
    "    print('File was uploaded successfully!')\n",
    "else:\n",
    "    print('File was not uploaded!')\n",
    "    \n",
    "# Preview input file We will use the same input for batch inference job using our new solution version.\n",
    "print(\"\\nPreviewing input file... \")\n",
    "!head -n 5 $fr_job_input_filename\n",
    "print('\\n')\n",
    "\n",
    "# Create and start a Batch Segmentation Job using our latest Solution Version\n",
    "s3_input_key_fr = \"batch-job-input/\" + fr_job_input_filename\n",
    "s3_output_path_fr = \"s3://\" + bucket_name + \"/batch-job-outputs/fully-retrained/\" # Define output location\n",
    "\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize/client/create_batch_inference_job.html\n",
    "response = personalize.create_batch_inference_job (\n",
    "    solutionVersionArn = retrained_user_personalization_solution_version_arn,\n",
    "    jobName = \"retaildemostore-user-personalization-job-fr-\" + token,\n",
    "    roleArn = role_arn,\n",
    "    jobInput = {\"s3DataSource\": {\"path\": s3_input_path_fr }},\n",
    "    jobOutput = {\"s3DataDestination\": {\"path\": s3_output_path_fr }},\n",
    "    numResults = y\n",
    ")\n",
    "\n",
    "user_personalization_retrained_job_arn = response['batchInferenceJobArn']\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the batch job to complete. This can take around 10 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Job Started on:  05:01:35 PM\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-fr-85968: CREATE PENDING\n",
      "At least one batch inference job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-fr-85968: CREATE IN_PROGRESS\n",
      "At least one batch inference job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-fr-85968: CREATE IN_PROGRESS\n",
      "At least one batch inference job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-fr-85968: CREATE IN_PROGRESS\n",
      "At least one batch inference job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-fr-85968: CREATE IN_PROGRESS\n",
      "At least one batch inference job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-fr-85968: CREATE IN_PROGRESS\n",
      "At least one batch inference job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-fr-85968: CREATE IN_PROGRESS\n",
      "At least one batch inference job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-fr-85968: CREATE IN_PROGRESS\n",
      "At least one batch inference job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-fr-85968: CREATE IN_PROGRESS\n",
      "At least one batch inference job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-fr-85968: CREATE IN_PROGRESS\n",
      "At least one batch inference job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-fr-85968: CREATE IN_PROGRESS\n",
      "At least one batch inference job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-fr-85968: CREATE IN_PROGRESS\n",
      "At least one batch inference job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-fr-85968: CREATE IN_PROGRESS\n",
      "At least one batch inference job still in progress\n",
      "DatasetInferenceJob arn:aws:personalize:us-east-1:402114309305:batch-inference-job/retaildemostore-user-personalization-job-fr-85968: ACTIVE\n",
      "Inference Job Completed on:  05:14:36 PM\n",
      "CPU times: user 381 ms, sys: 6.47 ms, total: 387 ms\n",
      "Wall time: 13min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "current_time = datetime.now()\n",
    "print(\"Inference Job Started on: \", current_time.strftime(\"%I:%M:%S %p\"))\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    \n",
    "    resp = personalize.describe_batch_inference_job(batchInferenceJobArn = user_personalization_retrained_job_arn)\n",
    "    status = resp[\"batchInferenceJob\"]['status']\n",
    "    print(\"DatasetInferenceJob {arn}: {status}\".format(arn=user_personalization_retrained_job_arn, status=status))\n",
    "\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        current_time = datetime.now()\n",
    "        print(\"Inference Job Completed on: \", current_time.strftime(\"%I:%M:%S %p\"))\n",
    "        break\n",
    "    else:\n",
    "        print('At least one batch inference job still in progress')\n",
    "        time.sleep(60)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job has completed.\n",
    "Now, let's download & inspect its output file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previewing output of the inference job that was run on the fully-trained solution version:\n",
      "{\"input\":{\"userId\":\"4016\"},\"output\":{\"recommendedItems\":[\"new_72994d99-e815-486e-9b8e-bfccbc230e4b\",\"new_1b2dda7c-7fd7-476a-bdea-87bcd101a022\",\"new_acfba3f9-f7d6-4fff-9cef-35db086d2869\",\"new_bbcda337-3411-47e4-aeec-079663f729df\",\"new_ac46fdbc-2369-4908-b0b1-d405572e4a5c\",\"61b1ad14-4e70-4029-ba55-d17bbf4ab62b\",\"aa4fca9d-d1ef-4529-9169-7bc075733bd5\",\"99c88141-7b7c-403b-b8eb-5dd5e8efd4a4\",\"new_91cfb05c-44cc-4eca-b622-1fa875e0256c\",\"cd78672a-0a5e-4931-ace1-2f2abb90b720\"],\"scores\":[0.1686713,0.0466978,0.0331571,0.0227852,0.0181023,0.0180926,0.017133,0.0126616,0.0100974,0.0100844]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"2755\"},\"output\":{\"recommendedItems\":[\"new_af2aba3d-c9f6-46e1-95db-100fc1a73726\",\"3630053e-3962-4549-bcce-402c3a980557\",\"b947ee58-a7e7-40bf-9926-42a445f3480f\",\"a58cc0a2-d54f-4e5a-85e7-e8c530592b76\",\"new_53ec1efb-0deb-48cf-96a3-7a7342b78608\",\"99c88141-7b7c-403b-b8eb-5dd5e8efd4a4\",\"new_1d21572e-f35d-4c7b-8a39-a4d97b08276a\",\"b630250c-41f3-4f14-865c-c1dc12e448ac\",\"new_bfbd28d2-d351-4a21-b799-9ba6a74c7b96\",\"new_5d28d7a1-4a71-4db3-9ec3-754c2b0b5d99\"],\"scores\":[0.7869672,0.025295,0.0163876,0.0143594,0.0107439,0.0094963,0.0093558,0.0084559,0.0072018,0.0046402]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"5866\"},\"output\":{\"recommendedItems\":[\"new_0987bfa1-0a23-4b90-8882-8a6e9bd91e24\",\"new_0de9bba0-1149-40e9-b1a6-7dcecaf68194\",\"5afced84-ed2d-4520-a06d-dcfeab382e52\",\"e1146e90-3274-4ad6-a6a2-0170f0f8d597\",\"new_7a619c82-a5da-4bc9-b6e6-64e93c51fb55\",\"6cc0deb8-4a56-4148-a2ab-677277522c80\",\"24c62ad2-6977-4f69-be75-e37d897c1434\",\"4496471c-b098-4915-9a1a-8b9e60043737\",\"new_25d7bbf6-7dd3-4912-93a7-4186ea417b54\",\"575c0ac0-5494-4c64-a886-a9c0cf8b779a\"],\"scores\":[0.8732009,0.0379004,0.0318795,0.0143929,0.0128519,0.0124082,0.0079406,0.0044158,0.0034907,7.699E-4]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"2896\"},\"output\":{\"recommendedItems\":[\"1dd4c2da-d174-43b1-8d40-fadc666c26c9\",\"ff47435d-97d2-4d05-b3e8-294d4c47cbc3\",\"7d278838-fb4e-45cd-8fb2-b5e736fb9aa2\",\"0665c441-8841-47c5-84e4-a897bd78ee84\",\"new_28762499-dd36-4b23-96d3-db3eeeaed548\",\"new_7d9271c9-ddce-4ccb-a8e7-161b94b1fc79\",\"new_eb46bfcc-34df-40e2-8670-ee808d5cb958\",\"4dc7226a-2225-488d-8f58-fe5efc2710c8\",\"new_59719e89-9677-4201-8aad-fb0157bbd30c\",\"new_180cec7c-7dd5-4ad5-a609-42d39a33479e\"],\"scores\":[0.5776186,0.0185266,0.0182808,0.0143907,0.0098805,0.009119,0.0082382,0.0079592,0.0076707,0.0065387]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"5023\"},\"output\":{\"recommendedItems\":[\"441c2a65-4b68-4864-b014-04a9bd9fe08a\",\"new_55763c45-6051-4fff-ae04-495c0fafa8ff\",\"84d6c26d-9760-49d8-854b-0a22becd8241\",\"new_e358575b-7983-4f61-9902-bd44ce9ead6b\",\"c6dd0909-46f3-4cf9-a059-fbdff93198dd\",\"635be5a7-3345-46f9-aa0d-419a6652b0f2\",\"new_7fd05ca4-ac20-46cf-b4f8-87147eef8d65\",\"641f3960-72a7-4e2b-be69-8a7539eb50bb\",\"36e1f150-3d83-4d3c-9855-8daf858d8e28\",\"new_e41fe9b4-db67-4726-9624-335e01098abb\"],\"scores\":[0.6246195,0.1523481,0.0411481,0.0288333,0.0206153,0.0155547,0.0072245,0.0070363,0.0065347,0.0051012]},\"error\":null}\n",
      "\n",
      "\n",
      "Previewing output of the inference job that was run on the auto-updated solution version:\n",
      "{\"input\":{\"userId\":\"4016\"},\"output\":{\"recommendedItems\":[\"ccdf737c-c4fd-4c78-abd2-d5ef0428ef20\",\"425cc876-3935-4e87-ad8d-77f42b0b6a75\",\"6be08307-1ec0-44dc-b436-5d489a8010e8\",\"2c1b34d6-0f3d-463d-be76-226cb87bdc6d\",\"89c4eeb4-c146-4434-a9f1-6943b4b552dc\",\"5a94b7d5-b210-44b3-9287-c8b0b5488a15\",\"61b1ad14-4e70-4029-ba55-d17bbf4ab62b\",\"8f8f015a-4166-4e9e-ac0b-6d980614ca5d\",\"eecbee28-73a3-425d-84e8-516c326e399c\",\"1daacea7-7d46-464a-8326-ed81951fecab\"],\"scores\":[0.8114071,0.0179528,0.0142529,0.0096143,0.0085326,0.0053707,0.0052884,0.0050952,0.003859,0.0037027]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"2755\"},\"output\":{\"recommendedItems\":[\"3630053e-3962-4549-bcce-402c3a980557\",\"6d488475-1d67-4076-96b1-8e706709a847\",\"b947ee58-a7e7-40bf-9926-42a445f3480f\",\"90ccfbb9-4538-4951-af8d-4f728578b237\",\"d537d92a-23fe-4673-a697-795652ff10c8\",\"78080d05-b078-441f-b245-54b2a2dec872\",\"61840d6a-6ba2-4ece-a644-6db6a3377b1c\",\"2e95f6fc-6be7-46cf-9e50-8c35313c2768\",\"b630250c-41f3-4f14-865c-c1dc12e448ac\",\"d2d8147f-0f24-42c3-bcbe-a232bab7e94d\"],\"scores\":[0.4696172,0.081137,0.0508355,0.0184729,0.0181902,0.013021,0.012466,0.0113657,0.0105704,0.0068574]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"5866\"},\"output\":{\"recommendedItems\":[\"5afced84-ed2d-4520-a06d-dcfeab382e52\",\"6cc0deb8-4a56-4148-a2ab-677277522c80\",\"e1146e90-3274-4ad6-a6a2-0170f0f8d597\",\"575c0ac0-5494-4c64-a886-a9c0cf8b779a\",\"24c62ad2-6977-4f69-be75-e37d897c1434\",\"4496471c-b098-4915-9a1a-8b9e60043737\",\"new_7a619c82-a5da-4bc9-b6e6-64e93c51fb55\",\"new_25d7bbf6-7dd3-4912-93a7-4186ea417b54\",\"new_0987bfa1-0a23-4b90-8882-8a6e9bd91e24\",\"new_0790267c-c708-424d-81f5-46903a9c8444\"],\"scores\":[0.5705479,0.2741997,0.11432,0.0153647,0.0065442,0.0059175,0.0027483,0.0012491,0.0012491,0.0012491]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"2896\"},\"output\":{\"recommendedItems\":[\"1dd4c2da-d174-43b1-8d40-fadc666c26c9\",\"e99c24df-ebe9-429e-8c69-cd80132b87b3\",\"e06f53cd-7776-41ce-9f7a-a88986192e24\",\"75cb828e-ccc5-41ff-9bdd-9ac3dc7740aa\",\"153b2374-36e3-466c-b08c-1078b839cd9b\",\"2f2995da-4768-478a-a4ae-906b76d8c6fe\",\"2a0a5c7b-ca68-4abf-9798-18ffb706832b\",\"e8e48eb7-0b66-4087-b280-1c3a62804a5c\",\"e5816ea5-3ce2-4b86-9530-9b221b357b43\",\"17ab5081-8414-4cab-9003-033ec02b44da\"],\"scores\":[0.5493492,0.0360736,0.0229345,0.0096209,0.0095998,0.0095507,0.0089388,0.0071489,0.0069227,0.0058341]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"5023\"},\"output\":{\"recommendedItems\":[\"c6dd0909-46f3-4cf9-a059-fbdff93198dd\",\"441c2a65-4b68-4864-b014-04a9bd9fe08a\",\"84d6c26d-9760-49d8-854b-0a22becd8241\",\"9257351d-59f7-481a-86c4-30dea451afa2\",\"635be5a7-3345-46f9-aa0d-419a6652b0f2\",\"72ae72f3-e7f0-4f03-b8eb-12e78c77741d\",\"d4cf35dd-b543-4b4f-9efb-c2de473c3fed\",\"4994caee-f0b7-4ce8-a4df-d542ce1d9bda\",\"0e3eb8f1-8f23-41fd-9f45-8e7747a5eb37\",\"fe96a096-a0b6-4b20-a332-e11db6c0c7b0\"],\"scores\":[0.3333074,0.2340497,0.1811992,0.0248861,0.0230975,0.0172334,0.0126071,0.008908,0.0077692,0.0074629]},\"error\":null}\n"
     ]
    }
   ],
   "source": [
    "# - Download the Inference job output from S3\n",
    "job_output_file_fr = fr_job_input_filename + \".out\"\n",
    "export_name_fr = 'batch-job-outputs/fully-retrained/' + job_output_file_fr\n",
    "s3.download_file(bucket_name, export_name_fr, job_output_file_fr)\n",
    "\n",
    "# - Inspect the Inference Job\n",
    "print(\"Previewing output of the inference job that was run on the fully-trained solution version:\")\n",
    "!head -n 5 $job_output_file_fr\n",
    "\n",
    "# - Inspect the Inference Job\n",
    "print(\"\\n\\nPreviewing output of the inference job that was run on the auto-updated solution version:\")\n",
    "!head -n 5 $job_output_file_au\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the two outputs are slightly different. Specifically, new items are much more likely to appear in the output from the fully retrained solution version, compared to the output from the auto-updated solution version.\n",
    "\n",
    "This is beacuse the fully-retrained solution version's output uses the new items and new interactions data to a more significant degree compared to the auto-updated solution version.\n",
    "\n",
    "Those new items won't *just* be recommended as part cold starts anymore. Now, they'll also be used in more relevant/meaningful recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Evaluate the metrics of the two solution versions\n",
    "\n",
    "Let's compare the metrics of the auto-updated solution version and fully retrained solution version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics of the auto-updated solution version:\n",
      "{\n",
      "  \"coverage\": 0.9968,\n",
      "  \"mean_reciprocal_rank_at_25\": 0.8459,\n",
      "  \"normalized_discounted_cumulative_gain_at_10\": 0.7873,\n",
      "  \"normalized_discounted_cumulative_gain_at_25\": 0.8089,\n",
      "  \"normalized_discounted_cumulative_gain_at_5\": 0.7661,\n",
      "  \"precision_at_10\": 0.1194,\n",
      "  \"precision_at_25\": 0.0541,\n",
      "  \"precision_at_5\": 0.2189\n",
      "}\n",
      "\n",
      "\n",
      "Metrics of the solution version with the updated datasets:\n",
      "{\n",
      "  \"coverage\": 0.985,\n",
      "  \"mean_reciprocal_rank_at_25\": 0.8425,\n",
      "  \"normalized_discounted_cumulative_gain_at_10\": 0.7377,\n",
      "  \"normalized_discounted_cumulative_gain_at_25\": 0.7709,\n",
      "  \"normalized_discounted_cumulative_gain_at_5\": 0.7077,\n",
      "  \"precision_at_10\": 0.1454,\n",
      "  \"precision_at_25\": 0.0692,\n",
      "  \"precision_at_5\": 0.2561\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Metrics of the auto-updated solution version:\")\n",
    "print(json.dumps(get_autoupdated_solution_metrics_response['metrics'], indent=2))\n",
    "\n",
    "# Get metrics for fully retrained solution version that used the updated datasets\n",
    "get_retrained_solution_metrics_response = personalize.get_solution_metrics(\n",
    "    solutionVersionArn = retrained_user_personalization_solution_version_arn\n",
    ")\n",
    "print(\"\\n\\nMetrics of the solution version with the updated datasets:\")\n",
    "print(json.dumps(get_retrained_solution_metrics_response['metrics'], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the metrics of the two solution versions are slightly different. This can serve as additional confirmation that the fully-retrained solution version used different datasets for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 7: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Chapter 7 Step 1**: Compare the outputs across the three batch inference jobs\n",
    "\n",
    "Compare the outputs of the original, auto-updated, and fully retrained solution versions. \n",
    "All three should be different.\n",
    "\n",
    "The original output should will only contain ITEM_ID values that were found in the trimmed-down version of the items dataset.\n",
    "\n",
    "The auto-updated output will contain ITEM_ID values from the complete version of the items dataset. This means that those new items may show up in the item recommendations for your users. Though keep in mind, that if a new item shows up, it'll be for cold-starting purposes so that you can start collecting interactions data for those new items.\n",
    "\n",
    "The fully-retrained output will contain ITEM_ID values from the complete version of the items dataset. Recall that the major difference between this output and the auto-updated output is that both are yielded by different solution versions. When we created a new solution version with ```trainingMode='FULL'```, Personalize trained a solution version using the new data, as supposed to just updating its pool of cold-start items (as was the case with the auto-update solution version).   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previewing output of the inference job that was run on the original solution version:\n",
      "{\"input\":{\"userId\":\"4016\"},\"output\":{\"recommendedItems\":[\"ccdf737c-c4fd-4c78-abd2-d5ef0428ef20\",\"425cc876-3935-4e87-ad8d-77f42b0b6a75\",\"6be08307-1ec0-44dc-b436-5d489a8010e8\",\"2c1b34d6-0f3d-463d-be76-226cb87bdc6d\",\"89c4eeb4-c146-4434-a9f1-6943b4b552dc\",\"5a94b7d5-b210-44b3-9287-c8b0b5488a15\",\"61b1ad14-4e70-4029-ba55-d17bbf4ab62b\",\"8f8f015a-4166-4e9e-ac0b-6d980614ca5d\",\"eecbee28-73a3-425d-84e8-516c326e399c\",\"1daacea7-7d46-464a-8326-ed81951fecab\"],\"scores\":[0.8449224,0.0186943,0.0148416,0.0100115,0.008885,0.0055925,0.0055068,0.0053057,0.0040184,0.0038556]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"2755\"},\"output\":{\"recommendedItems\":[\"3630053e-3962-4549-bcce-402c3a980557\",\"6d488475-1d67-4076-96b1-8e706709a847\",\"b947ee58-a7e7-40bf-9926-42a445f3480f\",\"90ccfbb9-4538-4951-af8d-4f728578b237\",\"d537d92a-23fe-4673-a697-795652ff10c8\",\"78080d05-b078-441f-b245-54b2a2dec872\",\"61840d6a-6ba2-4ece-a644-6db6a3377b1c\",\"2e95f6fc-6be7-46cf-9e50-8c35313c2768\",\"b630250c-41f3-4f14-865c-c1dc12e448ac\",\"d2d8147f-0f24-42c3-bcbe-a232bab7e94d\"],\"scores\":[0.5338279,0.0922309,0.0577862,0.0209987,0.0206773,0.0148014,0.0141705,0.0129197,0.0120157,0.0077951]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"5866\"},\"output\":{\"recommendedItems\":[\"5afced84-ed2d-4520-a06d-dcfeab382e52\",\"6cc0deb8-4a56-4148-a2ab-677277522c80\",\"e1146e90-3274-4ad6-a6a2-0170f0f8d597\",\"575c0ac0-5494-4c64-a886-a9c0cf8b779a\",\"24c62ad2-6977-4f69-be75-e37d897c1434\",\"4496471c-b098-4915-9a1a-8b9e60043737\",\"9c1a2048-7aac-4565-b836-d8d4f726322c\",\"ccb407b1-7620-4303-8521-fea86c51f503\",\"8cd7ffe0-a8a6-45b1-8d1f-bf731c9cd17b\",\"aa564ee3-67ef-4428-8ad9-fe785a0fff63\"],\"scores\":[0.5773515,0.2774695,0.1156833,0.0155479,0.0066222,0.0059881,3.773E-4,7.17E-5,6.78E-5,3.1E-5]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"2896\"},\"output\":{\"recommendedItems\":[\"1dd4c2da-d174-43b1-8d40-fadc666c26c9\",\"e99c24df-ebe9-429e-8c69-cd80132b87b3\",\"e06f53cd-7776-41ce-9f7a-a88986192e24\",\"75cb828e-ccc5-41ff-9bdd-9ac3dc7740aa\",\"153b2374-36e3-466c-b08c-1078b839cd9b\",\"2f2995da-4768-478a-a4ae-906b76d8c6fe\",\"2a0a5c7b-ca68-4abf-9798-18ffb706832b\",\"e8e48eb7-0b66-4087-b280-1c3a62804a5c\",\"e5816ea5-3ce2-4b86-9530-9b221b357b43\",\"17ab5081-8414-4cab-9003-033ec02b44da\"],\"scores\":[0.6146742,0.0403633,0.0256617,0.0107649,0.0107413,0.0106864,0.0100017,0.007999,0.0077459,0.0065279]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"5023\"},\"output\":{\"recommendedItems\":[\"c6dd0909-46f3-4cf9-a059-fbdff93198dd\",\"441c2a65-4b68-4864-b014-04a9bd9fe08a\",\"84d6c26d-9760-49d8-854b-0a22becd8241\",\"9257351d-59f7-481a-86c4-30dea451afa2\",\"635be5a7-3345-46f9-aa0d-419a6652b0f2\",\"72ae72f3-e7f0-4f03-b8eb-12e78c77741d\",\"d4cf35dd-b543-4b4f-9efb-c2de473c3fed\",\"4994caee-f0b7-4ce8-a4df-d542ce1d9bda\",\"0e3eb8f1-8f23-41fd-9f45-8e7747a5eb37\",\"fe96a096-a0b6-4b20-a332-e11db6c0c7b0\"],\"scores\":[0.3460568,0.2430023,0.1881302,0.025838,0.023981,0.0178926,0.0130894,0.0092487,0.0080664,0.0077484]},\"error\":null}\n",
      "\n",
      "\n",
      "Previewing output of the inference job that was run on the auto-updated solution version:\n",
      "{\"input\":{\"userId\":\"4016\"},\"output\":{\"recommendedItems\":[\"ccdf737c-c4fd-4c78-abd2-d5ef0428ef20\",\"425cc876-3935-4e87-ad8d-77f42b0b6a75\",\"6be08307-1ec0-44dc-b436-5d489a8010e8\",\"2c1b34d6-0f3d-463d-be76-226cb87bdc6d\",\"89c4eeb4-c146-4434-a9f1-6943b4b552dc\",\"5a94b7d5-b210-44b3-9287-c8b0b5488a15\",\"61b1ad14-4e70-4029-ba55-d17bbf4ab62b\",\"8f8f015a-4166-4e9e-ac0b-6d980614ca5d\",\"eecbee28-73a3-425d-84e8-516c326e399c\",\"1daacea7-7d46-464a-8326-ed81951fecab\"],\"scores\":[0.8114071,0.0179528,0.0142529,0.0096143,0.0085326,0.0053707,0.0052884,0.0050952,0.003859,0.0037027]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"2755\"},\"output\":{\"recommendedItems\":[\"3630053e-3962-4549-bcce-402c3a980557\",\"6d488475-1d67-4076-96b1-8e706709a847\",\"b947ee58-a7e7-40bf-9926-42a445f3480f\",\"90ccfbb9-4538-4951-af8d-4f728578b237\",\"d537d92a-23fe-4673-a697-795652ff10c8\",\"78080d05-b078-441f-b245-54b2a2dec872\",\"61840d6a-6ba2-4ece-a644-6db6a3377b1c\",\"2e95f6fc-6be7-46cf-9e50-8c35313c2768\",\"b630250c-41f3-4f14-865c-c1dc12e448ac\",\"d2d8147f-0f24-42c3-bcbe-a232bab7e94d\"],\"scores\":[0.4696172,0.081137,0.0508355,0.0184729,0.0181902,0.013021,0.012466,0.0113657,0.0105704,0.0068574]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"5866\"},\"output\":{\"recommendedItems\":[\"5afced84-ed2d-4520-a06d-dcfeab382e52\",\"6cc0deb8-4a56-4148-a2ab-677277522c80\",\"e1146e90-3274-4ad6-a6a2-0170f0f8d597\",\"575c0ac0-5494-4c64-a886-a9c0cf8b779a\",\"24c62ad2-6977-4f69-be75-e37d897c1434\",\"4496471c-b098-4915-9a1a-8b9e60043737\",\"new_7a619c82-a5da-4bc9-b6e6-64e93c51fb55\",\"new_25d7bbf6-7dd3-4912-93a7-4186ea417b54\",\"new_0987bfa1-0a23-4b90-8882-8a6e9bd91e24\",\"new_0790267c-c708-424d-81f5-46903a9c8444\"],\"scores\":[0.5705479,0.2741997,0.11432,0.0153647,0.0065442,0.0059175,0.0027483,0.0012491,0.0012491,0.0012491]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"2896\"},\"output\":{\"recommendedItems\":[\"1dd4c2da-d174-43b1-8d40-fadc666c26c9\",\"e99c24df-ebe9-429e-8c69-cd80132b87b3\",\"e06f53cd-7776-41ce-9f7a-a88986192e24\",\"75cb828e-ccc5-41ff-9bdd-9ac3dc7740aa\",\"153b2374-36e3-466c-b08c-1078b839cd9b\",\"2f2995da-4768-478a-a4ae-906b76d8c6fe\",\"2a0a5c7b-ca68-4abf-9798-18ffb706832b\",\"e8e48eb7-0b66-4087-b280-1c3a62804a5c\",\"e5816ea5-3ce2-4b86-9530-9b221b357b43\",\"17ab5081-8414-4cab-9003-033ec02b44da\"],\"scores\":[0.5493492,0.0360736,0.0229345,0.0096209,0.0095998,0.0095507,0.0089388,0.0071489,0.0069227,0.0058341]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"5023\"},\"output\":{\"recommendedItems\":[\"c6dd0909-46f3-4cf9-a059-fbdff93198dd\",\"441c2a65-4b68-4864-b014-04a9bd9fe08a\",\"84d6c26d-9760-49d8-854b-0a22becd8241\",\"9257351d-59f7-481a-86c4-30dea451afa2\",\"635be5a7-3345-46f9-aa0d-419a6652b0f2\",\"72ae72f3-e7f0-4f03-b8eb-12e78c77741d\",\"d4cf35dd-b543-4b4f-9efb-c2de473c3fed\",\"4994caee-f0b7-4ce8-a4df-d542ce1d9bda\",\"0e3eb8f1-8f23-41fd-9f45-8e7747a5eb37\",\"fe96a096-a0b6-4b20-a332-e11db6c0c7b0\"],\"scores\":[0.3333074,0.2340497,0.1811992,0.0248861,0.0230975,0.0172334,0.0126071,0.008908,0.0077692,0.0074629]},\"error\":null}\n",
      "\n",
      "\n",
      "Previewing output of the inference job that was run on the fully-trained solution version:\n",
      "{\"input\":{\"userId\":\"4016\"},\"output\":{\"recommendedItems\":[\"new_72994d99-e815-486e-9b8e-bfccbc230e4b\",\"new_1b2dda7c-7fd7-476a-bdea-87bcd101a022\",\"new_acfba3f9-f7d6-4fff-9cef-35db086d2869\",\"new_bbcda337-3411-47e4-aeec-079663f729df\",\"new_ac46fdbc-2369-4908-b0b1-d405572e4a5c\",\"61b1ad14-4e70-4029-ba55-d17bbf4ab62b\",\"aa4fca9d-d1ef-4529-9169-7bc075733bd5\",\"99c88141-7b7c-403b-b8eb-5dd5e8efd4a4\",\"new_91cfb05c-44cc-4eca-b622-1fa875e0256c\",\"cd78672a-0a5e-4931-ace1-2f2abb90b720\"],\"scores\":[0.1686713,0.0466978,0.0331571,0.0227852,0.0181023,0.0180926,0.017133,0.0126616,0.0100974,0.0100844]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"2755\"},\"output\":{\"recommendedItems\":[\"new_af2aba3d-c9f6-46e1-95db-100fc1a73726\",\"3630053e-3962-4549-bcce-402c3a980557\",\"b947ee58-a7e7-40bf-9926-42a445f3480f\",\"a58cc0a2-d54f-4e5a-85e7-e8c530592b76\",\"new_53ec1efb-0deb-48cf-96a3-7a7342b78608\",\"99c88141-7b7c-403b-b8eb-5dd5e8efd4a4\",\"new_1d21572e-f35d-4c7b-8a39-a4d97b08276a\",\"b630250c-41f3-4f14-865c-c1dc12e448ac\",\"new_bfbd28d2-d351-4a21-b799-9ba6a74c7b96\",\"new_5d28d7a1-4a71-4db3-9ec3-754c2b0b5d99\"],\"scores\":[0.7869672,0.025295,0.0163876,0.0143594,0.0107439,0.0094963,0.0093558,0.0084559,0.0072018,0.0046402]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"5866\"},\"output\":{\"recommendedItems\":[\"new_0987bfa1-0a23-4b90-8882-8a6e9bd91e24\",\"new_0de9bba0-1149-40e9-b1a6-7dcecaf68194\",\"5afced84-ed2d-4520-a06d-dcfeab382e52\",\"e1146e90-3274-4ad6-a6a2-0170f0f8d597\",\"new_7a619c82-a5da-4bc9-b6e6-64e93c51fb55\",\"6cc0deb8-4a56-4148-a2ab-677277522c80\",\"24c62ad2-6977-4f69-be75-e37d897c1434\",\"4496471c-b098-4915-9a1a-8b9e60043737\",\"new_25d7bbf6-7dd3-4912-93a7-4186ea417b54\",\"575c0ac0-5494-4c64-a886-a9c0cf8b779a\"],\"scores\":[0.8732009,0.0379004,0.0318795,0.0143929,0.0128519,0.0124082,0.0079406,0.0044158,0.0034907,7.699E-4]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"2896\"},\"output\":{\"recommendedItems\":[\"1dd4c2da-d174-43b1-8d40-fadc666c26c9\",\"ff47435d-97d2-4d05-b3e8-294d4c47cbc3\",\"7d278838-fb4e-45cd-8fb2-b5e736fb9aa2\",\"0665c441-8841-47c5-84e4-a897bd78ee84\",\"new_28762499-dd36-4b23-96d3-db3eeeaed548\",\"new_7d9271c9-ddce-4ccb-a8e7-161b94b1fc79\",\"new_eb46bfcc-34df-40e2-8670-ee808d5cb958\",\"4dc7226a-2225-488d-8f58-fe5efc2710c8\",\"new_59719e89-9677-4201-8aad-fb0157bbd30c\",\"new_180cec7c-7dd5-4ad5-a609-42d39a33479e\"],\"scores\":[0.5776186,0.0185266,0.0182808,0.0143907,0.0098805,0.009119,0.0082382,0.0079592,0.0076707,0.0065387]},\"error\":null}\n",
      "{\"input\":{\"userId\":\"5023\"},\"output\":{\"recommendedItems\":[\"441c2a65-4b68-4864-b014-04a9bd9fe08a\",\"new_55763c45-6051-4fff-ae04-495c0fafa8ff\",\"84d6c26d-9760-49d8-854b-0a22becd8241\",\"new_e358575b-7983-4f61-9902-bd44ce9ead6b\",\"c6dd0909-46f3-4cf9-a059-fbdff93198dd\",\"635be5a7-3345-46f9-aa0d-419a6652b0f2\",\"new_7fd05ca4-ac20-46cf-b4f8-87147eef8d65\",\"641f3960-72a7-4e2b-be69-8a7539eb50bb\",\"36e1f150-3d83-4d3c-9855-8daf858d8e28\",\"new_e41fe9b4-db67-4726-9624-335e01098abb\"],\"scores\":[0.6246195,0.1523481,0.0411481,0.0288333,0.0206153,0.0155547,0.0072245,0.0070363,0.0065347,0.0051012]},\"error\":null}\n"
     ]
    }
   ],
   "source": [
    "# - Inspect the outputs of the Inference Jobs\n",
    "\n",
    "print(\"Previewing output of the inference job that was run on the original solution version:\")\n",
    "!head -n 10 $job_output_file_orig\n",
    "\n",
    "print(\"\\n\\nPreviewing output of the inference job that was run on the auto-updated solution version:\")\n",
    "!head -n 10 $job_output_file_au\n",
    "\n",
    "print(\"\\n\\nPreviewing output of the inference job that was run on the fully-trained solution version:\")\n",
    "!head -n 10 $job_output_file_fr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Chapter 7 Step 2**: Compare the metrics across the two solution versions (original, auto-updated, fully-trained)\n",
    "Since the solution version was fully retrained with additional data, we should expect the metrics to be slightly different.\n",
    "\n",
    "We have looked at the metrics for the original solution version, the original solution version after it was auto-updated, and the (new) fully-retrained solution version in previous parts of this notebook. Now, let's display these three sets of metrics side-by-side, so you can easily compare them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics of the original solution version:\n",
      "{\n",
      "  \"coverage\": 0.9968,\n",
      "  \"mean_reciprocal_rank_at_25\": 0.8459,\n",
      "  \"normalized_discounted_cumulative_gain_at_10\": 0.7873,\n",
      "  \"normalized_discounted_cumulative_gain_at_25\": 0.8089,\n",
      "  \"normalized_discounted_cumulative_gain_at_5\": 0.7661,\n",
      "  \"precision_at_10\": 0.1194,\n",
      "  \"precision_at_25\": 0.0541,\n",
      "  \"precision_at_5\": 0.2189\n",
      "}\n",
      "\n",
      "\n",
      "Metrics of the auto-updated solution version:\n",
      "{\n",
      "  \"coverage\": 0.9968,\n",
      "  \"mean_reciprocal_rank_at_25\": 0.8459,\n",
      "  \"normalized_discounted_cumulative_gain_at_10\": 0.7873,\n",
      "  \"normalized_discounted_cumulative_gain_at_25\": 0.8089,\n",
      "  \"normalized_discounted_cumulative_gain_at_5\": 0.7661,\n",
      "  \"precision_at_10\": 0.1194,\n",
      "  \"precision_at_25\": 0.0541,\n",
      "  \"precision_at_5\": 0.2189\n",
      "}\n",
      "\n",
      "\n",
      "Metrics of the fully-retrained solution version:\n",
      "{\n",
      "  \"coverage\": 0.985,\n",
      "  \"mean_reciprocal_rank_at_25\": 0.8425,\n",
      "  \"normalized_discounted_cumulative_gain_at_10\": 0.7377,\n",
      "  \"normalized_discounted_cumulative_gain_at_25\": 0.7709,\n",
      "  \"normalized_discounted_cumulative_gain_at_5\": 0.7077,\n",
      "  \"precision_at_10\": 0.1454,\n",
      "  \"precision_at_25\": 0.0692,\n",
      "  \"precision_at_5\": 0.2561\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Metrics of the original solution version:\")\n",
    "print(json.dumps(get_original_solution_metrics_response['metrics'], indent=2))\n",
    "\n",
    "print(\"\\n\\nMetrics of the auto-updated solution version:\")\n",
    "print(json.dumps(get_autoupdated_solution_metrics_response['metrics'], indent=2))\n",
    "\n",
    "print(\"\\n\\nMetrics of the fully-retrained solution version:\")\n",
    "print(json.dumps(get_retrained_solution_metrics_response['metrics'], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the metrics stay the same from the original solution version to the auto-updated solution version. This is because the underlying model isn't retrained with the new data. What does change is that the new data we imported is used for cold-starts.\n",
    "\n",
    "However, after fully-retraining our solution version (creating a new solution version w/ ```trainingMode='FULL'```), the underlying model *is* retrained. In other words, those new items and new interactions were used for training this solution version, which in turn changes its metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: \n",
    "\n",
    "In this notebook, we walked through the process for updating your items and interactions datasets in Amazon Personalize in the context of Item Recommendation (specifically user-personalization-recipe-backed) use cases.\n",
    "\n",
    "The notebook required some set up. To recap those steps, we:\n",
    "- Set Up Amazon S3 Bucket, IAM Policies, and IAM Roles\n",
    "- Fetched, Inspected and trimmed Datasets \n",
    "- Created Schemas and Imported Datasets in Amazon Personalize\n",
    "- Created an e-commerce custom solution in Amazon Personalize\n",
    "- Ran a Batch Inference Job on your Solution Version (Yielded Output A)\n",
    "\n",
    "After performing some set up and obtaining a baseline for future comparison, we then analyzed how updating items and interactions datasets affects your existing solution versions. We: \n",
    "- Imported new items and new interactions into our Amazon Personalize Dataset Group.\n",
    "- Re-Ran the Inference Job on the Solution Version. Upon submission of the job request, the solution version auto-updated to consider new items and new interactions data. Upon completion of the job, we received Output B and compared it to Output A.\n",
    "- Performed A Full Retraining (by creating a new Solution Version w/ TrainingMode=FULL). This created a new model which allowed Personalize to give greater weight to the new data. \n",
    "- Ran the Batch Inference Job on the new Solution Version (Yielded Output C). We inspected Output C and found that the new items are now more likely to show up in item recommendations.\n",
    "\n",
    "In the analysis portion of this notebook, we:\n",
    "- Did a side-by-side comparison of Outputs A, B, and C. All three were slightly different.\n",
    "- Tracked the changes of the metrics across the solution versions (original solution version, the original solution version after it was auto-updated, and fully-retrained solution version).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Cleanup\n",
    "\n",
    "This chapter will walk through deleting all of the resources created throughout this notebook.\n",
    "\n",
    "First, we will delete the S3 and IAM resources. \n",
    "\n",
    "Then, we will delete the Amazon Personalize resources.\n",
    "\n",
    "Amazon Personalize resources have to deleted in a specific sequence* to avoid dependency errors.\n",
    "The order in which you should delete resources in Amazon Personalize are: recommenders and campaigns, then solutions, then event trackers, then filters, then datasets and dataset schemas, and finally, the dataset group. \n",
    "\n",
    "To declutter this notebook, we will be leveraging a utility module written in python that provides an orderly delete process for deleting all resources in each dataset group.\n",
    "\n",
    "This section should take about 15 minutes, though most of this time will be spent waiting for the Personalize resources to finish deleting.\n",
    "\n",
    "*: Note, we didn't use some of these resource types (such as recommenders and campaigns) in this notebook. The list is just for your knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emptying and Deleting the S3 bucket\n",
    "\n",
    "NOTE: THE FOLLOWING CODE WILL DELETE ALL OF THE OBJECTS, INCLUDING THE CSVs & BATCH JOB FILES. \n",
    "If you dont want to delete the S3 bucket, DONT run the code block below.\n",
    "\n",
    "Alternatively, if you want to delete the bucket, consider directly downloading the files (eg: via the Console or CLI) to \n",
    "persist your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting batch-job-input/au_job_input.json...\n",
      "Deleting batch-job-input/fr_job_input.json...\n",
      "Deleting batch-job-input/orig_job_input.json...\n",
      "Deleting batch-job-outputs/auto-update/_CHECK...\n",
      "Deleting batch-job-outputs/auto-update/au_job_input.json.out...\n",
      "Deleting batch-job-outputs/fully-retrained/_CHECK...\n",
      "Deleting batch-job-outputs/fully-retrained/fr_job_input.json.out...\n",
      "Deleting batch-job-outputs/original/_CHECK...\n",
      "Deleting batch-job-outputs/original/orig_job_input.json.out...\n",
      "Deleting interactions.csv...\n",
      "Deleting interactions_trimmed.csv...\n",
      "Deleting items.csv...\n",
      "Deleting items_trimmed.csv...\n",
      "Deleting users.csv...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'A6JT9K4G6T33DBVA',\n",
       "  'HostId': 'qyefAeFHe/HX95f5a9kY/oFHj11Q26uC+hz7szE3byH3rM9XUPvatFtTWgSvEVQHFna6FTBhJDIZJOCUx8bS1Q==',\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'qyefAeFHe/HX95f5a9kY/oFHj11Q26uC+hz7szE3byH3rM9XUPvatFtTWgSvEVQHFna6FTBhJDIZJOCUx8bS1Q==',\n",
       "   'x-amz-request-id': 'A6JT9K4G6T33DBVA',\n",
       "   'date': 'Fri, 27 Oct 2023 17:14:39 GMT',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List objects in the bucket and delete them\n",
    "objects = s3.list_objects_v2(Bucket=bucket_name)\n",
    "\n",
    "if 'Contents' in objects:\n",
    "    for obj in objects['Contents']:\n",
    "        print(f'Deleting {obj[\"Key\"]}...')\n",
    "        s3.delete_object(Bucket=bucket_name, Key=obj['Key'])\n",
    "\n",
    "# Delete the bucket\n",
    "s3.delete_bucket(Bucket=bucket_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete the IAM Execution Role and Policy\n",
    "\n",
    "Now, lets delete the IAM Policy and IAM Role that we created for the Personalize Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting: PersonalizeRole-85968\n",
      "Deleting: arn:aws:iam::402114309305:policy/PersonalizePolicy-85968\n",
      "Bucket and IAM role and policy deleted successfully!\n"
     ]
    }
   ],
   "source": [
    "iam.detach_role_policy(RoleName=role_name, PolicyArn=policy_arn)\n",
    "\n",
    "print(\"Deleting: \" + role_name)\n",
    "iam.delete_role(RoleName=role_name)\n",
    "\n",
    "print(\"Deleting: \" + policy_arn)\n",
    "iam.delete_policy(PolicyArn=policy_arn)\n",
    "\n",
    "# Check if the IAM role and policy were deleted\n",
    "if role_name in [role['RoleName'] for role in iam.list_roles()['Roles']]:\n",
    "    print('Role was not deleted!')\n",
    "    \n",
    "if policy_arn in [policy['Arn'] for policy in iam.list_policies()['Policies']]:\n",
    "    print('Policy was not deleted!')\n",
    "    \n",
    "print('Bucket and IAM role and policy deleted successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up deletion script\n",
    "\n",
    "Next we will download a helper script that will simplify the cleanup process. If you want to look at the underlying code, the helper script can be found in the [retail-demo-store github repo](https://github.com/aws-samples/retail-demo-store/blob/b80137c6edb2c975c50221fcaba46b6abadd7b99/src/aws-lambda/personalize-pre-create-resources/delete_dataset_groups.py).\n",
    "\n",
    "Note: Under the hood, this script uses the 'delete_*' Personalize API boto3 commands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 18958  100 18958    0     0   147k      0 --:--:-- --:--:-- --:--:--  146k\n"
     ]
    }
   ],
   "source": [
    "# Download the helper script from the github repo.\n",
    "!curl -O https://raw.githubusercontent.com/aws-samples/retail-demo-store/b80137c6edb2c975c50221fcaba46b6abadd7b99/src/aws-lambda/personalize-pre-create-resources/delete_dataset_groups.py\n",
    "\n",
    "# Import the module\n",
    "import delete_dataset_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up logging for deletion of Personalize resources\n",
    "The following code cell ensures we import and set up the native python logging module. Our resource deletion script requires this so that we can provide information about the deletion status of Personalize resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "\n",
    "delete_dataset_groups.logger.setLevel(logging.INFO)\n",
    "delete_dataset_groups.logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Amazon Personalize Resources\n",
    "\n",
    "Now we can delete the active dataset groups. This can take up to 10 minutes depending on the resources within your dataset group. The function below will log its progress until finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active dataset groups that need to be deleted: retaildemostore-products-DSG-85968\n",
      "\n",
      "Dataset Group ARN: arn:aws:personalize:us-east-1:402114309305:dataset-group/retaildemostore-products-DSG-85968\n",
      "All recommenders have been deleted or none exist for dataset group\n",
      "All campaigns have been deleted or none exist for dataset group\n",
      "Deleting solution: arn:aws:personalize:us-east-1:402114309305:solution/retaildemostore-user-personalization-solution-85968\n",
      "Waiting for 1 solution(s) to be deleted\n",
      "Waiting for 1 solution(s) to be deleted\n",
      "Waiting for 1 solution(s) to be deleted\n",
      "All solutions have been deleted or none exist for dataset group\n",
      "All event trackers have been deleted or none exist for dataset group\n",
      "All filters have been deleted or none exist for dataset group\n",
      "Deleting dataset arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85968/ITEMS\n",
      "Deleting dataset arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85968/USERS\n",
      "Deleting dataset arn:aws:personalize:us-east-1:402114309305:dataset/retaildemostore-products-DSG-85968/INTERACTIONS\n",
      "Waiting for 3 dataset(s) to be deleted\n",
      "Waiting for 1 dataset(s) to be deleted\n",
      "All datasets have been deleted or none exist for dataset group\n",
      "Deleting schema arn:aws:personalize:us-east-1:402114309305:schema/retaildemostore-products-items-schema-85968\n",
      "Deleting schema arn:aws:personalize:us-east-1:402114309305:schema/retaildemostore-products-users-schema-85968\n",
      "Deleting schema arn:aws:personalize:us-east-1:402114309305:schema/retaildemostore-products-interactions-schema-85968\n",
      "All schemas used exclusively by datasets have been deleted or none exist for dataset group\n",
      "Deleting dataset group arn:aws:personalize:us-east-1:402114309305:dataset-group/retaildemostore-products-DSG-85968\n",
      "Waiting for dataset group to be deleted\n",
      "Waiting for dataset group to be deleted\n",
      "Waiting for dataset group to be deleted\n",
      "Dataset group arn:aws:personalize:us-east-1:402114309305:dataset-group/retaildemostore-products-DSG-85968 has been fully deleted\n",
      "Dataset group retaildemostore-products-DSG-85968 fully deleted\n",
      "CPU times: user 155 ms, sys: 12.7 ms, total: 168 ms\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(f'Active dataset groups that need to be deleted: {dataset_group_name}\\n')\n",
    "\n",
    "delete_dataset_groups.delete_dataset_groups(\n",
    "    dataset_group_names = [ dataset_group_name ], \n",
    "    wait_for_resources = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete local files\n",
    "If you want to retain these files, dont run the following cell block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm delete_dataset_groups.py\n",
    "\n",
    "# Delete input & output files from original solution version\n",
    "!rm {orig_job_input_filename} # \"orig_job_input.json\"\n",
    "!rm {job_output_file_orig}    # \"orig_job_input.json.out\"\n",
    "\n",
    "# Delete input & output files from the auto-updated original solution version\n",
    "!rm {au_job_input_filename} # \"au_job_input.json\"\n",
    "!rm {job_output_file_au}    # \"au_job_input.json.out\"\n",
    "\n",
    "# Delete input & output files from the auto-updated original solution version\n",
    "!rm {fr_job_input_filename} # \"fr_job_input.json\"\n",
    "!rm {job_output_file_fr}    # \"fr_job_input.json.out\"\n",
    "\n",
    "# Delete the locally-saved csv files\n",
    "!rm {items_filename}\n",
    "!rm {interactions_filename}\n",
    "\n",
    "!rm {items_trimmed_filename}\n",
    "!rm {interactions_trimmed_filename}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Complete\n",
    "\n",
    "All resources created by this Personalize Notebook have been deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final note:\n",
    "If you are running this notebook on Amazon Sagemaker, don't forget to `stop` or `terminate` your sagemaker instance so that you don't incur additional costs.\n",
    "Afterwards, feel free to delete the execution role of this Sagemaker notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congrats on completing this Personalize Demonstration! \n",
    "If you are further interested in learning how you can leverage Amazon Personalize to power your business's ML-powered recommendation services, refer to the [AWS Documentation on Amazon Personlize](https://docs.aws.amazon.com/personalize/latest/dg/what-is-personalize.html).\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

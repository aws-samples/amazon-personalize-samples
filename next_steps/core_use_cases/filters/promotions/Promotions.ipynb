{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f760989d",
   "metadata": {},
   "source": [
    "# Including promoted items in recommendations\n",
    "\n",
    "This notebook will walk you through an example of using promotions in [Amazon Personalize](https://aws.amazon.com/personalize/) to ensure the results for a recommender contain specific items that you want users to see. \n",
    "\n",
    "Personalized recommendations increase user engagement with your websites or apps, but often there are additional business rules you want to apply when deciding what items you want to present to your users.  [Amazon Personalize promotions](https://docs.aws.amazon.com/personalize/latest/dg/promoting-items.html) can help when you want a portion of the items presented to users to be from specific categories.  For example, a video on demand app may want to include promotions for new shows or an ecommerce website might want to include sale items in reommendations.  \n",
    "\n",
    "![promotions-overview.png](images/promotions-overview.png \"Diagram showing how promotions changes the result of recommended items\")\n",
    "\n",
    "First, we'll follow the steps to build a Domain dataset group and a recommender that returns product recommendations based on synthetic data generated for a fictitious retail store data set. The goal is to recommend products that are relevant for each particular user.\n",
    "\n",
    "Then, we'll create a promotions filter to ensure that certain promoted items are present in the recommended items for each user.\n",
    "\n",
    "Finally, we'll cleanup all of the resources we created so we avoid incurring costs for resources that are no longer being used.  \n",
    "\n",
    "The estimated time to run through this notebook is about 40 minutes.  \n",
    "\n",
    "## How to use the Notebook\n",
    "\n",
    "The code is broken up into cells like the one below. There's a triangular Run button at the top of this page that you can click to execute each cell and move onto the next, or you can press `Shift` + `Enter` while in the cell to execute it and move onto the next one.\n",
    "\n",
    "As a cell is executing you'll notice a line to the side showcase an `*` while the cell is running or it will update to a number to indicate the last cell that completed executing after it has finished exectuting all the code within a cell.\n",
    "\n",
    "Simply follow the instructions below and execute the cells to get started.\n",
    "\n",
    "## Introduction to Amazon Personalize\n",
    "\n",
    "[Amazon Personalize](https://aws.amazon.com/personalize/) makes it easy for customers to develop applications with a wide array of personalization use cases, including real time product recommendations and customized direct marketing. Amazon Personalize brings the same machine learning technology used by Amazon.com to everyone for use in their applications â€“ with no machine learning experience required. Amazon Personalize customers pay for what they use, with no minimum fees or upfront commitment. \n",
    "\n",
    "You can start using Amazon Personalize with a simple three step process, which only takes a few clicks in the AWS console, or a set of simple API calls. \n",
    "\n",
    "First, point Amazon Personalize to user data, catalog data, and activity stream of views, clicks, purchases, etc. in Amazon S3 or upload using a simple API call. \n",
    "\n",
    "Second, with a single click in the console or an API call, train a private recommendation model for your data. \n",
    "\n",
    "Third, retrieve personalized recommendations for any user by creating a recommender, and using the GetRecommendations API.\n",
    "\n",
    "If you are not familiar with Amazon Personalize, you can learn more about the service on by looking at [Github Sample Notebooks](https://github.com/aws-samples/amazon-personalize-samples) and [Product Documentation](https://docs.aws.amazon.com/personalize/latest/dg/what-is-personalize.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ca987",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Python ships with a broad collection of libraries and we need to import those as well as the ones installed to help us like [boto3](https://aws.amazon.com/sdk-for-python/) (AWS SDK for python) and [Pandas](https://pandas.pydata.org/)/[Numpy](https://numpy.org/) which are core data science tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb0420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest version of botocore to ensure we have the latest features in the SDK\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade --no-deps --force-reinstall botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca0fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af0b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SDK to Personalize:\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime3 = boto3.client('personalize-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12983cae",
   "metadata": {},
   "source": [
    "Next you will want to validate that your environment can communicate successfully with Amazon Personalize, the lines below do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.list_dataset_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e3ff5",
   "metadata": {},
   "source": [
    "Setup names for items and interactions files to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a59b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_file_path = 'cleaned_interactions_training_data.csv'\n",
    "items_file_path = 'cleaned_item_training_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0765d343",
   "metadata": {},
   "source": [
    "## Specify an S3 Bucket and Data Output Location\n",
    "\n",
    "Amazon Personalize will need an S3 bucket to act as the source of your data. The code bellow will create a bucket with a unique `bucket_name`.\n",
    "\n",
    "The Amazon S3 bucket needs to be in the same region as the Amazon Personalize resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the same region as current Amazon SageMaker Notebook\n",
    "with open('/opt/ml/metadata/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "print('region:', region)\n",
    "\n",
    "# Or you can specify the region where your bucket and model will be domiciled\n",
    "# region = \"us-east-1\" \n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket_name = account_id + \"-\" + region + \"-\" + \"personalizemanagedretailers\"\n",
    "print('bucket_name:', bucket_name)\n",
    "\n",
    "try: \n",
    "    if region == \"us-east-1\":\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    else:\n",
    "        s3.create_bucket(\n",
    "            Bucket = bucket_name,\n",
    "            CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "except s3.exceptions.BucketAlreadyOwnedByYou:\n",
    "    print(\"Bucket already exists. Using bucket\", bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b430601",
   "metadata": {},
   "source": [
    "## Download, Prepare, and Upload Training Data\n",
    "\n",
    "We generated the synthetic data based on the code in the [Retail Demo Store project](https://github.com/aws-samples/retail-demo-store). Follow the link to learn more about the data and potential uses.\n",
    "\n",
    "First we need to download the data (training data). In this tutorial we'll use the Purchase history from a retail store  dataset. The dataset contains the user_id, item_id, the interactions between customers and items and the time this interaction took place (Timestamp).\n",
    "\n",
    "### Download and Explore the Interactions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec851d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://retail-demo-store-us-east-1/csvs/interactions.csv ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae49d831",
   "metadata": {},
   "source": [
    "The dataset has been successfully downloaded as interactions.csv\n",
    "\n",
    "Lets learn more about the dataset by viewing its charateristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475cb0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./interactions.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66032111",
   "metadata": {},
   "source": [
    "From the cells above, we've learned that our data has has 5 columns, 675004 rows and the headers are: ITEM_ID, USER_ID, EVENT_TYPE, TIMESTAMP and DISCOUNT.\n",
    "\n",
    "To be compatible with an Amazon Personalize interactions schema, this dataset requires column headings compatible with Amazon Personalize default column names (read about column names [here](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html) )\n",
    "\n",
    "The ECOMMERCE recommenders require you to provide specific EVENT_TYPE values in order to understand the context of an interaction. Let's look at what event types are currently in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383fa167",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.EVENT_TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9033359c",
   "metadata": {},
   "source": [
    "We can see that 'View' and 'Purchase' event are present and we can proceed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bee3c96",
   "metadata": {},
   "source": [
    "### Prepare the Interactions Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b067f",
   "metadata": {},
   "source": [
    "### Drop Columns\n",
    "\n",
    "Some columns in this dataset would not add value to our model and as such need to be dropped from this dataset. Columns such as *discount*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65efe415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test=df.drop(columns=['DISCOUNT'])\n",
    "df=test\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e13d9db",
   "metadata": {},
   "source": [
    "In the cell below, we will write our cleaned data to a file named \"final_training_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ad8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(interactions_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3b13d",
   "metadata": {},
   "source": [
    "### Download and Explore the Items Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89324993",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://retail-demo-store-us-east-1/csvs/items.csv ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1fa09",
   "metadata": {},
   "source": [
    "The dataset has been successfully downloaded as items.csv\n",
    "\n",
    "Lets learn more about the dataset by viewing its charateristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9588db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = pd.read_csv('./items.csv')\n",
    "items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b6c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fda300e",
   "metadata": {},
   "source": [
    "Let's explore the kinds of items included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3cf7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.CATEGORY_L1.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a631e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.CATEGORY_L2.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d55069",
   "metadata": {},
   "source": [
    "### Drop Columns\n",
    "\n",
    "Some columns in this dataset could add value to our model but are not relevant for this example. For simplicity, we will drop them from this dataset. Columns such as *product_decription*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79effaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=items_df.drop(columns=['PRODUCT_DESCRIPTION'])\n",
    "items_df=test\n",
    "items_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef827e5b",
   "metadata": {},
   "source": [
    "Write our cleaned data to a .csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcf134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.to_csv(items_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966bbd6",
   "metadata": {},
   "source": [
    "## Configure an S3 bucket and an IAM role\n",
    "\n",
    "So far, we have downloaded, manipulated, and saved the data onto the Amazon EBS instance attached to instance running this Jupyter notebook. However, Amazon Personalize will need an S3 bucket to act as the source of your data, as well as IAM roles for accessing that bucket. Let's set all of that up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b13fe",
   "metadata": {},
   "source": [
    "## Set the S3 bucket policy\n",
    "Amazon Personalize needs to be able to read the contents of your S3 bucket. So add a bucket policy which allows that.\n",
    "\n",
    "Note: Make sure the role you are using to run the code in this notebook has the necessary permissions to modify the S3 bucket policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket_name),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket_name)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2884e83",
   "metadata": {},
   "source": [
    "### Upload Interactions data to S3\n",
    "Now that our training data is ready for Amazon Personalize,the next step is to upload it to the s3 bucket created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ee211",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(interactions_file_path).upload_file(interactions_file_path)\n",
    "interactions_s3DataPath = \"s3://\"+bucket_name+\"/\"+interactions_file_path\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb04ad2",
   "metadata": {},
   "source": [
    "### Upload Items data to S3\n",
    "Now that our training data is ready for Amazon Personalize,the next step is to upload it to the s3 bucket created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(items_file_path).upload_file(items_file_path)\n",
    "items_s3DataPath = \"s3://\"+bucket_name+\"/\"+items_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbe117d",
   "metadata": {},
   "source": [
    "## Create and Wait for Dataset Group\n",
    "The largest grouping in Personalize is a Dataset Group, this will isolate your data, event trackers, solutions, Recommenders, and campaigns. Grouping things together that share a common collection of data. Feel free to alter the name below if you'd like. \n",
    "\n",
    "When you create a Domain dataset group, you choose your domain. The domain you specify determines the default schemas for datasets and the use cases that are available for recommenders. \n",
    "\n",
    "You can find more information about creating a Domain dataset group in [the documentation](https://docs.aws.amazon.com/personalize/latest/dg/create-domain-dataset-group.html).\n",
    "\n",
    "### Create Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeed98c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = personalize.create_dataset_group(\n",
    "    name='personalize_ecomemerce_ds_group',\n",
    "    domain='ECOMMERCE'\n",
    ")\n",
    "\n",
    "dataset_group_arn = response['datasetGroupArn']\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757bcc10",
   "metadata": {},
   "source": [
    "Wait for Dataset Group to Have ACTIVE Status\n",
    "Before we can use the Dataset Group in any items below it must be active, execute the cell below and wait for it to show active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b6c37",
   "metadata": {},
   "source": [
    "## Create Interactions Schema\n",
    "A core component of how Personalize understands your data comes from the Schema that is defined below. This configuration tells the service how to digest the data provided via your CSV file. Note the columns and types align to what was in the file you created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591cd9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"EVENT_TYPE\",\n",
    "            \"type\": \"string\"\n",
    "            \n",
    "        }\n",
    "        \n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"personalize-ecommerce-interatn_group\",\n",
    "    domain = \"ECOMMERCE\",\n",
    "    schema = json.dumps(interactions_schema)\n",
    ")\n",
    "\n",
    "interaction_schema_arn = create_schema_response['schemaArn']\n",
    "print(json.dumps(create_schema_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e58dc0b",
   "metadata": {},
   "source": [
    "## Create Items Schema\n",
    "A core component of how Personalize understands your data comes from the Schema that is defined below. This configuration tells the service how to digest the data provided via your CSV file. Note the columns and types align to what was in the file you created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a791d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Items\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PRICE\",\n",
    "            \"type\": \"float\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CATEGORY_L1\",\n",
    "            \"type\": [\"string\"],\n",
    "            \"categorical\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CATEGORY_L2\",\n",
    "            \"type\": [\"string\"],\n",
    "            \"categorical\": True\n",
    "            \n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENDER\",\n",
    "            \"type\": [\"string\"],\n",
    "            \"categorical\": True\n",
    "            \n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"personalize-ecommerce-item_group\",\n",
    "    domain = \"ECOMMERCE\",\n",
    "    schema = json.dumps(items_schema)\n",
    ")\n",
    "\n",
    "items_schema_arn = create_schema_response['schemaArn']\n",
    "\n",
    "print(json.dumps(create_schema_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd41ad",
   "metadata": {},
   "source": [
    "## Create Datasets\n",
    "After the group, the next thing to create is the datasets where your data will be uploaded to in Amazon Personalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad88e7d",
   "metadata": {},
   "source": [
    "### Create Interactions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885bb616",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"INTERACTIONS\"\n",
    "\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    name = \"personalize_ecommerce_demo_interactions\",\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = interaction_schema_arn\n",
    ")\n",
    "\n",
    "interactions_dataset_arn = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7001e1",
   "metadata": {},
   "source": [
    "### Create Items Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"ITEMS\"\n",
    "\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    name = \"personalize_ecommerce_demo_items\",\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = items_schema_arn\n",
    ")\n",
    "\n",
    "items_dataset_arn = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c72c02",
   "metadata": {},
   "source": [
    "## Create Personalize Role\n",
    "Also Amazon Personalize needs the ability to assume Roles in AWS in order to have the permissions to execute certain tasks, the lines below grant that.\n",
    "\n",
    "Note: Make sure the role you are using to run the code in this notebook has the necessary permissions to create a role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "role_name = \"PersonalizeRoleEcommerceDemoRecommender\"\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": \"personalize.amazonaws.com\"\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName = role_name,\n",
    "    AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    ")\n",
    "\n",
    "# AmazonPersonalizeFullAccess provides access to any S3 bucket with a name that includes \"personalize\" or \"Personalize\" \n",
    "# if you would like to use a bucket with a different name, please consider creating and attaching a new policy\n",
    "# that provides read access to your bucket or attaching the AmazonS3ReadOnlyAccess policy to the role\n",
    "policy_arn = \"arn:aws:iam::aws:policy/service-role/AmazonPersonalizeFullAccess\"\n",
    "iam.attach_role_policy(\n",
    "    RoleName = role_name,\n",
    "    PolicyArn = policy_arn\n",
    ")\n",
    "\n",
    "# Now add S3 support\n",
    "iam.attach_role_policy(\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonS3FullAccess',\n",
    "    RoleName=role_name\n",
    ")\n",
    "time.sleep(60) # wait for a minute to allow IAM role policy attachment to propagate\n",
    "\n",
    "role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "print(role_arn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616f86ce",
   "metadata": {},
   "source": [
    "## Import the data\n",
    "Earlier you created the DatasetGroup and Dataset to house your information, now you will execute an import job that will load the data from S3 into Amazon Personalize for usage building your model.\n",
    "### Create Interactions Dataset Import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e902d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_interactions_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"personalize_ecommerce_demo_interactions_import\",\n",
    "    datasetArn = interactions_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, interactions_file_path)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_interactions_import_job_arn = create_interactions_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_interactions_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d20685",
   "metadata": {},
   "source": [
    "### Create Items Dataset Import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432da810",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_items_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"personalize_ecommerce_demo_items_import\",\n",
    "    datasetArn = items_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, items_file_path)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_items_import_job_arn = create_items_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_items_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06519930",
   "metadata": {},
   "source": [
    "### Wait for Dataset Import Jobs to Have ACTIVE Status\n",
    "It can take a while before the import jobs complete, please wait until you see that they are active below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26369f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_items_import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"ItemsDatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_interactions_import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"InteractionsDatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df7d504",
   "metadata": {},
   "source": [
    "## Choose a recommender use case\n",
    "\n",
    "Each domain has different use cases. When you create a recommender you create it for a specific use case, and each use case has different requirements for getting recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6bdb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_recipes = personalize.list_recipes(domain='ECOMMERCE') # See a list of recommenders for the domain. \n",
    "display (available_recipes['recipes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2960a2b6",
   "metadata": {},
   "source": [
    "We are going to create a recommender of the type \"Recommended For You\". This type of recommender offers personalized recommendations for items based on a user that you specify. With this use case, Amazon Personalize automatically filters items the user purchased based on the userId that you specify and `Purchase` events.\n",
    "\n",
    "[More use cases per domain](https://docs.aws.amazon.com/personalize/latest/dg/domain-use-cases.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef90d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_recommender_response = personalize.create_recommender(\n",
    "  name = 'recommended_for_you_demo',\n",
    "  recipeArn = 'arn:aws:personalize:::recipe/aws-ecomm-recommended-for-you',\n",
    "  datasetGroupArn = dataset_group_arn\n",
    ")\n",
    "recommended_for_you_arn = create_recommender_response[\"recommenderArn\"]\n",
    "print (json.dumps(create_recommender_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78af7483",
   "metadata": {},
   "source": [
    "We wait until the recomenders have finished creating and have status `ACTIVE`. We check periodically on the status of the recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a86c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 10*60*60 # 10 hours\n",
    "    \n",
    "while time.time() < max_time:\n",
    "\n",
    "    version_response = personalize.describe_recommender(\n",
    "        recommenderArn = recommended_for_you_arn\n",
    "    )\n",
    "    status = version_response[\"recommender\"][\"status\"]\n",
    "    print(status)\n",
    "\n",
    "    if status == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(recommended_for_you_arn))\n",
    "        \n",
    "    elif status == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(recommended_for_you_arn))\n",
    "        break\n",
    "\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "    else:\n",
    "        print('The \"Recommended for you\" Recommender build is still in progress')\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1146c9e",
   "metadata": {},
   "source": [
    "## Getting recommendations with a recommender\n",
    "Now that the recommender has been trained, lets have a look at the recommendations we can get for our users!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa6533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the original data in order to have a dataframe that has both item_ids \n",
    "# and the corresponding titles to make our recommendations easier to read.\n",
    "items_df = pd.read_csv('./items.csv')\n",
    "items_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_by_id(item_id, item_df):\n",
    "    \"\"\"\n",
    "    This takes in an item_id from a recommendation in string format,\n",
    "    converts it to an int, and then does a lookup in a default or specified\n",
    "    dataframe and returns the item description.\n",
    "    \n",
    "    A really broad try/except clause was added in case anything goes wrong.\n",
    "    \n",
    "    Feel free to add more debugging or filtering here to improve results if\n",
    "    you hit an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return items_df.loc[items_df[\"ITEM_ID\"]==str(item_id)]['PRODUCT_DESCRIPTION'].values[0]\n",
    "    except:\n",
    "        print (item_id)\n",
    "        return \"Error obtaining item description\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b1cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_by_id(item_id, item_df):\n",
    "    \"\"\"\n",
    "    This takes in an item_id from a recommendation in string format,\n",
    "    converts it to an int, and then does a lookup in a default or specified\n",
    "    dataframe and returns the item category.\n",
    "    \n",
    "    A really broad try/except clause was added in case anything goes wrong.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        return items_df.loc[items_df[\"ITEM_ID\"]==str(item_id)]['CATEGORY_L2'].values[0]\n",
    "    except:\n",
    "        print (item_id)\n",
    "        return \"Error obtaining item category\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cb379f",
   "metadata": {},
   "source": [
    "Let us get some  recommendations from the recommender returning \"Recommended for you\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First pick a user\n",
    "test_user_id = \"777\" \n",
    "\n",
    "# Get recommendations for the user\n",
    "get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "    recommenderArn = recommended_for_you_arn,\n",
    "    userId = test_user_id,\n",
    "    numResults = 20\n",
    ")\n",
    "\n",
    "# Build a new dataframe for the recommendations\n",
    "item_list = get_recommendations_response['itemList']\n",
    "recommendation_id_list = []\n",
    "recommendation_description_list = []\n",
    "recommendation_category_list = []\n",
    "\n",
    "for item in item_list:\n",
    "    description = get_item_by_id(item['itemId'], items_df)\n",
    "    recommendation_description_list.append(description)\n",
    "    recommendation_id_list.append(item['itemId'])\n",
    "    recommendation_category_list.append(get_category_by_id(item['itemId'], items_df))\n",
    "\n",
    "user_recommendations_df = pd.DataFrame(recommendation_id_list, columns = [\"ID\"])\n",
    "user_recommendations_df[\"description\"] = recommendation_description_list\n",
    "user_recommendations_df[\"category level 2\"] = recommendation_category_list\n",
    "\n",
    "pd.options.display.max_rows =20\n",
    "display(user_recommendations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff9ddd9",
   "metadata": {},
   "source": [
    "## Using promotions with a recommender\n",
    "Now, lets create a promotion to ensure that recommendations for users contain specific items we want to promote.  In this example our ecommerce store is promoting items for the upcoming Halloween holiday.  So, we want to make sure that users see halloween items.\n",
    "\n",
    "For more information on how to use promotions, please refer to [the documentation](https://docs.aws.amazon.com/personalize/latest/dg/promoting-items.html). \n",
    "\n",
    "### Create a filter to use with the promotion\n",
    "\n",
    "First we need to create a filter that will define what items will be promoted. We will use a dynamic [filter](https://docs.aws.amazon.com/personalize/latest/dg/filter-expressions.html) that takes in a value at inference time. \n",
    "\n",
    "This filter will include only items that have the specified value for *Items.CATEGORY_L2*.\n",
    "\n",
    "To get the same results we could also use a static filter of the form:\n",
    "\n",
    "```\n",
    "'INCLUDE ItemID WHERE Items.CATEGORY_L2 IN (\"halloween\")'\n",
    "```\n",
    "\n",
    "however, using the dynamic filter gives the customer more flexibility if they later want to promote items in a different category without having to create a different filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed562d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_filter_response = personalize.create_filter(\n",
    "    name = 'category_filter',\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    filterExpression = 'INCLUDE ItemID WHERE Items.CATEGORY_L2 IN ($CATEGORY)'\n",
    ") \n",
    "filter_arn = create_filter_response[\"filterArn\"]\n",
    "print(\"Filter ARN: \" + filter_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59294595",
   "metadata": {},
   "source": [
    "Wait for the filter we created to have status \"Active\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0da60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 10*60*60 # 10 hours\n",
    "    \n",
    "while time.time() < max_time:\n",
    "    version_response = personalize.describe_filter(\n",
    "        filterArn = filter_arn\n",
    "    )\n",
    "    status = version_response[\"filter\"][\"status\"]\n",
    "\n",
    "    if status == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(filter_arn))\n",
    "        \n",
    "    elif status == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(filter_arn))\n",
    "        break\n",
    "\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "    else:\n",
    "        print('The Filter build is still in progress')\n",
    "        \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40388118",
   "metadata": {},
   "source": [
    "### Get recommendations for our test user using the filter \n",
    "\n",
    "Now that the filter has been created, we can get recommendations for a user using the filter. Let's check our existing test user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be991bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "    recommenderArn = recommended_for_you_arn,\n",
    "    userId = test_user_id,\n",
    "    numResults = 20,\n",
    "    filterArn = filter_arn,\n",
    "    filterValues={\"CATEGORY\" : \"\\\"halloween\\\"\"}\n",
    ")\n",
    "user_recommendations_df =[]\n",
    "\n",
    "# Build a new dataframe for the recommendations\n",
    "item_list = get_recommendations_response['itemList']\n",
    "recommendation_id_list = []\n",
    "recommendation_description_list = []\n",
    "recommendation_category_list = []\n",
    "\n",
    "for item in item_list:\n",
    "    description = get_item_by_id(item['itemId'], items_df)\n",
    "    recommendation_description_list.append(description)\n",
    "    recommendation_id_list.append(item['itemId'])\n",
    "    recommendation_category_list.append(get_category_by_id(item['itemId'], items_df))\n",
    "\n",
    "user_recommendations_df = pd.DataFrame(recommendation_id_list, columns = [\"ID\"])\n",
    "user_recommendations_df[\"description\"] = recommendation_description_list\n",
    "user_recommendations_df[\"category level 2\"] = recommendation_category_list\n",
    "\n",
    "pd.options.display.max_rows =20\n",
    "display(user_recommendations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3984415e",
   "metadata": {},
   "source": [
    "As you can see from the results, all returned items have the category level 2 \"halloween\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bbc039",
   "metadata": {},
   "source": [
    "### Get recommendations using the same filter with promotions\n",
    "\n",
    "We want to make personalized recommendations for each user, but instead of having all items be of a certain category, we want to still include some items for the Halloween promotion.  Let's use _promotions_ to make it happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a0f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "    recommenderArn = recommended_for_you_arn,\n",
    "    userId = test_user_id,\n",
    "    numResults = 20,\n",
    "    promotions = [{\n",
    "        \"name\" : \"halloween_promotion\",\n",
    "        \"percentPromotedItems\" : 20,\n",
    "        \"filterArn\": filter_arn,\n",
    "        \"filterValues\": {\n",
    "            \"CATEGORY\" : \"\\\"halloween\\\"\"\n",
    "        }\n",
    "    }]\n",
    ")\n",
    "\n",
    "\n",
    "# Build a new dataframe for the recommendations\n",
    "item_list = get_recommendations_response['itemList']\n",
    "recommendation_id_list = []\n",
    "recommendation_description_list = []\n",
    "recommendation_category_list = []\n",
    "\n",
    "for item in item_list:\n",
    "    description = get_item_by_id(item['itemId'], items_df)\n",
    "    recommendation_description_list.append(description)\n",
    "    recommendation_id_list.append(item['itemId'])\n",
    "    recommendation_category_list.append(get_category_by_id(item['itemId'], items_df))\n",
    "\n",
    "user_recommendations_df = pd.DataFrame(recommendation_id_list, columns = [\"ID\"])\n",
    "user_recommendations_df[\"description\"] = recommendation_description_list\n",
    "user_recommendations_df[\"category level 2\"] = recommendation_category_list\n",
    "\n",
    "pd.options.display.max_rows =20\n",
    "display(user_recommendations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbab006",
   "metadata": {},
   "source": [
    "### Combine filters and promotions\n",
    "\n",
    "You can use a filter combined with promotions. The filter in the the top-level parameter block applies to only to the non-promoted items. The filter to select the promoted items is specified in the `promotions` parameter block. The following example uses the same dynamic filter we have been using twice. The first filter applies to non-promoted items, selecting items of the categry level 2 \"decorative\", and the second filter applies to the promotion, promoting items of the category level 2 \"halloween\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc31280",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "    recommenderArn=recommended_for_you_arn,\n",
    "    userId=test_user_id,\n",
    "    numResults=20,\n",
    "    filterArn=filter_arn,\n",
    "    filterValues={\n",
    "        \"CATEGORY\": \"\\\"decorative\\\"\"\n",
    "    },\n",
    "    promotions=[{\n",
    "        \"name\": \"halloween_promotion\",\n",
    "        \"percentPromotedItems\": 20,\n",
    "        \"filterArn\": filter_arn,\n",
    "        \"filterValues\": {\n",
    "            \"CATEGORY\": \"\\\"halloween\\\"\"\n",
    "        }\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Build a new dataframe for the recommendations\n",
    "item_list = get_recommendations_response['itemList']\n",
    "recommendation_id_list = []\n",
    "recommendation_description_list = []\n",
    "recommendation_category_list = []\n",
    "\n",
    "for item in item_list:\n",
    "    description = get_item_by_id(item['itemId'], items_df)\n",
    "    recommendation_description_list.append(description)\n",
    "    recommendation_id_list.append(item['itemId'])\n",
    "    recommendation_category_list.append(\n",
    "        get_category_by_id(item['itemId'], items_df))\n",
    "\n",
    "user_recommendations_df = pd.DataFrame(recommendation_id_list, columns=[\"ID\"])\n",
    "user_recommendations_df[\"description\"] = recommendation_description_list\n",
    "user_recommendations_df[\"category level 2\"] = recommendation_category_list\n",
    "\n",
    "pd.options.display.max_rows = 20\n",
    "display(user_recommendations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a4725",
   "metadata": {},
   "source": [
    "## Review\n",
    "Using the codes above you have successfully trained a deep learning model to generate item recommendations based on prior user behavior. You have created a recommenders for a foundational use case and you have used filters and [promotions](https://docs.aws.amazon.com/personalize/latest/dg/promoting-items.html) to apply additional business rules for items that should be presented to users.\n",
    "\n",
    "Going forward, you can adapt this code to create other recommenders.\n",
    "\n",
    "If you are done with this sample, make sure to follow the steps in the next section to cleanup the resources created in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa077309",
   "metadata": {},
   "source": [
    "## Cleanup Resources \n",
    "This section contains instructions on how to clean up the resources created in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231035cc",
   "metadata": {},
   "source": [
    "### Save resource information for cleanup:\n",
    "There are a few values you will need for the next notebook, execute the cell below to store them so they can be used in the `Clean_Up_Resources.ipynb` notebook.\n",
    "\n",
    "This will overwite any data stored for those variables and set them to the values specified in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store for cleanup\n",
    "%store dataset_group_arn\n",
    "%store role_name\n",
    "%store region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a9eb53",
   "metadata": {},
   "source": [
    "### Run the cleanup notebook\n",
    "\n",
    "Continue to [Clean_Up_Resources.ipynb](Clean_Up_Resources.ipynb) clean up resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 AWS User Personalization 算法和用户-项目-交互数据的批量推荐<a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "在此笔记本中，您将选择一个数据集，以准备将该数据集用于 Amazon Personalize 批量推荐。\n",
    "\n",
    "1. [选择数据集或数据源](#source)\n",
    "1. [准备数据](#prepare)\n",
    "1. [创建数据集组和交互数据集](#group_dataset)\n",
    "1. [配置 S3 存储桶和 IAM 角色](#bucket_role)\n",
    "1. [导入交互数据](#import)\n",
    "1. [创建解决方案](#solutions)\n",
    "1. [批量推荐](#batch)\n",
    "1. [清理](#cleanup)\n",
    "\n",
    "## 简介<a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "在大部分情况下，Amazon Personalize 中的算法（称为配方）在很大程度上寻求解决不同的任务，如下所述：\n",
    "\n",
    "1. **AWS User Personalization** – 根据以前用户与项目的交互推荐项目。\n",
    "1. **Personalized-Ranking** – 获取一个项目集合，然后使用类似 HRNN 的方法按照可能感兴趣的顺序对它们进行排序。\n",
    "1. **SIMS（类似项目）**– 根据给定的项目，推荐用户也可交互的其他项目。\n",
    "1. **Popularity-Count** – 如果 HRNN 或 HRNN-Metadata 没有答案，则推荐最受欢迎的项目 – 这是默认返回值。\n",
    "\n",
    "无论使用案例如何，所有算法都共享基于用户-项目-交互数据的学习基础，该数据由 3 个核心属性定义：\n",
    "\n",
    "1. **UserID** – 参与交互的用户\n",
    "1. **ItemID** – 用户与之交互的项目\n",
    "1. **Timestamp** – 交互发生的时间\n",
    "\n",
    "我们还支持按以下值定义的事件类型和事件值：\n",
    "\n",
    "1. **事件类型** – 事件的分类标签（浏览、已购买、已评价等）。\n",
    "1. **事件值** – 与发生的事件类型对应的值。一般来说，我们在事件类型中寻找 0 和 1 之间的归一化值。例如，如果完成交易有三个阶段（点击、添加到购物车和购买），则每个阶段的事件值分别为 0.33、0.66 和 1.0。\n",
    "\n",
    "事件类型和事件值字段是附加数据，可用于筛选为训练个性化模型而发送的数据。在这个特定的练习中，我们没有事件类型或事件值。\n",
    "\n",
    "## 选择数据集或数据源<a class=\"anchor\" id=\"source\"></a>\n",
    "[返回页首](#top)\n",
    "\n",
    "正如我们所提到的，用户-项目-交互数据是开始使用服务的关键。这意味着我们需要寻找生成这种数据的使用案例，几个常见示例包括：\n",
    "\n",
    "1. 视频点播应用\n",
    "1. 电子商务平台\n",
    "1. 社交媒体聚合器/平台\n",
    "\n",
    "使用一些指导原则可帮助确定适合 Personalize 的问题的范围。我们建议将下面的值作为起点，不过[官方限制](https://docs.aws.amazon.com/personalize/latest/dg/limits.html)略低一些。\n",
    "\n",
    "* 经过身份验证的用户\n",
    "* 至少 50 个独立用户\n",
    "* 至少 100 个独立项目\n",
    "* 每个用户至少 24 次交互 \n",
    "\n",
    "大多数情况下，这是很容易实现的，如果您在一个类别中排名较低，您通常可以通过在另一个类别中拥有更高排名来弥补。\n",
    "\n",
    "一般来说，您的数据不会是适合 Personalize 的完美形式，需要进行一些修改才能正确结构化。此笔记本会指导您完成所有这些任务。\n",
    "\n",
    "首先，我们将使用 [Last.FM](https://grouplens.org/datasets/hetrec-2011/) 数据集。这些数据是有关用户的音乐收听行为的记录。这些数据符合我们的指导原则，包括大量的用户、项目和交互。\n",
    "\n",
    "首先，您将下载数据集并使用下面的代码将数据集解压缩到一个新文件夹中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "!mkdir $data_dir\n",
    "!cd $data_dir && wget http://files.grouplens.org/datasets/hetrec2011/hetrec2011-lastfm-2k.zip\n",
    "!cd $data_dir && unzip hetrec2011-lastfm-2k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看您下载的数据文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前，除了我们似乎掌握的许多 .dat 文件和 README 外，我们对这些数据还知之甚少。打开 README 将告诉我们有关该数据的整体结构。除非数据源来自外部团队，否则对于自定义数据，您可以跳过这个步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 README 中，我们可以看到此数据集中有多个交互类型。将彼此标记为朋友的用户之间的交互、收听艺术家的用户的交互以及分配给用户和艺术家的标签的交互。\n",
    "\n",
    "在这种情况下，我们关注的是用户、艺术家和收听交互。我们有 1892 个用户，17632 个艺术家（在这种情况下是我们的项目），以及 92834 个用户-收听艺术家交互。这足以让我们开始使用 Personalize。\n",
    "\n",
    "继续阅读 README，进入 `Files` 部分。数据集中的大多数文件与我们无关，但 `users_artists.dat` 文件看起来很有希望。README 的 `Data format` 部分提供了有关该文件内容的更多详细信息。这是我们遇到第一个问题的地方。\n",
    "\n",
    "| userID | artistID | 权重  |\n",
    "|--------|----------|---------|\n",
    "| 2      | 51       | 13883   |\n",
    "\n",
    "尽管在用户和他们正在收听的艺术家之间存在交互数据，但是这些交互被存储为权重而不是时间戳。在 Amazon Personalize 中，我们需要的是用户-项目-时间戳交互数据。\n",
    "\n",
    "如果您再次查看数据集中的文件，您应该看到 `users_taggedartists-timestamps.dat` 确实包含时间戳数据。那么，如果我们使用标记行为作为我们的交互数据，而不是收听行为呢？ 我们可以假设用户标记艺术家是一种积极情绪的表现吗？ 通常，您会与您的客户或具有领域知识的人进行讨论，以了解此交互是否适合您想要解决的用例。现在，我们将假设标记行为适合我们的需要。\n",
    "\n",
    "`user_taggedartists-timestamps.dat` 的架构为：\n",
    "\n",
    "| userID | artistID | tagID | 时间戳     |\n",
    "|--------|----------|-------|---------------|\n",
    "| 2      | 52       | 13    | 1238536800000 |\n",
    "\n",
    "如果我们删除 `tagID` 属性，我们就会得到 Amazon Personalize 所需的格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据<a class=\"anchor\" id=\"prepare\"></a>\n",
    "[返回页首](#top)\n",
    "\n",
    "接下来要做的是加载数据并确认数据处于良好状态，然后将数据保存到 CSV 中，以便用于 Amazon Personalize。\n",
    "\n",
    "首先，导入数据科学中常用的 Python 库集合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import sleep\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，打开数据文件并查看前几行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(data_dir + '/user_taggedartists-timestamps.dat')\n",
    "original_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很明显，数据没有正确加载。CSV（逗号分隔值）文件的默认分隔符是逗号（`,`），但在这种情况下，文件是使用制表符（`\\t`）分隔符保存的。所以，让我们指定正确的分隔符并再次尝试加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(data_dir + '/user_taggedartists-timestamps.dat', delimiter='\\t')\n",
    "original_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样就好多了。现在数据已经成功加载到内存中，让我们提取一些额外的信息。首先，从数据中计算出一些基本的统计数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这表明我们有一个很好的 `userID` 和 `artistID` 值范围。接下来，确认数据格式始终是一个好主意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此可以看出，数据集中总共有 186479 个条目，有 4 列，每个单元格都存储为 int64 格式。\n",
    "\n",
    "Int64 格式显然适用于 `userID` 和 `artistID`。但是，我们需要更深入地了解数据中的时间戳。要使用 Amazon Personalize，您需要以 [Unix Epoch](https://en.wikipedia.org/wiki/Unix_time) 格式保存时间戳。\n",
    "\n",
    "目前，时间戳值不是人类可读的。因此，让我们获取一个任意的时间戳值，并找出如何解释它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arb_time_stamp = original_data.iloc[50]['timestamp']\n",
    "print(arb_time_stamp)\n",
    "print(datetime.utcfromtimestamp(arb_time_stamp).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "哎呀！对于这个特定的时间戳值，代码呈现的年份为 41,132。这对我们来说有点遥远，所以很明显这不是解析数据的正确方法。我们需要第二次尝试。\n",
    "\n",
    "JavaScript 以毫秒为单位记录时间，这是来自 Web 应用程序的数据集合，因此在应用代码之前，让我们将时间戳值除以 1000。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arb_time_stamp = arb_time_stamp/1000\n",
    "print(datetime.utcfromtimestamp(arb_time_stamp).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于我们的数据集来说，2009 年 2 月感觉更加真实。虽然我们不需要人类可读的时间戳也可使用 Amazon Personalize，但我们确实希望日期是真实的，所以现在我们将数据集中的每个时间戳从 JavaScript 毫秒格式转换过来，以继续下面的操作。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.timestamp = original_data.timestamp / 1000\n",
    "original_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过选择任意时间戳并将时间戳转换为人类可读的格式，对转换后的数据集进行快速的可用性检查。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arb_time_stamp = original_data.iloc[50]['timestamp']\n",
    "print(arb_time_stamp)\n",
    "print(datetime.utcfromtimestamp(arb_time_stamp).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个日期作为时间戳是有意义的，因此我们可以继续格式化其余的数据。请记住，我们需要的数据是用户-项目-交互数据，在本例中是 `userID`、`artistID` 和 `timestamp`。我们的数据集有一个额外的列，即 `tagID`，可将它从数据集中删除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = original_data.copy()\n",
    "interactions_df = interactions_df[['userID', 'artistID', 'timestamp']]\n",
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "操作数据后，请务必确认数据格式是否已更改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本例中，时间戳列已从 int64 更改为 float64。因此，让我们将格式改回 int64。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.astype({'timestamp': 'int64'}).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Amazon Personalize 为用户、项目和时间戳提供了默认列名。这些默认列名为 `USER_ID`、`ITEM_ID` 和 `TIMESTAMP`。因此，对数据集的最后修改是用默认标题替换现有的列标题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.rename(columns = {'userID':'USER_ID', 'artistID':'ITEM_ID', \n",
    "                              'timestamp':'TIMESTAMP'}, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是这样！此时，数据已准备就绪，我们只需将数据保存为 CSV 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_filename = \"interactions.csv\"\n",
    "interactions_df.to_csv((data_dir+\"/\"+interactions_filename), index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建数据集组和交互数据集<a class=\"anchor\" id=\"group_dataset\"></a>\n",
    "[返回页首](#top)\n",
    "\n",
    "Amazon Personalize 的最高隔离和抽象级别是*数据集组*。存储在其中一个数据集组中的信息不会影响任何其他数据集组或从其中一个数据集组创建的模型，这些数据集组是完全隔离的。这允许您运行许多实验，是我们如何保持您的模型的私密性并仅对您的数据进行全面训练的一部分。\n",
    "\n",
    "在导入先前准备好的数据之前，需要有一个数据集组和一个添加到其中的数据集来处理交互。\n",
    "\n",
    "数据集组可以包含以下类型的信息：\n",
    "\n",
    "* 用户-项目-交互\n",
    "* 事件流（实时交互）\n",
    "* 用户元数据\n",
    "* 项目元数据\n",
    "\n",
    "在为交互数据创建数据集组和数据集之前，让我们验证您的环境是否可以成功地与 Amazon Personalize 通信。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SDK to Personalize:\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建数据集组\n",
    "\n",
    "下面的单元格将创建一个名为 `personalize-poc-lastfm` 的新数据集组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_group_response = personalize.create_dataset_group(\n",
    "    name = \"personalize-batch-recommendations-dg\"\n",
    ")\n",
    "\n",
    "dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "print(json.dumps(create_dataset_group_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用数据集组之前，它必须处于活跃状态。这可能需要一两分钟。执行下面的单元格并等待单元格显示 ACTIVE 状态。它每秒检查一次数据集组的状态，工作最长可达 3 小时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，您有了一个数据集组，您可以为交互数据创建一个数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建数据集\n",
    "\n",
    "首先，定义一个架构，告诉 Amazon Personalize 您正在上载的数据集的类型。根据数据集的类型，架构中需要几个保留关键字和强制关键字。更详细的信息可以在[文档](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html)中找到。\n",
    "\n",
    "在这里，您将为需要 `USER_ID`、`ITEM_ID` 和 `TIMESTAMP` 字段的项目元数据创建架构。它们在架构中的定义顺序必须与它们在数据集中的出现顺序相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_schema = schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"personalize-batch-recommendations-interactions\",\n",
    "    schema = json.dumps(interactions_schema)\n",
    ")\n",
    "\n",
    "schema_arn = create_schema_response['schemaArn']\n",
    "print(json.dumps(create_schema_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建架构后，可以在数据集组中创建数据集。请注意，这还不会加载数据。加载数据将在几个步骤后发生。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"INTERACTIONS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    name = \"personalize-batch-recommendations-ds\",\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = schema_arn\n",
    ")\n",
    "\n",
    "interactions_dataset_arn = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置 S3 存储桶和 IAM 角色<a class=\"anchor\" id=\"bucket_role\"></a>\n",
    "[返回页首](#top)\n",
    "\n",
    "到目前为止，我们已下载、操作数据并将数据保存到运行此 Jupyter 笔记本的实例所连接的 Amazon EBS 实例中。但是，Amazon Personalize 需要一个 S3 存储桶充当您的数据源以及用于访问该存储桶的 IAM 角色。让我们把一切都安排好。\n",
    "\n",
    "使用存储在此 Amazon SageMaker 笔记本底层的实例上的元数据来确定运行区域。如果您使用的是 Amazon SageMaker 以外的 Jupyter 笔记本，只需将区域定义为下面的字符串。Amazon S3 存储桶需要与我们目前创建的 Amazon Personalize 资源位于同一区域。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/metadata/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon S3 存储桶名称是全局唯一的。要创建唯一的存储桶名称，下面的代码会将字符串 `personalizepoc` 附加到您的 AWS 账号。然后，它会使用此名称在前一个单元格中发现的区域创建一个存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket_name = account_id + \"-personalize-batch-recommendations\"\n",
    "print(bucket_name)\n",
    "if region != \"us-east-1\":\n",
    "    s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': region})\n",
    "else:\n",
    "    s3.create_bucket(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据上载到 S3\n",
    "\n",
    "现在，您的 Amazon S3 存储桶已经创建，请上载包含我们的用户-项目-交互数据的 CSV 文件。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_file_path = data_dir + \"/\" + interactions_filename\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(interactions_filename).upload_file(interactions_file_path)\n",
    "interactions_s3DataPath = \"s3://\"+bucket_name+\"/\"+interactions_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置 S3 存储桶策略\n",
    "Amazon Personalize 需要能够读取 S3 存储桶的内容。因此，添加可实现此操作的存储桶策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:*Object\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket_name),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket_name)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建 IAM 角色\n",
    "\n",
    "Amazon Personalize 需要能够承担 AWS 中的角色，以便拥有执行某些任务的权限。让我们来创建一个 IAM 角色并将所需的策略附加到该角色。下面的代码附加了非常宽松的策略；请对任何生产应用程序使用更严格的策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "role_name = \"PersonalizeRoleBatchRecommendations\"\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": \"personalize.amazonaws.com\"\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName = role_name,\n",
    "    AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    ")\n",
    "\n",
    "# AmazonPersonalizeFullAccess provides access to any S3 bucket with a name that includes \"personalize\" or \"Personalize\" \n",
    "# if you would like to use a bucket with a different name, please consider creating and attaching a new policy\n",
    "# that provides read access to your bucket or attaching the AmazonS3ReadOnlyAccess policy to the role\n",
    "policy_arn = \"arn:aws:iam::aws:policy/service-role/AmazonPersonalizeFullAccess\"\n",
    "iam.attach_role_policy(\n",
    "    RoleName = role_name,\n",
    "    PolicyArn = policy_arn\n",
    ")\n",
    "\n",
    "# Now add S3 support\n",
    "iam.attach_role_policy(\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonS3FullAccess',\n",
    "    RoleName=role_name\n",
    ")\n",
    "time.sleep(60) # wait for a minute to allow IAM role policy attachment to propagate\n",
    "\n",
    "role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入交互数据<a class=\"anchor\" id=\"import\"></a>\n",
    "[返回页首](#top)\n",
    "\n",
    "之前，您创建了数据集组和数据集来存放您的信息，因此现在您将执行一个导入作业，将数据从 S3 存储桶加载到 Amazon Personalize 数据集。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"personalize-batch-recommendations\",\n",
    "    datasetArn = interactions_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, interactions_filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用数据集之前，导入作业必须处于活跃状态。执行下面的单元格并等待单元格显示 ACTIVE 状态。它每秒检查一次导入作业的状态，工作最长可达 3 小时。\n",
    "\n",
    "导入数据可能需要一些时间，具体取决于数据集的大小。在此研讨会中，数据导入作业大约需要 15 分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"DatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当数据集导入处于活跃状态时，您可以开始使用 SIMS、Personalized-Ranking、Popularity-Count 和 AWS User Personalization 来构建模型。此过程将在其他笔记本中继续。运行下面的单元格，然后继续存储一些值，以便在下一个笔记本中使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建解决方案<a class=\"anchor\" id=\"solutions\"></a>\n",
    "[返回页首](#top)\n",
    "\n",
    "在本笔记本中，您将使用以下配方创建解决方案：\n",
    "\n",
    "1. AWS User Personalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Amazon Personalize 中，算法的特定变体称为配方。不同的配方适用于不同的情况。经过训练的模型称为解决方案，每个解决方案可以有许多版本，这些版本与模型训练时给定的数据量相关。\n",
    "\n",
    "首先，我们将列出支持的所有配方。这将允许您选择一个配方并使用它来构建模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.list_recipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出只是简介中提到的所有算法的 JSON 表示。\n",
    "\n",
    "接下来，我们将选择特定的配方并使用它们构建模型。\n",
    "\n",
    "### AWS User Personalization\n",
    "\n",
    "AWS User Personalization 是您可以使用的更高级的推荐模型之一，它允许根据用户行为实时更新推荐。它还往往优于其他方法，如协同筛选。这个配方的训练时间最长，所以让我们先从这个配方开始。\n",
    "\n",
    "对于我们使用 LastFM 数据的使用案例，我们可以使用 AWS User Personalization 算法，根据用户之前的艺术家标记行为向用户推荐新艺术家。请记住，我们使用标记数据来表示用户和艺术家之间的积极交互。\n",
    "\n",
    "首先，通过在上面的配方列表中找到 ARN 来选择配方。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_arn = \"arn:aws:personalize:::recipe/aws-user-personalization\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建解决方案\n",
    "\n",
    "首先，使用配方创建一个解决方案。尽管您在此步骤中提供了数据集 ARN，但模型尚未经过训练。将模型视为标识符，而不是经过训练的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_solution_response = personalize.create_solution(\n",
    "    name = \"personalize-batch-recommendations-user-personalization\",\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    recipeArn = recipe_arn\n",
    ")\n",
    "\n",
    "solution_arn = create_solution_response['solutionArn']\n",
    "print(json.dumps(create_solution_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建解决方案版本\n",
    "\n",
    "拥有解决方案之后，您需要创建一个版本来完成模型训练。训练过程可能需要一段时间才能完成（超过 25 分钟），并且使用我们的数据集进行训练时，采用此配方时平均需要 40 分钟。通常，我们会使用循环进行轮询，直到任务完成。但是，该任务会阻止其他单元格执行，这里的目标是创建许多模型并快速部署它们。因此，我们将为笔记本中的所有解决方案设置 While 循环。在那里，您还可以找到在 AWS 控制台中查看进度的说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_solution_version_response = personalize.create_solution_version(\n",
    "    solutionArn = solution_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_version_arn = create_solution_version_response['solutionVersionArn']\n",
    "print(json.dumps(create_solution_version_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看解决方案创建状态\n",
    "\n",
    "如前面所述，如何在控制台中查看状态更新呢：\n",
    "\n",
    "* 在另一个浏览器选项卡中，您应该已经在打开此笔记本实例时启动了 AWS 控制台。\n",
    "* 切换到该选项卡并在顶部搜索服务 `Personalize`，然后转到该服务页面。\n",
    "* 单击 `View dataset groups`。\n",
    "* 单击数据集组的名称，名称中很可能包含 POC。\n",
    "* 单击 `Solutions and recipes`。\n",
    "* 现在，您将看到您在上面创建的所有解决方案的列表，包括一个包含解决方案版本状态的列。在状态变成 `Active` 之后，您的解决方案就可以接受审核了。它也能够被部署。\n",
    "\n",
    "也可简单地运行下面的单元格以跟踪解决方案版本的创建状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_solution_versions = [\n",
    "    solution_version_arn,\n",
    "]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    for solution_version_arn in in_progress_solution_versions:\n",
    "        version_response = personalize.describe_solution_version(\n",
    "            solutionVersionArn = solution_version_arn\n",
    "        )\n",
    "        status = version_response[\"solutionVersion\"][\"status\"]\n",
    "        \n",
    "        if status == \"ACTIVE\":\n",
    "            print(\"Build succeeded for {}\".format(solution_version_arn))\n",
    "            in_progress_solution_versions.remove(solution_version_arn)\n",
    "        elif status == \"CREATE FAILED\":\n",
    "            print(\"Build failed for {}\".format(solution_version_arn))\n",
    "            in_progress_solution_versions.remove(solution_version_arn)\n",
    "    \n",
    "    if len(in_progress_solution_versions) <= 0:\n",
    "        break\n",
    "    else:\n",
    "        print(\"At least one solution build is still in progress\")\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与解决方案交互<a class=\"anchor\" id=\"interact\"></a>\n",
    "[返回页首](#top)\n",
    "\n",
    "现在，我们所有的解决方案都已部署并处于活跃状态，我们可以开始通过 API 调用来获取推荐。\n",
    "\n",
    "首先加载数据集，我们可以将数据集用于我们的查找表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for the items by reading in the correct source CSV\n",
    "items_df = pd.read_csv(data_dir + '/artists.dat', delimiter='\\t', index_col=0)\n",
    "\n",
    "# Render some sample data\n",
    "items_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过将 ID 列定义为索引列，只需查询 ID 即可返回艺术家。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_example = 987\n",
    "artist = items_df.loc[item_id_example]['name']\n",
    "print(artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这并不可怕，但在我们的代码中到处重复这一点会变得很混乱，所以下面的函数将对查询进行清理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artist_by_id(artist_id, artist_df=items_df):\n",
    "    \"\"\"\n",
    "    This takes in an artist_id from Personalize so it will be a string,\n",
    "    converts it to an int, and then does a lookup in a default or specified\n",
    "    dataframe.\n",
    "    \n",
    "    A really broad try/except clause was added in case anything goes wrong.\n",
    "    \n",
    "    Feel free to add more debugging or filtering here to improve results if\n",
    "    you hit an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return artist_df.loc[int(artist_id)]['name']\n",
    "    except:\n",
    "        return \"Error obtaining artist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们测试几个简单的值来检查我们的错误捕获。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A known good id\n",
    "print(get_artist_by_id(artist_id=\"987\"))\n",
    "# A bad type of value\n",
    "print(get_artist_by_id(artist_id=\"987.9393939\"))\n",
    "# Really bad values\n",
    "print(get_artist_by_id(artist_id=\"Steve\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "棒极了！现在我们有了一种呈现结果的方法。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量推荐<a class=\"anchor\" id=\"batch\"></a>\n",
    "[返回页首](#top)\n",
    "\n",
    "在许多情况下，您可能希望拥有更大的已导出推荐数据集。最近，Amazon Personalize 推出了批量推荐，作为将推荐集合导出到 S3 的一种方式。在此示例中，我们将介绍如何为 AWS User Personalization 解决方案执行此操作。有关批量推荐的更多信息，请参阅[文档](https://docs.aws.amazon.com/personalize/latest/dg/getting-recommendations.html#recommendations-batch)。此功能适用于所有配方，但输出格式会有所不同。\n",
    "\n",
    "一个简单的实现方法如下所示：\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "personalize_rec = boto3.client(service_name='personalize')\n",
    "\n",
    "personalize_rec.create_batch_inference_job (\n",
    "    solutionVersionArn = \"Solution version ARN\",\n",
    "    jobName = \"Batch job name\",\n",
    "    roleArn = \"IAM role ARN\",\n",
    "    jobInput = \n",
    "       {\"s3DataSource\": {\"path\": S3 input path}},\n",
    "    jobOutput = \n",
    "       {\"s3DataDestination\": {\"path\":S3 output path\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "SDK 导入、解决方案版本 ARN 和角色 ARN 均已确定。这只剩下输入、输出和待定义的作业名称。\n",
    "\n",
    "首先进行 User Personalization 配方的输入，它看起来类似于：\n",
    "\n",
    "\n",
    "```JSON\n",
    "{\"userId\": \"4638\"}\n",
    "{\"userId\": \"663\"}\n",
    "{\"userId\": \"3384\"}\n",
    "```\n",
    "\n",
    "这将产生如下所示的输出：\n",
    "\n",
    "```JSON\n",
    "{\"input\":{\"userId\":\"4638\"}, \"output\": {\"recommendedItems\": [\"296\", \"1\", \"260\", \"318\"]}}\n",
    "{\"input\":{\"userId\":\"663\"}, \"output\": {\"recommendedItems\": [\"1393\", \"3793\", \"2701\", \"3826\"]}}\n",
    "{\"input\":{\"userId\":\"3384\"}, \"output\": {\"recommendedItems\": [\"8368\", \"5989\", \"40815\", \"48780\"]}}\n",
    "```\n",
    "\n",
    "输出是一个 JSON 行文件。该文件由单独的 JSON 对象组成，每行一个对象。因此，我们稍后需要投入更多的工作来转化这种格式的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建输入文件\n",
    "\n",
    "使用批量功能时，可以指定要在作业完成时接收推荐的用户。下面的单元格将再次选择几个随机用户，然后构建文件并将文件保存到磁盘。从那里，您将把它上载到 S3，以便稍后在 API 调用中使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_csv(data_dir + '/user_artists.dat', delimiter='\\t', index_col=0)\n",
    "# Render some sample data\n",
    "users_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the user list\n",
    "batch_users = users_df.sample(3).index.tolist()\n",
    "\n",
    "# Write the file to disk\n",
    "json_input_filename = \"json_input.json\"\n",
    "with open(data_dir + \"/\" + json_input_filename, 'w') as json_input:\n",
    "    for user_id in batch_users:\n",
    "        json_input.write('{\"userId\": \"' + str(user_id) + '\"}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showcase the input file:\n",
    "!cat $data_dir\"/\"$json_input_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将文件上载到 S3，并将路径保存为变量以备后用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload files to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(json_input_filename).upload_file(data_dir+\"/\"+json_input_filename)\n",
    "s3_input_path = \"s3://\" + bucket_name + \"/\" + json_input_filename\n",
    "print(s3_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量推荐从我们上载到 S3 的文件中读取输入。类似地，批量推荐将输出保存到 S3 中的文件。因此，我们定义了应用于保存结果的输出路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output path\n",
    "s3_output_path = \"s3://\" + bucket_name + \"/\"\n",
    "print(s3_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，只需进行调用以启动批量导出过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchInferenceJobArn = personalize.create_batch_inference_job (\n",
    "    solutionVersionArn = solution_version_arn,\n",
    "    jobName = \"Batch-Inference-Job\",\n",
    "    roleArn = role_arn,\n",
    "    jobInput = \n",
    "     {\"s3DataSource\": {\"path\": s3_input_path}},\n",
    "    jobOutput = \n",
    "     {\"s3DataDestination\":{\"path\": s3_output_path}}\n",
    ")\n",
    "batchInferenceJobArn = batchInferenceJobArn['batchInferenceJobArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面的 While 循环以跟踪批量推荐调用的状态。这可能需要大约 25 分钟才能完成，因为 Personalize 需要建立基础架构来执行任务。我们正在使用仅有 3 个用户的数据集测试该功能，这不是对该机制的有效使用。通常，您只会将此功能用于批量处理，在这种情况下，效率将变得非常明显。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now()\n",
    "print(\"Import Started on: \", current_time.strftime(\"%I:%M:%S %p\"))\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_inference_job_response = personalize.describe_batch_inference_job(\n",
    "        batchInferenceJobArn = batchInferenceJobArn\n",
    "    )\n",
    "    status = describe_dataset_inference_job_response[\"batchInferenceJob\"]['status']\n",
    "    print(\"DatasetInferenceJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "    \n",
    "current_time = datetime.now()\n",
    "print(\"Import Completed on: \", current_time.strftime(\"%I:%M:%S %p\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦批量推荐作业完成处理，我们就可以获取上载到 S3 的输出并对输出进行解析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "export_name = json_input_filename + \".out\"\n",
    "s3.download_file(bucket_name, export_name, data_dir+\"/\"+export_name)\n",
    "\n",
    "# Update DF rendering\n",
    "pd.set_option('display.max_rows', 30)\n",
    "with open(data_dir+\"/\"+export_name) as json_file:\n",
    "    # Get the first line and parse it\n",
    "    line = json.loads(json_file.readline())\n",
    "    # Do the same for the other lines\n",
    "    while line:\n",
    "        # extract the user ID \n",
    "        col_header = \"User: \" + line['input']['userId']\n",
    "        # Create a list for all the artists\n",
    "        recommendation_list = []\n",
    "        # Add all the entries\n",
    "        for item in line['output']['recommendedItems']:\n",
    "            artist = get_artist_by_id(item)\n",
    "            recommendation_list.append(artist)\n",
    "        if 'bulk_recommendations_df' in locals():\n",
    "            new_rec_DF = pd.DataFrame(recommendation_list, columns = [col_header])\n",
    "            bulk_recommendations_df = bulk_recommendations_df.join(new_rec_DF)\n",
    "        else:\n",
    "            bulk_recommendations_df = pd.DataFrame(recommendation_list, columns=[col_header])\n",
    "        try:\n",
    "            line = json.loads(json_file.readline())\n",
    "        except:\n",
    "            line = None\n",
    "bulk_recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清理解决方案\n",
    "\n",
    "接下来，清理解决方案。下面的代码将列出您账户中的所有解决方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = personalize.get_paginator('list_solutions')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for solution in paginate_result[\"solutions\"]:\n",
    "        print(solution[\"solutionArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看 ARN 列表以确定要删除的解决方案。然后使用下面的代码通过插入 ARN 来删除解决方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.delete_solution(\n",
    "    solutionArn = \"INSERT ARN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清理数据集\n",
    "\n",
    "接下来，清理数据集。下面的代码将列出您账户中的所有数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = personalize.get_paginator('list_datasets')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for datasets in paginate_result[\"datasets\"]:\n",
    "        print(datasets[\"datasetArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看 ARN 列表以确定要删除的数据集。然后使用下面的代码通过插入 ARN 来删除解决方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.delete_dataset(\n",
    "    datasetArn = \"INSERT ARN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清理架构\n",
    "\n",
    "接下来，清理架构。下面的代码将列出您账户中的所有架构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = personalize.get_paginator('list_schemas')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for schema in paginate_result[\"schemas\"]:\n",
    "        print(schema[\"schemaArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看 ARN 列表以确定要删除的架构。然后使用下面的代码通过插入 ARN 来删除解决方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.delete_schema(\n",
    "    schemaArn = \"INSERT ARN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清理数据集组\n",
    "\n",
    "最后，清理数据集组。下面的代码将列出您账户中的所有数据集组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = personalize.get_paginator('list_dataset_groups')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for dataset_group in paginate_result[\"datasetGroups\"]:\n",
    "        print(dataset_group[\"datasetGroupArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看 ARN 列表以确定要删除的数据集组。然后使用下面的代码通过插入 ARN 来删除解决方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.delete_dataset_group(\n",
    "    datasetGroupArn = \"INSERT ARN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清除 S3 存储桶和 IAM 角色\n",
    "\n",
    "（可选）您可以删除我们在研讨会中创建的 IAM 角色和 S3 存储桶。\n",
    "\n",
    "首先使用下面的代码列出您账户中的所有 IAM 角色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client('iam')\n",
    "\n",
    "paginator = iam.get_paginator('list_roles')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for roles in paginate_result[\"Roles\"]:\n",
    "        print(roles[\"RoleName\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确定要删除的角色的名称。\n",
    "\n",
    "不能删除仍附加有策略的 IAM 角色。因此，在您确定了相关角色之后，让我们列出该角色的附加策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.list_attached_role_policies(\n",
    "    RoleName = \"INSERT ROLE NAME HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您需要使用下面的代码分离上面结果中的策略。对附加的每个策略重复此操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.detach_role_policy(\n",
    "    RoleName = \"INSERT ROLE NAME HERE\",\n",
    "    PolicyArn = \"INSERT ARN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，您应该能够删除 IAM 角色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.delete_role(\n",
    "    RoleName = \"INSERT ROLE NAME HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要删除 S3 存储桶，首先它需要是空的。删除 S3 存储桶最简单的方法是在 AWS 控制台中导航到 S3，删除存储桶中的对象，然后删除 S3 存储桶本身。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommandations par lots à l'aide de l'algorithme de personnalisation de l'utilisateur AWS et des données d'interaction article-utilisateur <a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "Dans ce bloc-notes, vous allez choisir un jeu de données et le préparer à être utilisé avec les recommandations par lots d'Amazon Personalize.\n",
    "\n",
    "1. [Choisir un jeu de données ou une source de données](#source)\n",
    "1. [Préparer vos données](#prepare)\n",
    "1. [Créer des groupes de jeux de données et le jeu de données d'interactions](#group_dataset)\n",
    "1. [Configurer un compartiment S3 et un rôle IAM](#bucket_role)\n",
    "1. [Importer les données d'interactions](#import)\n",
    "1. [Créer des solutions](#solutions)\n",
    "1. [Recommandations par lots](#batch)\n",
    "1. [Nettoyer](#cleanup)\n",
    "\n",
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "Pour la plupart, les algorithmes d'Amazon Personalize (appelés recettes) cherchent à résoudre différentes tâches, expliquées ici :\n",
    "\n",
    "1. **Personnalisation de l'utilisateur AWS** – Recommande des articles en fonction des interactions précédentes de l'utilisateur avec les articles.\n",
    "1. **Classement personnalisé** – Prend une collection d'articles et les ordonne dans l'ordre probable d'intérêt en utilisant une approche de type HRNN.\n",
    "1. **SIMS (Éléments similaires)** – À partir d'un article donné, recommande d'autres articles avec lesquels les utilisateurs ont également interagi.\n",
    "1. **Compte de popularité** – Recommande les articles les plus populaires, si HRNN ou métadonnées HRNN n'ont pas de réponse – cela est renvoyé par défaut.\n",
    "\n",
    "Quel que soit le cas d'utilisation, les algorithmes partagent tous une base d'apprentissage sur les données d'interaction entre l'utilisateur et l'article, qui sont définies par trois attributs fondamentaux :\n",
    "\n",
    "1. **UserID** – L'utilisateur qui a interagi\n",
    "1. **ItemID** – Article avec lequel l'utilisateur a interagi\n",
    "1. **Horodatage** – L'heure à laquelle l'interaction s'est produite\n",
    "\n",
    "Nous prenons également en charge les types d'événements et les valeurs d'événements définis par :\n",
    "\n",
    "1. **Type d'événement​​** – Étiquette catégorique d'un événement (parcourir, acheté, évalué, etc.).\n",
    "1. **Valeur de l'événement** – Une valeur correspondant au type d'événement qui s'est produit. En règle générale, nous recherchons des valeurs normalisées entre 0 et 1 relatives aux types d'événements. Par exemple, s'il y a trois phases pour terminer une transaction (cliqué, ajouté au panier et acheté), alors il y aura une valeur event_value pour chaque phase comme 0,33, 0,66 et 1,0 respectivement.\n",
    "\n",
    "Les champs type d'événement et valeur d'événement sont des données supplémentaires qui peuvent être utilisées pour filtrer les données envoyées en vue de l'entraînement du modèle de personnalisation. Dans cet exercice particulier, nous n'aurons pas de type d'événement ou de valeur d'événement. \n",
    "\n",
    "## Choisir un jeu de données ou une source de données <a class=\"anchor\" id=\"source\"></a>\n",
    "[Retour au début](#top)\n",
    "\n",
    "Comme nous l'avons mentionné, les données d'interaction utilisateur-article sont essentielles pour commencer à utiliser le service. Cela signifie que nous devons rechercher les cas d'utilisation qui génèrent ce type de données, dont voici quelques exemples courants :\n",
    "\n",
    "1. Applications de vidéo à la demande\n",
    "1. Plateformes d'e-commerce\n",
    "1. Agrégateurs/plateformes de médias sociaux\n",
    "\n",
    "Il existe quelques lignes directrices pour définir la portée d'un problème adapté à Personalize. Nous recommandons les valeurs ci-dessous comme point de départ, bien que les [limites officielles](https://docs.aws.amazon.com/personalize/latest/dg/limits.html) soient un peu plus basses.\n",
    "\n",
    "* Utilisateurs authentifiés\n",
    "* Au moins 50 utilisateurs uniques\n",
    "* Au moins 100 articles uniques\n",
    "* Au moins deux douzaines d'interactions pour chaque utilisateur \n",
    "\n",
    "La plupart du temps, il est facile d'y parvenir, et si vous êtes dans une catégorie faible, vous pouvez souvent compenser en ayant un chiffre plus élevé dans une autre catégorie.\n",
    "\n",
    "En règle générale, vos données n'arriveront pas sous une forme parfaite pour Personalize, et il faudra les modifier afin qu'elles soient structurées correctement. Ce bloc-notes a pour objectif de vous guider dans tout ceci. \n",
    "\n",
    "Pour commencer, nous allons utiliser le jeu de données [Last.FM](https://grouplens.org/datasets/hetrec-2011/). Il s'agit d'enregistrements du comportement d'écoute musicale de ses utilisateurs. Les données correspondent à nos directives avec un grand nombre d'utilisateurs, d'articles et d'interactions.\n",
    "\n",
    "Tout d'abord, vous allez télécharger le jeu de données et le décompresser dans un nouveau dossier en utilisant le code ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "!mkdir $data_dir\n",
    "!cd $data_dir && wget http://files.grouplens.org/datasets/hetrec2011/hetrec2011-lastfm-2k.zip\n",
    "!cd $data_dir && unzip hetrec2011-lastfm-2k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinez les fichiers de données que vous avez téléchargés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'instant, nous avons très peu d'informations sur les données, si ce n'est qu'il semble exister un grand nombre de fichiers .dat et un fichier README. L'ouverture du fichier README nous renseignera sur la structure globale de ces données. C'est une étape que vous pouvez probablement ignorer avec des données personnalisées, sauf si la source de données provient d'une équipe externe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après le fichier README, nous constatons qu'il existe plusieurs types d'interactions dans ce jeu de données. Les interactions entre les utilisateurs qui se marquent mutuellement comme amis, les interactions des utilisateurs qui écoutent des artistes et les interactions des identifications attribuées aux utilisateurs et aux artistes.\n",
    "\n",
    "Dans ce cas, nous nous concentrons sur les utilisateurs, les artistes et les interactions d'écoute. Nous avons 1 892 utilisateurs, 17 632 artistes (nos articles dans ce cas) et 92 834 interactions entre les utilisateurs et les artistes écoutés. Cela suffit amplement pour commencer à utiliser Personalize.\n",
    "\n",
    "Continuez à lire le fichier README jusqu'à la section `Files`. La plupart des fichiers du jeu de données ne sont pas pertinents pour nous, mais ce fichier `users_artists.dat` semble prometteur. La section `Data format` du fichier README fournit plus de détails sur le contenu du fichier. C'est là que nous rencontrons notre premier problème.\n",
    "\n",
    "| userID | artistID | poids  |\n",
    "|--------|----------|---------|\n",
    "| 2      | 51       | 13883   |\n",
    "\n",
    "Bien qu'il existe des données d'interaction entre les utilisateurs et les artistes qu'ils écoutent, ces interactions sont stockées sous forme de poids et non d'horodatage. Nous avons besoin des données d'interaction utilisateur-article-horodatage pour Amazon Personalize. \n",
    "\n",
    "Si vous examinez à nouveau les fichiers dans le jeu de données, vous devez constater que `users_taggedartists-timestamps.dat` contient des données d'horodatage. Et si nous utilisions le comportement d'identification comme données d'interaction, au lieu du comportement d'écoute ? Pouvons-nous supposer qu'un utilisateur qui identifie un artiste est une indication d'un sentiment positif ? Normalement, vous discuterez avec votre client, ou avec quelqu'un qui a des connaissances dans le domaine, pour comprendre si cette interaction est adaptée au cas d'utilisation que vous voulez résoudre. Pour l'instant, nous supposerons que le comportement d'identification est adapté à nos besoins. \n",
    "\n",
    "Le schéma pour `user_taggedartists-timestamps.dat` est :\n",
    "\n",
    "| userID | artistID | tagID | timestamp     |\n",
    "|--------|----------|-------|---------------|\n",
    "| 2      | 52       | 13    | 1238536800000 |\n",
    "\n",
    "Si nous supprimons l'attribut `tagID`, nous avons exactement le format dont nous avons besoin pour Amazon Personalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparer vos données <a class=\"anchor\" id=\"prepare\"></a>\n",
    "[Retour au début](#top)\n",
    "\n",
    "Chargez ensuite les données et assurez-vous qu'elles sont en bon état, puis enregistrez-les dans un CSV où elles sont prêtes à être utilisées avec Amazon Personalize.\n",
    "\n",
    "Pour commencer, importez une collection de bibliothèques Python couramment utilisées en science des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import sleep\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, ouvrez le fichier de données et examinez les premières lignes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(data_dir + '/user_taggedartists-timestamps.dat')\n",
    "original_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À l'évidence, les données ne se sont pas chargées correctement. Le délimiteur par défaut des fichiers CSV (comma-separated value) est la virgule (`,`), mais dans ce cas, le fichier a été enregistré avec des caractères de tabulation (`\\t`). Spécifions donc le délimiteur correct et essayons à nouveau de charger les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(data_dir + '/user_taggedartists-timestamps.dat', delimiter='\\t')\n",
    "original_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est mieux. Maintenant que les données ont été chargées en mémoire, extrayons quelques informations supplémentaires. Tout d'abord, calculez quelques statistiques de base à partir des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela montre que nous avons une bonne gamme de valeurs pour `userID` et `artistID`. Ensuite, il est toujours bon de vérifier le format des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À partir de là, vous pouvez voir qu'il y a un total de 186 479 entrées dans le jeu de données, avec 4 colonnes, et que chaque cellule est stockée au format int64.\n",
    "\n",
    "Le format int64 est clairement adapté à `userID` et `artistID`. Cependant, nous effectuer une analyse plus approfondie pour comprendre les horodatages dans les données. Pour utiliser Amazon Personalize, vous devez enregistrer les horodatages au format [Unix Epoch](https://en.wikipedia.org/wiki/Unix_time).\n",
    "\n",
    "Actuellement, les valeurs d'horodatage ne sont pas lisibles par l'homme. Prenons donc une valeur d'horodatage arbitraire et voyons comment l'interpréter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arb_time_stamp = original_data.iloc[50]['timestamp']\n",
    "print(arb_time_stamp)\n",
    "print(datetime.utcfromtimestamp(arb_time_stamp).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oups ! Pour cette valeur d'horodatage particulière, le code a rendu l'année 41 132. C'est un peu loin dans le futur pour nous, et, clairement, ce n'est la bonne façon d'analyser les données. Nous avons besoin d'une deuxième tentative.\n",
    "\n",
    "JavaScript enregistre le temps en millisecondes, et il s'agit d'une collection de données provenant d'une application web. Divisons donc la valeur de l'horodatage par 1 000 avant d'appliquer notre code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arb_time_stamp = arb_time_stamp/1000\n",
    "print(datetime.utcfromtimestamp(arb_time_stamp).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le mois de février 2009 semble beaucoup plus réaliste pour notre jeu de données. Nous n'avons pas besoin d'horodatages lisible par l'homme pour utiliser Amazon Personalize, mais nous tenons à ce que les dates soient réalistes. Nous allons donc transformer chaque horodatage du jeu de données pour l'éloigner du format JavaScript en millisecondes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.timestamp = original_data.timestamp / 1000\n",
    "original_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effectuez un test d'intégrité du jeu de données transformé en choisissant un horodatage arbitraire et en le transformant en un format lisible par l'homme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arb_time_stamp = original_data.iloc[50]['timestamp']\n",
    "print(arb_time_stamp)\n",
    "print(datetime.utcfromtimestamp(arb_time_stamp).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette date a du sens en tant qu'horodatage. Nous pouvons donc continuer à formater le reste des données. Rappelez-vous que les données dont nous avons besoin sont des données d'interaction utilisateur-article, qui sont `userID`, `artistID` et `timestamp` dans ce cas. Notre jeu de données comporte une colonne supplémentaire, `tagID`, qui peut être supprimée du jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = original_data.copy()\n",
    "interactions_df = interactions_df[['userID', 'artistID', 'timestamp']]\n",
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir manipulé les données, vérifiez toujours si le format des données a changé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cas, le format int64 de la colonne horodatage a été remplacé par float64. Par conséquent, rétablissons le format int64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.astype({'timestamp': 'int64'}).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Amazon Personalize utilise des noms de colonne par défaut pour les utilisateurs, les articles et l'horodatage. Ces noms de colonnes par défaut sont `USER_ID`, `ITEM_ID` ET `TIMESTAMP`. La dernière modification apportée au jeu de données consiste donc à remplacer les en-têtes de colonnes existants par les en-têtes par défaut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.rename(columns = {'userID':'USER_ID', 'artistID':'ITEM_ID', \n",
    "                              'timestamp':'TIMESTAMP'}, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà ! À ce stade, les données sont prêtes à être utilisées et il ne nous reste plus qu'à les enregistrer dans un fichier CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_filename = \"interactions.csv\"\n",
    "interactions_df.to_csv((data_dir+\"/\"+interactions_filename), index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer des groupes de jeux de données et le jeu de données d'interactions <a class=\"anchor\" id=\"group_dataset\"></a>\n",
    "[Retour au début](#top)\n",
    "\n",
    "Le plus haut niveau d'isolation et d'abstraction avec Amazon Personalize est le *groupe de jeux de données*. Toute information stockée dans l'un de ces groupes de jeux de données n'a aucun impact sur un autre groupe de données ou sur les modèles créés à partir de celui-ci – ils sont entièrement indépendants. Vous pouvez ainsi réaliser de nombreuses expériences, et c'est en partie grâce à cela que vos modèles restent privés et ne sont entraînés que sur vos données. \n",
    "\n",
    "Avant d'importer les données préparées antérieurement, il faut créer un groupe de jeux de données et y ajouter un jeu de données qui gère les interactions.\n",
    "\n",
    "Les groupes de jeux de données peuvent comporter les types d'informations suivants :\n",
    "\n",
    "* Interactions utilisateur-article\n",
    "* Flux d'événements (interactions en temps réel)\n",
    "* Métadonnées de l'utilisateur\n",
    "* Métadonnées de l'article\n",
    "\n",
    "Avant de créer le groupe de jeux de données et le jeu de données pour nos données d'interaction, vérifions que votre environnement peut communiquer avec Amazon Personalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SDK to Personalize:\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer le groupe de jeux de données\n",
    "\n",
    "La cellule suivante va créer un nouveau groupe de jeux de données avec le nom `personalize-poc-lastfm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_group_response = personalize.create_dataset_group(\n",
    "    name = \"personalize-batch-recommendations-dg\"\n",
    ")\n",
    "\n",
    "dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "print(json.dumps(create_dataset_group_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir utiliser le groupe de jeux de données, il doit être actif. Cela peut prendre une ou deux minutes. Exécutez la cellule ci-dessous et attendez qu'elle affiche le statut ACTIF. Il vérifie l'état du groupe de jeux de données toutes les secondes, jusqu'à un maximum de 3 heures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que vous avez un groupe de jeux de données, vous pouvez créer un jeu de données pour les données d'interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer le jeu de données\n",
    "\n",
    "Tout d'abord, définissez un schéma afin d'indiquer à Amazon Personalize le type de jeu de données que vous chargez. Plusieurs mots-clés réservés et obligatoires sont requis dans le schéma, en fonction du type de jeu de données. Des informations plus détaillées peuvent être retrouvées dans la [documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "Ici, vous allez créer un schéma pour les données d'interactions, qui nécessite les champs `USER_ID`, `ITEM_ID` et `TIMESTAMP`  Ils doivent être définis dans le schéma en respectant l'ordre dans lequel ils apparaissent dans le jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_schema = schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"personalize-batch-recommendations-interactions\",\n",
    "    schema = json.dumps(interactions_schema)\n",
    ")\n",
    "\n",
    "schema_arn = create_schema_response['schemaArn']\n",
    "print(json.dumps(create_schema_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois le schéma créé, vous pouvez créer un jeu de données dans le groupe de jeux de données. Il est à noter que cette opération ne charge pas encore les données. Cela arrivera quelques étapes plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"INTERACTIONS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    name = \"personalize-batch-recommendations-ds\",\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = schema_arn\n",
    ")\n",
    "\n",
    "interactions_dataset_arn = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurer un compartiment S3 et un rôle IAM <a class=\"anchor\" id=\"bucket_role\"></a>\n",
    "[Retour au début](#top)\n",
    "\n",
    "Pour l'instant, nous avons téléchargé, manipulé et enregistré les données sur l'instance Amazon EBS associée à l'instance qui exécute ce bloc-notes Jupyter. Toutefois, Amazon Personalize aura besoin d'un compartiment S3 pour servir de source à vos données, ainsi que de rôles IAM pour accéder à ce compartiment. Mettons tout cela en place.\n",
    "\n",
    "Utilisez les métadonnées stockées sur l'instance sous-jacente de ce bloc-notes Amazon SageMaker pour déterminer la région dans laquelle il fonctionne. Si vous utilisez un bloc-notes Jupyter en dehors d'Amazon SageMaker, définissez simplement la région sous forme de chaîne comme suit. Le compartiment Amazon S3 doit se trouver dans la même région que les ressources Amazon Personalize que nous avons créées jusqu'à présent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/metadata/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les noms des compartiments Amazon S3 sont uniques au niveau mondial. Pour créer un nom de compartiment unique, le code ci-dessous ajoutera la chaîne de caractères `personalizepoc` à votre numéro de compte AWS. Puis il crée un compartiment avec ce nom dans la région découverte dans la cellule précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket_name = account_id + \"-personalize-batch-recommendations\"\n",
    "print(bucket_name)\n",
    "if region != \"us-east-1\":\n",
    "    s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': region})\n",
    "else:\n",
    "    s3.create_bucket(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger les données vers S3\n",
    "\n",
    "Maintenant que votre compartiment Amazon S3 a été créé, chargez le fichier CSV de nos données d'interaction utilisateur-article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_file_path = data_dir + \"/\" + interactions_filename\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(interactions_filename).upload_file(interactions_file_path)\n",
    "interactions_s3DataPath = \"s3://\"+bucket_name+\"/\"+interactions_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définir la politique du compartiment S3\n",
    "Amazon Personalize doit pouvoir lire le contenu de votre compartiment S3. Il faut donc ajouter une politique de compartiment qui l'autorise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:*Object\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket_name),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket_name)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un rôle IAM\n",
    "\n",
    "Amazon Personalize doit pouvoir assumer des rôles dans AWS, afin de disposer des autorisations nécessaires pour exécuter certaines tâches. Créons un rôle IAM et ajoutons-lui les politiques requises. Le code ci-dessous associe des politiques très permissives ; veuillez utiliser des politiques plus restrictives pour toute application de production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "role_name = \"PersonalizeRoleBatchRecommendations\"\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": \"personalize.amazonaws.com\"\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName = role_name,\n",
    "    AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    ")\n",
    "\n",
    "# AmazonPersonalizeFullAccess provides access to any S3 bucket with a name that includes \"personalize\" or \"Personalize\" \n",
    "# if you would like to use a bucket with a different name, please consider creating and attaching a new policy\n",
    "# that provides read access to your bucket or attaching the AmazonS3ReadOnlyAccess policy to the role\n",
    "policy_arn = \"arn:aws:iam::aws:policy/service-role/AmazonPersonalizeFullAccess\"\n",
    "iam.attach_role_policy(\n",
    "    RoleName = role_name,\n",
    "    PolicyArn = policy_arn\n",
    ")\n",
    "\n",
    "# Now add S3 support\n",
    "iam.attach_role_policy(\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonS3FullAccess',\n",
    "    RoleName=role_name\n",
    ")\n",
    "time.sleep(60) # wait for a minute to allow IAM role policy attachment to propagate\n",
    "\n",
    "role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer les données d'interactions <a class=\"anchor\" id=\"import\"></a>\n",
    "[Retour au début](#top)\n",
    "\n",
    "Précédemment, vous avez créé le groupe de jeux de données et le jeu de données pour héberger vos informations. Vous allez maintenant exécuter une tâche d'importation qui chargera les données du compartiment S3 dans le jeu de données Amazon Personalize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"personalize-batch-recommendations\",\n",
    "    datasetArn = interactions_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, interactions_filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir utiliser le jeu de données, la tâche d'importation doit être active. Exécutez la cellule ci-dessous et attendez qu'elle affiche le statut ACTIF. Il vérifie l'état de la tâche d'importation toutes les secondes, jusqu'à un maximum de 3 heures.\n",
    "\n",
    "L'importation des données peut prendre un certain temps, en fonction de la taille du jeu de données. Dans cet atelier, l'importation des données devrait durer environ 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"DatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque l'importation du jeu de données est active, vous êtes prêt à commencer à créer des modèles avec SIMS, Classement personnalisé, Compte de popularité et Personnalisation de l'utilisateur AWS. Ce processus se poursuivra dans d'autres blocs-notes. Exécutez la cellule ci-dessous avant de passer au stockage de quelques valeurs à utiliser dans les prochains blocs-notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer des solutions <a class=\"anchor\" id=\"solutions\"></a>\n",
    "[Retour au début](#top)\n",
    "\n",
    "Dans ce bloc-notes, vous allez créer des solutions avec la recette suivante :\n",
    "\n",
    "1. Personnalisation de l'utilisateur AWS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans Amazon Personalize, une variation spécifique d'un algorithme est appelée une recette. Des recettes différentes conviennent à des situations différentes. Un modèle entraîné s'appelle une solution, et chaque solution peut avoir de nombreuses versions qui se rapportent à un volume de données spécifique au moment où le modèle a été entraîné.\n",
    "\n",
    "Tout d'abord, nous allons énumérer toutes les recettes qui sont prises en charge. Cela vous permettra d'en sélectionner une et de l'utiliser pour créer votre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.list_recipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat est juste une représentation JSON de tous les algorithmes mentionnés dans l'introduction.\n",
    "\n",
    "Ensuite, nous sélectionnerons des recettes spécifiques et créerons des modèles avec elles.\n",
    "\n",
    "### Personnalisation de l'utilisateur AWS\n",
    "\n",
    "La personnalisation de l'utilisateur AWS est l'un des modèles de recommandation les plus avancés disponibles. Elle permet de mettre à jour les recommandations en temps réel en fonction du comportement des utilisateurs. Elle tend également à surpasser d'autres approches, comme le filtrage collaboratif. Cette recette est la plus longue à entraîner. Commençons par elle.\n",
    "\n",
    "Dans notre cas d'utilisation, en utilisant les données LastFM, nous pouvons utiliser l'algorithme de personnalisation de l'utilisateur AWS pour recommander de nouveaux artistes à un utilisateur en fonction de son comportement antérieur d'identification des artistes. Rappelez-vous que nous avons utilisé les données de marquage pour représenter les interactions positives entre un utilisateur et un artiste.\n",
    "\n",
    "Tout d'abord, sélectionnez la recette en trouvant l'ARN dans la liste des recettes ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_arn = \"arn:aws:personalize:::recipe/aws-user-personalization\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Créer la solution\n",
    "\n",
    "Tout d'abord, vous créez une solution à l'aide de la recette. Même si vous fournissez le jeu de données ARN dans cette étape, le modèle n'est pas encore entraîné. Considérez cela comme un identifiant plutôt qu'un modèle entraîné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_solution_response = personalize.create_solution(\n",
    "    name = \"personalize-batch-recommendations-user-personalization\",\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    recipeArn = recipe_arn\n",
    ")\n",
    "\n",
    "solution_arn = create_solution_response['solutionArn']\n",
    "print(json.dumps(create_solution_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Créer la version de solution\n",
    "\n",
    "Une fois que vous avez une solution, vous devez créer une version afin d'achever l'entraînement du modèle. L'entraînement peut prendre un certain temps, plus de 25 minutes, et une moyenne de 40 minutes pour cette recette avec notre jeu de données. Normalement, nous utiliserions une boucle while pour interroger le système jusqu'à ce que la tâche soit terminée. Toutefois, cette tâche bloquerait l'exécution d'autres cellules, et l'objectif ici est de créer de nombreux modèles et de les déployer rapidement. Nous allons donc mettre en place la boucle while pour toutes les solutions plus bas dans ce bloc-notes. Vous y trouverez également des instructions pour visualiser la progression dans la console AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_solution_version_response = personalize.create_solution_version(\n",
    "    solutionArn = solution_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_version_arn = create_solution_version_response['solutionVersionArn']\n",
    "print(json.dumps(create_solution_version_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichez le statut de création de la solution\n",
    "\n",
    "Voici, comme promis, comment afficher les mises à jour du statut dans la console :\n",
    "\n",
    "* La console AWS devrait déjà être ouverte dans un autre onglet du navigateur, après l'ouverture de cette instance du bloc-notes. \n",
    "* Passez à cet onglet et recherchez en haut le service `Personalize`, puis référez-vous à cette page de service. \n",
    "* Cliquez sur `View dataset groups`.\n",
    "* Cliquez sur le nom de votre groupe de jeux de données, très probablement un nom contenant POC.\n",
    "* Cliquez sur `Solutions and recipes`.\n",
    "* Vous voyez maintenant la liste de toutes les solutions que vous avez créées ci-dessus, y compris une colonne avec le statut des versions de la solution. Une fois qu'elle est `Active`, votre solution est prête à être examinée. Elle est également en mesure d'être déployée.\n",
    "\n",
    "Ou exécutez simplement la cellule ci-dessous pour surveiller le statut de création des versions de la solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_solution_versions = [\n",
    "    solution_version_arn,\n",
    "]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    for solution_version_arn in in_progress_solution_versions:\n",
    "        version_response = personalize.describe_solution_version(\n",
    "            solutionVersionArn = solution_version_arn\n",
    "        )\n",
    "        status = version_response[\"solutionVersion\"][\"status\"]\n",
    "        \n",
    "        if status == \"ACTIVE\":\n",
    "            print(\"Build succeeded for {}\".format(solution_version_arn))\n",
    "            in_progress_solution_versions.remove(solution_version_arn)\n",
    "        elif status == \"CREATE FAILED\":\n",
    "            print(\"Build failed for {}\".format(solution_version_arn))\n",
    "            in_progress_solution_versions.remove(solution_version_arn)\n",
    "    \n",
    "    if len(in_progress_solution_versions) <= 0:\n",
    "        break\n",
    "    else:\n",
    "        print(\"At least one solution build is still in progress\")\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interagir avec les solutions <a class=\"anchor\" id=\"interact\"></a>\n",
    "[Retour au début](#top)\n",
    "\n",
    "Maintenant que toutes nos solutions sont déployées et actives, nous pouvons commencer à obtenir des recommandations au moyen d'un appel d'API. \n",
    "\n",
    "Commencez par charger le jeu de données que nous pouvons utiliser pour notre table de consultation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for the items by reading in the correct source CSV\n",
    "items_df = pd.read_csv(data_dir + '/artists.dat', delimiter='\\t', index_col=0)\n",
    "\n",
    "# Render some sample data\n",
    "items_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En définissant la colonne ID comme colonne d'index, il est aisé de trouver un artiste en recherchant simplement l'ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_example = 987\n",
    "artist = items_df.loc[item_id_example]['name']\n",
    "print(artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode est acceptable. Cependant, il serait fastidieux de la répéter partout dans notre code. C'est pourquoi la fonction ci-dessous va la supprimer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artist_by_id(artist_id, artist_df=items_df):\n",
    "    \"\"\"\n",
    "    This takes in an artist_id from Personalize so it will be a string,\n",
    "    converts it to an int, and then does a lookup in a default or specified\n",
    "    dataframe.\n",
    "    \n",
    "    A really broad try/except clause was added in case anything goes wrong.\n",
    "    \n",
    "    Feel free to add more debugging or filtering here to improve results if\n",
    "    you hit an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return artist_df.loc[int(artist_id)]['name']\n",
    "    except:\n",
    "        return \"Error obtaining artist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testons à présent quelques valeurs simples pour vérifier notre système de détection d'erreurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A known good id\n",
    "print(get_artist_by_id(artist_id=\"987\"))\n",
    "# A bad type of value\n",
    "print(get_artist_by_id(artist_id=\"987.9393939\"))\n",
    "# Really bad values\n",
    "print(get_artist_by_id(artist_id=\"Steve\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super ! Nous disposons désormais d'un moyen de présentation des résultats. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommandations par lots<a class=\"anchor\" id=\"batch\"></a>\n",
    "[Retour au début](#top)\n",
    "\n",
    "Dans de nombreux cas, vous pouvez souhaiter disposer d'un plus grand jeu de données de recommandations exportées. Récemment, Amazon Personalize a introduit les recommandations par lots comme moyen d'exporter une collection de recommandations vers S3. Dans cet exemple, nous allons voir comment procéder pour la solution de personnalisation de l'utilisateur AWS. Pour en savoir plus sur les recommandations par lots, veuillez vous reporter à la [documentation](https://docs.aws.amazon.com/personalize/latest/dg/getting-recommendations.html#recommendations-batch). Cette fonction s'applique à toutes les recettes, mais le format de sortie varie.\n",
    "\n",
    "Une implémentation simple se présente comme suit :\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "personalize_rec = boto3.client(service_name='personalize')\n",
    "\n",
    "personalize_rec.create_batch_inference_job (\n",
    "    solutionVersionArn = \"ARN de la version de la solution,\"\n",
    "    jobName = \"Nom de la tâche du lot\",\n",
    "    roleArn = \"ARN du rôle IAM\",\n",
    "    jobInput = \n",
    "       {\"s3DataSource\": {\"path\": S3 input path}},\n",
    "    jobOutput = \n",
    "       {\"s3DataDestination\": {\"path\":S3 output path\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "L'importation du kit SDK, l'ARN de la version de la solution et les ARN de rôle ont tous été définis. Il ne reste plus qu'à définir une entrée, une sortie et un nom de tâche.\n",
    "\n",
    "En commençant par l'entrée pour la personnalisation de l'utilisateur, cela se présente comme suit :\n",
    "\n",
    "\n",
    "```JSON\n",
    "{\"userId\": \"4638\"}\n",
    "{\"userId\": \"663\"}\n",
    "{\"userId\": \"3384\"}\n",
    "```\n",
    "\n",
    "Le résultat devrait ressembler à ce qui suit :\n",
    "\n",
    "```JSON\n",
    "{\"input\":{\"userId\":\"4638\"}, \"output\": {\"recommendedItems\": [\"296\", \"1\", \"260\", \"318\"]}}\n",
    "{\"input\":{\"userId\":\"663\"}, \"output\": {\"recommendedItems\": [\"1393\", \"3793\", \"2701\", \"3826\"]}}\n",
    "{\"input\":{\"userId\":\"3384\"}, \"output\": {\"recommendedItems\": [\"8368\", \"5989\", \"40815\", \"48780\"]}}\n",
    "```\n",
    "\n",
    "La sortie est un fichier JSON Lines. Il est constitué d'objets JSON individuels, un par ligne. Nous devrons donc fournir un effort supplémentaire par la suite pour digérer les résultats dans ce format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du fichier d'entrée\n",
    "\n",
    "Lorsque vous utilisez la fonction de traitement par lots, vous indiquez les utilisateurs pour lesquels vous souhaitez recevoir des recommandations lorsque la tâche est terminée. La cellule ci-dessous va à nouveau sélectionner quelques utilisateurs au hasard, puis créer le fichier et le sauvegarder sur le disque. À partir de là, vous le chargerez sur S3 pour l'utiliser plus tard dans l'appel d'API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_csv(data_dir + '/user_artists.dat', delimiter='\\t', index_col=0)\n",
    "# Render some sample data\n",
    "users_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the user list\n",
    "batch_users = users_df.sample(3).index.tolist()\n",
    "\n",
    "# Write the file to disk\n",
    "json_input_filename = \"json_input.json\"\n",
    "with open(data_dir + \"/\" + json_input_filename, 'w') as json_input:\n",
    "    for user_id in batch_users:\n",
    "        json_input.write('{\"userId\": \"' + str(user_id) + '\"}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showcase the input file:\n",
    "!cat $data_dir\"/\"$json_input_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargez le fichier vers S3 et enregistrez le chemin d'accès comme variable pour plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload files to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(json_input_filename).upload_file(data_dir+\"/\"+json_input_filename)\n",
    "s3_input_path = \"s3://\" + bucket_name + \"/\" + json_input_filename\n",
    "print(s3_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les recommandations par lots lisent l'entrée du fichier que nous avons chargé sur S3. De la même manière, les recommandations par lots enregistreront le résultat dans un fichier dans S3. Nous définissons donc le chemin de sortie où les résultats doivent être enregistrés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output path\n",
    "s3_output_path = \"s3://\" + bucket_name + \"/\"\n",
    "print(s3_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, exécutez simplement l'appel pour lancer l'exportation par lots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchInferenceJobArn = personalize.create_batch_inference_job (\n",
    "    solutionVersionArn = solution_version_arn,\n",
    "    jobName = \"Batch-Inference-Job\",\n",
    "    roleArn = role_arn,\n",
    "    jobInput = \n",
    "     {\"s3DataSource\": {\"path\": s3_input_path}},\n",
    "    jobOutput = \n",
    "     {\"s3DataDestination\":{\"path\": s3_output_path}}\n",
    ")\n",
    "batchInferenceJobArn = batchInferenceJobArn['batchInferenceJobArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécutez la boucle while ci-dessous pour suivre le statut de l'appel de recommandation par lot. Cette opération peut prendre environ 25 minutes, car Personalize doit mettre en place l'infrastructure nécessaire à l'exécution de cette tâche. Nous testons la fonction avec un jeu de données de seulement 3 utilisateurs, ce qui n'est pas une utilisation efficace de ce mécanisme. Normalement, vous n'utiliserez cette fonction que pour le traitement en masse, auquel cas les gains d'efficacité deviendront évidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now()\n",
    "print(\"Import Started on: \", current_time.strftime(\"%I:%M:%S %p\"))\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_inference_job_response = personalize.describe_batch_inference_job(\n",
    "        batchInferenceJobArn = batchInferenceJobArn\n",
    "    )\n",
    "    status = describe_dataset_inference_job_response[\"batchInferenceJob\"]['status']\n",
    "    print(\"DatasetInferenceJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "    \n",
    "current_time = datetime.now()\n",
    "print(\"Import Completed on: \", current_time.strftime(\"%I:%M:%S %p\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois que la tâche des recommandations par lots a terminé le traitement, nous pouvons analyser la sortie chargée vers S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "export_name = json_input_filename + \".out\"\n",
    "s3.download_file(bucket_name, export_name, data_dir+\"/\"+export_name)\n",
    "\n",
    "# Update DF rendering\n",
    "pd.set_option('display.max_rows', 30)\n",
    "with open(data_dir+\"/\"+export_name) as json_file:\n",
    "    # Get the first line and parse it\n",
    "    line = json.loads(json_file.readline())\n",
    "    # Do the same for the other lines\n",
    "    while line:\n",
    "        # extract the user ID \n",
    "        col_header = \"User: \" + line['input']['userId']\n",
    "        # Create a list for all the artists\n",
    "        recommendation_list = []\n",
    "        # Add all the entries\n",
    "        for item in line['output']['recommendedItems']:\n",
    "            artist = get_artist_by_id(item)\n",
    "            recommendation_list.append(artist)\n",
    "        if 'bulk_recommendations_df' in locals():\n",
    "            new_rec_DF = pd.DataFrame(recommendation_list, columns = [col_header])\n",
    "            bulk_recommendations_df = bulk_recommendations_df.join(new_rec_DF)\n",
    "        else:\n",
    "            bulk_recommendations_df = pd.DataFrame(recommendation_list, columns=[col_header])\n",
    "        try:\n",
    "            line = json.loads(json_file.readline())\n",
    "        except:\n",
    "            line = None\n",
    "bulk_recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyer les solutions\n",
    "\n",
    "Nettoyez ensuite les solutions. Le code ci-dessous énumérera toutes les solutions dans votre compte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = personalize.get_paginator('list_solutions')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for solution in paginate_result[\"solutions\"]:\n",
    "        print(solution[\"solutionArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consultez la liste des ARN pour déterminer la solution à supprimer. Utilisez ensuite le code ci-dessous pour le supprimer en insérant l'ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.delete_solution(\n",
    "    solutionArn = \"INSERT ARN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyer les jeux de données\n",
    "\n",
    "Nettoyez ensuite les jeux de données. Le code ci-dessous énumérera tous les jeux de données de votre compte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = personalize.get_paginator('list_datasets')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for datasets in paginate_result[\"datasets\"]:\n",
    "        print(datasets[\"datasetArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consultez la liste des ARN pour déterminer le jeu de données à supprimer. Utilisez ensuite le code ci-dessous pour le supprimer en insérant l'ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.delete_dataset(\n",
    "    datasetArn = \"INSERT ARN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyer les schémas\n",
    "\n",
    "Nettoyez ensuite les schémas. Le code ci-dessous énumérera tous les schémas de votre compte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = personalize.get_paginator('list_schemas')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for schema in paginate_result[\"schemas\"]:\n",
    "        print(schema[\"schemaArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consultez la liste des ARN pour déterminer le schéma à supprimer. Utilisez ensuite le code ci-dessous pour le supprimer en insérant l'ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.delete_schema(\n",
    "    schemaArn = \"INSERT ARN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyer le groupe de jeux de données\n",
    "\n",
    "Enfin, nettoyez le groupe de jeux de données. Le code ci-dessous énumérera tous les groupes de jeux de données de votre compte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = personalize.get_paginator('list_dataset_groups')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for dataset_group in paginate_result[\"datasetGroups\"]:\n",
    "        print(dataset_group[\"datasetGroupArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consultez la liste des ARN pour déterminer le groupe de jeux de données à supprimer. Utilisez ensuite le code ci-dessous pour le supprimer en insérant l'ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.delete_dataset_group(\n",
    "    datasetGroupArn = \"INSERT ARN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyer le compartiment S3 et le rôle IAM\n",
    "\n",
    "Vous pouvez éventuellement supprimer le rôle IAM et le compartiment S3 que nous avons créés pendant l'atelier.\n",
    "\n",
    "Commencez par répertorier tous les rôles IAM de votre compte à l'aide du code ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client('iam')\n",
    "\n",
    "paginator = iam.get_paginator('list_roles')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for roles in paginate_result[\"Roles\"]:\n",
    "        print(roles[\"RoleName\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifiez le nom du rôle que vous souhaitez supprimer.\n",
    "\n",
    "Vous ne pouvez pas supprimer un rôle IAM auquel des politiques sont encore attachées. Donc, après avoir identifié le rôle pertinent, répertorions la liste des politiques attachées au rôle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.list_attached_role_policies(\n",
    "    RoleName = \"INSERT ROLE NAME HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous devez détacher les politiques dans le résultat ci-dessus en utilisant le code ci-dessous. Répétez l'opération pour chaque politique attachée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.detach_role_policy(\n",
    "    RoleName = \"INSERT ROLE NAME HERE\",\n",
    "    PolicyArn = \"INSERT ARN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, vous devriez être en mesure de supprimer le rôle IAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.delete_role(\n",
    "    RoleName = \"INSERT ROLE NAME HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour supprimer un compartiment S3, il doit d'abord être vide. La façon la plus facile de supprimer un compartiment S3 est simplement de naviguer vers S3 dans la console AWS, de supprimer les objets du compartiment, puis de supprimer le compartiment S3 lui-même."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
